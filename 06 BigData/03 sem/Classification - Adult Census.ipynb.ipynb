{"cells":[{"cell_type":"markdown","source":["## 101 - Training and Evaluating Classifiers with `mmlspark`\n\nIn this example, we try to predict incomes from the *Adult Census* dataset.\n\nFirst, we import the packages (use `help(mmlspark)` to view contents),"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"447d4836-3b7e-4e1d-87d2-3d51f3598422"}}},{"cell_type":"code","source":["import numpy as np\nimport pandas as pd"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"57997d69-e2aa-44f4-827c-b71706da7a67"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["import mmlspark"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"80340d9e-8cd3-4c6a-b927-527edeccd6c2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Now let's read the data and split it to train and test sets:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f2255bc1-e139-4897-b70b-6b8dad0cd95b"}}},{"cell_type":"code","source":["data = spark.read.parquet(\"wasbs://publicwasb@mmlspark.blob.core.windows.net/AdultCensusIncome.parquet\")\ndata = data.select([\"education\", \"marital-status\", \"hours-per-week\", \"income\"])\ntrain, test = data.randomSplit([0.75, 0.25], seed=123)\ntrain.limit(10).toPandas()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9ba4ab4f-ffa6-41ad-82ec-c01ae56a1b91"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"data","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"education","nullable":true,"type":"string"},{"metadata":{},"name":"marital-status","nullable":true,"type":"string"},{"metadata":{},"name":"hours-per-week","nullable":true,"type":"double"},{"metadata":{},"name":"income","nullable":true,"type":"string"}],"type":"struct"},"tableIdentifier":null},{"name":"train","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"education","nullable":true,"type":"string"},{"metadata":{},"name":"marital-status","nullable":true,"type":"string"},{"metadata":{},"name":"hours-per-week","nullable":true,"type":"double"},{"metadata":{},"name":"income","nullable":true,"type":"string"}],"type":"struct"},"tableIdentifier":null},{"name":"test","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"education","nullable":true,"type":"string"},{"metadata":{},"name":"marital-status","nullable":true,"type":"string"},{"metadata":{},"name":"hours-per-week","nullable":true,"type":"double"},{"metadata":{},"name":"income","nullable":true,"type":"string"}],"type":"struct"},"tableIdentifier":null}],"data":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>education</th>\n      <th>marital-status</th>\n      <th>hours-per-week</th>\n      <th>income</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10th</td>\n      <td>Divorced</td>\n      <td>1.0</td>\n      <td>&lt;=50K</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10th</td>\n      <td>Divorced</td>\n      <td>40.0</td>\n      <td>&lt;=50K</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>10th</td>\n      <td>Divorced</td>\n      <td>40.0</td>\n      <td>&lt;=50K</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>10th</td>\n      <td>Divorced</td>\n      <td>40.0</td>\n      <td>&lt;=50K</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>10th</td>\n      <td>Divorced</td>\n      <td>40.0</td>\n      <td>&lt;=50K</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>10th</td>\n      <td>Divorced</td>\n      <td>40.0</td>\n      <td>&lt;=50K</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>10th</td>\n      <td>Divorced</td>\n      <td>40.0</td>\n      <td>&lt;=50K</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>10th</td>\n      <td>Divorced</td>\n      <td>40.0</td>\n      <td>&lt;=50K</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>10th</td>\n      <td>Divorced</td>\n      <td>68.0</td>\n      <td>&gt;50K</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>10th</td>\n      <td>Married-civ-spouse</td>\n      <td>16.0</td>\n      <td>&lt;=50K</td>\n    </tr>\n  </tbody>\n</table>\n</div>","textData":"<div class=\"ansiout\">Out[2]: </div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>education</th>\n      <th>marital-status</th>\n      <th>hours-per-week</th>\n      <th>income</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10th</td>\n      <td>Divorced</td>\n      <td>1.0</td>\n      <td>&lt;=50K</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10th</td>\n      <td>Divorced</td>\n      <td>40.0</td>\n      <td>&lt;=50K</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>10th</td>\n      <td>Divorced</td>\n      <td>40.0</td>\n      <td>&lt;=50K</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>10th</td>\n      <td>Divorced</td>\n      <td>40.0</td>\n      <td>&lt;=50K</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>10th</td>\n      <td>Divorced</td>\n      <td>40.0</td>\n      <td>&lt;=50K</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>10th</td>\n      <td>Divorced</td>\n      <td>40.0</td>\n      <td>&lt;=50K</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>10th</td>\n      <td>Divorced</td>\n      <td>40.0</td>\n      <td>&lt;=50K</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>10th</td>\n      <td>Divorced</td>\n      <td>40.0</td>\n      <td>&lt;=50K</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>10th</td>\n      <td>Divorced</td>\n      <td>68.0</td>\n      <td>&gt;50K</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>10th</td>\n      <td>Married-civ-spouse</td>\n      <td>16.0</td>\n      <td>&lt;=50K</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["`TrainClassifier` can be used to initialize and fit a model, it wraps SparkML classifiers.\nYou can use `help(mmlspark.TrainClassifier)` to view the different parameters.\n\nNote that it implicitly converts the data into the format expected by the algorithm: tokenize\nand hash strings, one-hot encodes categorical variables, assembles the features into a vector\nand so on.  The parameter `numFeatures` controls the number of hashed features."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8aa034fa-7526-4315-8149-49c602de84a9"}}},{"cell_type":"code","source":["from mmlspark.train import TrainClassifier\nfrom pyspark.ml.classification import LogisticRegression\nmodel = TrainClassifier(model=LogisticRegression(), labelCol=\"income\", numFeatures=256).fit(train)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b64b0490-6abf-42db-8388-b275d51f0a34"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["help(TrainClassifier)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8adf548a-831c-4cf7-a03b-949e0d2d3e4d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Help on class TrainClassifier in module mmlspark.train.TrainClassifier:\n\nclass TrainClassifier(mmlspark.train._TrainClassifier._TrainClassifier)\n |  Args:\n |  \n |      featuresCol (str): The name of the features column (default: [self.uid]_features)\n |      labelCol (str): The name of the label column\n |      labels (list): Sorted label values on the labels column\n |      model (object): Classifier to run\n |      numFeatures (int): Number of features to hash to (default: 0)\n |      reindexLabel (bool): Re-index the label column (default: true)\n |  \n |  Method resolution order:\n |      TrainClassifier\n |      mmlspark.train._TrainClassifier._TrainClassifier\n |      mmlspark.core.schema.Utils.ComplexParamsMixin\n |      pyspark.ml.util.JavaMLReadable\n |      pyspark.ml.util.MLReadable\n |      pyspark.ml.util.JavaMLWritable\n |      pyspark.ml.util.MLWritable\n |      pyspark.ml.wrapper.JavaEstimator\n |      pyspark.ml.wrapper.JavaParams\n |      pyspark.ml.wrapper.JavaWrapper\n |      pyspark.ml.base.Estimator\n |      pyspark.ml.param.Params\n |      pyspark.ml.util.Identifiable\n |      builtins.object\n |  \n |  Methods inherited from mmlspark.train._TrainClassifier._TrainClassifier:\n |  \n |  __init__(self, featuresCol=None, labelCol=None, labels=None, model=None, numFeatures=0, reindexLabel=True)\n |      Initialize self.  See help(type(self)) for accurate signature.\n |  \n |  getFeaturesCol(self)\n |      Returns:\n |      \n |          str: The name of the features column (default: [self.uid]_features)\n |  \n |  getLabelCol(self)\n |      Returns:\n |      \n |          str: The name of the label column\n |  \n |  getLabels(self)\n |      Returns:\n |      \n |          list: Sorted label values on the labels column\n |  \n |  getModel(self)\n |      Returns:\n |      \n |          object: Classifier to run\n |  \n |  getNumFeatures(self)\n |      Returns:\n |      \n |          int: Number of features to hash to (default: 0)\n |  \n |  getReindexLabel(self)\n |      Returns:\n |      \n |          bool: Re-index the label column (default: true)\n |  \n |  setFeaturesCol(self, value)\n |      Args:\n |      \n |          featuresCol: The name of the features column (default: [self.uid]_features)\n |  \n |  setLabelCol(self, value)\n |      Args:\n |      \n |          labelCol: The name of the label column\n |  \n |  setLabels(self, value)\n |      Args:\n |      \n |          labels: Sorted label values on the labels column\n |  \n |  setModel(self, value)\n |      Args:\n |      \n |          model: Classifier to run\n |  \n |  setNumFeatures(self, value)\n |      Args:\n |      \n |          numFeatures: Number of features to hash to (default: 0)\n |  \n |  setParams(self, featuresCol=None, labelCol=None, labels=None, model=None, numFeatures=0, reindexLabel=True)\n |      Set the (keyword only) parameters\n |      \n |      Args:\n |      \n |          featuresCol (str): The name of the features column (default: [self.uid]_features)\n |          labelCol (str): The name of the label column\n |          labels (list): Sorted label values on the labels column\n |          model (object): Classifier to run\n |          numFeatures (int): Number of features to hash to (default: 0)\n |          reindexLabel (bool): Re-index the label column (default: true)\n |  \n |  setReindexLabel(self, value)\n |      Args:\n |      \n |          reindexLabel: Re-index the label column (default: true)\n |  \n |  ----------------------------------------------------------------------\n |  Class methods inherited from mmlspark.train._TrainClassifier._TrainClassifier:\n |  \n |  read() from builtins.type\n |      Returns an MLReader instance for this class.\n |  \n |  ----------------------------------------------------------------------\n |  Static methods inherited from mmlspark.train._TrainClassifier._TrainClassifier:\n |  \n |  getJavaPackage()\n |      Returns package name String.\n |  \n |  ----------------------------------------------------------------------\n |  Class methods inherited from pyspark.ml.util.MLReadable:\n |  \n |  load(path) from builtins.type\n |      Reads an ML instance from the input path, a shortcut of `read().load(path)`.\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from pyspark.ml.util.MLReadable:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.util.JavaMLWritable:\n |  \n |  write(self)\n |      Returns an MLWriter instance for this ML instance.\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.util.MLWritable:\n |  \n |  save(self, path)\n |      Save this ML instance to the given path, a shortcut of &#39;write().save(path)&#39;.\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes inherited from pyspark.ml.wrapper.JavaEstimator:\n |  \n |  __metaclass__ = &lt;class &#39;abc.ABCMeta&#39;&gt;\n |      Metaclass for defining Abstract Base Classes (ABCs).\n |      \n |      Use this metaclass to create an ABC.  An ABC can be subclassed\n |      directly, and then acts as a mix-in class.  You can also register\n |      unrelated concrete classes (even built-in classes) and unrelated\n |      ABCs as &#39;virtual subclasses&#39; -- these and their descendants will\n |      be considered subclasses of the registering ABC by the built-in\n |      issubclass() function, but the registering ABC won&#39;t show up in\n |      their MRO (Method Resolution Order) nor will method\n |      implementations defined by the registering ABC be callable (not\n |      even via super()).\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.wrapper.JavaParams:\n |  \n |  copy(self, extra=None)\n |      Creates a copy of this instance with the same uid and some\n |      extra params. This implementation first calls Params.copy and\n |      then make a copy of the companion Java pipeline component with\n |      extra params. So both the Python wrapper and the Java pipeline\n |      component get copied.\n |      \n |      :param extra: Extra parameters to copy to the new instance\n |      :return: Copy of this instance\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.wrapper.JavaWrapper:\n |  \n |  __del__(self)\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.base.Estimator:\n |  \n |  fit(self, dataset, params=None)\n |      Fits a model to the input dataset with optional parameters.\n |      \n |      :param dataset: input dataset, which is an instance of :py:class:`pyspark.sql.DataFrame`\n |      :param params: an optional param map that overrides embedded params. If a list/tuple of\n |                     param maps is given, this calls fit on each param map and returns a list of\n |                     models.\n |      :returns: fitted model(s)\n |      \n |      .. versionadded:: 1.3.0\n |  \n |  fitMultiple(self, dataset, paramMaps)\n |      Fits a model to the input dataset for each param map in `paramMaps`.\n |      \n |      :param dataset: input dataset, which is an instance of :py:class:`pyspark.sql.DataFrame`.\n |      :param paramMaps: A Sequence of param maps.\n |      :return: A thread safe iterable which contains one model for each param map. Each\n |               call to `next(modelIterator)` will return `(index, model)` where model was fit\n |               using `paramMaps[index]`. `index` values may not be sequential.\n |      \n |      .. note:: DeveloperApi\n |      .. note:: Experimental\n |      \n |      .. versionadded:: 2.3.0\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.param.Params:\n |  \n |  explainParam(self, param)\n |      Explains a single param and returns its name, doc, and optional\n |      default value and user-supplied value in a string.\n |  \n |  explainParams(self)\n |      Returns the documentation of all params with their optionally\n |      default values and user-supplied values.\n |  \n |  extractParamMap(self, extra=None)\n |      Extracts the embedded default param values and user-supplied\n |      values, and then merges them with extra values from input into\n |      a flat param map, where the latter value is used if there exist\n |      conflicts, i.e., with ordering: default param values &lt;\n |      user-supplied values &lt; extra.\n |      \n |      :param extra: extra param values\n |      :return: merged param map\n |  \n |  getOrDefault(self, param)\n |      Gets the value of a param in the user-supplied param map or its\n |      default value. Raises an error if neither is set.\n |  \n |  getParam(self, paramName)\n |      Gets a param by its name.\n |  \n |  hasDefault(self, param)\n |      Checks whether a param has a default value.\n |  \n |  hasParam(self, paramName)\n |      Tests whether this instance contains a param with a given\n |      (string) name.\n |  \n |  isDefined(self, param)\n |      Checks whether a param is explicitly set by user or has\n |      a default value.\n |  \n |  isSet(self, param)\n |      Checks whether a param is explicitly set by user.\n |  \n |  set(self, param, value)\n |      Sets a parameter in the embedded param map.\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from pyspark.ml.param.Params:\n |  \n |  params\n |      Returns all params ordered by name. The default implementation\n |      uses :py:func:`dir` to get all attributes of type\n |      :py:class:`Param`.\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.util.Identifiable:\n |  \n |  __repr__(self)\n |      Return repr(self).\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Help on class TrainClassifier in module mmlspark.train.TrainClassifier:\n\nclass TrainClassifier(mmlspark.train._TrainClassifier._TrainClassifier)\n  Args:\n  \n      featuresCol (str): The name of the features column (default: [self.uid]_features)\n      labelCol (str): The name of the label column\n      labels (list): Sorted label values on the labels column\n      model (object): Classifier to run\n      numFeatures (int): Number of features to hash to (default: 0)\n      reindexLabel (bool): Re-index the label column (default: true)\n  \n  Method resolution order:\n      TrainClassifier\n      mmlspark.train._TrainClassifier._TrainClassifier\n      mmlspark.core.schema.Utils.ComplexParamsMixin\n      pyspark.ml.util.JavaMLReadable\n      pyspark.ml.util.MLReadable\n      pyspark.ml.util.JavaMLWritable\n      pyspark.ml.util.MLWritable\n      pyspark.ml.wrapper.JavaEstimator\n      pyspark.ml.wrapper.JavaParams\n      pyspark.ml.wrapper.JavaWrapper\n      pyspark.ml.base.Estimator\n      pyspark.ml.param.Params\n      pyspark.ml.util.Identifiable\n      builtins.object\n  \n  Methods inherited from mmlspark.train._TrainClassifier._TrainClassifier:\n  \n  __init__(self, featuresCol=None, labelCol=None, labels=None, model=None, numFeatures=0, reindexLabel=True)\n      Initialize self.  See help(type(self)) for accurate signature.\n  \n  getFeaturesCol(self)\n      Returns:\n      \n          str: The name of the features column (default: [self.uid]_features)\n  \n  getLabelCol(self)\n      Returns:\n      \n          str: The name of the label column\n  \n  getLabels(self)\n      Returns:\n      \n          list: Sorted label values on the labels column\n  \n  getModel(self)\n      Returns:\n      \n          object: Classifier to run\n  \n  getNumFeatures(self)\n      Returns:\n      \n          int: Number of features to hash to (default: 0)\n  \n  getReindexLabel(self)\n      Returns:\n      \n          bool: Re-index the label column (default: true)\n  \n  setFeaturesCol(self, value)\n      Args:\n      \n          featuresCol: The name of the features column (default: [self.uid]_features)\n  \n  setLabelCol(self, value)\n      Args:\n      \n          labelCol: The name of the label column\n  \n  setLabels(self, value)\n      Args:\n      \n          labels: Sorted label values on the labels column\n  \n  setModel(self, value)\n      Args:\n      \n          model: Classifier to run\n  \n  setNumFeatures(self, value)\n      Args:\n      \n          numFeatures: Number of features to hash to (default: 0)\n  \n  setParams(self, featuresCol=None, labelCol=None, labels=None, model=None, numFeatures=0, reindexLabel=True)\n      Set the (keyword only) parameters\n      \n      Args:\n      \n          featuresCol (str): The name of the features column (default: [self.uid]_features)\n          labelCol (str): The name of the label column\n          labels (list): Sorted label values on the labels column\n          model (object): Classifier to run\n          numFeatures (int): Number of features to hash to (default: 0)\n          reindexLabel (bool): Re-index the label column (default: true)\n  \n  setReindexLabel(self, value)\n      Args:\n      \n          reindexLabel: Re-index the label column (default: true)\n  \n  ----------------------------------------------------------------------\n  Class methods inherited from mmlspark.train._TrainClassifier._TrainClassifier:\n  \n  read() from builtins.type\n      Returns an MLReader instance for this class.\n  \n  ----------------------------------------------------------------------\n  Static methods inherited from mmlspark.train._TrainClassifier._TrainClassifier:\n  \n  getJavaPackage()\n      Returns package name String.\n  \n  ----------------------------------------------------------------------\n  Class methods inherited from pyspark.ml.util.MLReadable:\n  \n  load(path) from builtins.type\n      Reads an ML instance from the input path, a shortcut of `read().load(path)`.\n  \n  ----------------------------------------------------------------------\n  Data descriptors inherited from pyspark.ml.util.MLReadable:\n  \n  __dict__\n      dictionary for instance variables (if defined)\n  \n  __weakref__\n      list of weak references to the object (if defined)\n  \n  ----------------------------------------------------------------------\n  Methods inherited from pyspark.ml.util.JavaMLWritable:\n  \n  write(self)\n      Returns an MLWriter instance for this ML instance.\n  \n  ----------------------------------------------------------------------\n  Methods inherited from pyspark.ml.util.MLWritable:\n  \n  save(self, path)\n      Save this ML instance to the given path, a shortcut of &#39;write().save(path)&#39;.\n  \n  ----------------------------------------------------------------------\n  Data and other attributes inherited from pyspark.ml.wrapper.JavaEstimator:\n  \n  __metaclass__ = &lt;class &#39;abc.ABCMeta&#39;&gt;\n      Metaclass for defining Abstract Base Classes (ABCs).\n      \n      Use this metaclass to create an ABC.  An ABC can be subclassed\n      directly, and then acts as a mix-in class.  You can also register\n      unrelated concrete classes (even built-in classes) and unrelated\n      ABCs as &#39;virtual subclasses&#39; -- these and their descendants will\n      be considered subclasses of the registering ABC by the built-in\n      issubclass() function, but the registering ABC won&#39;t show up in\n      their MRO (Method Resolution Order) nor will method\n      implementations defined by the registering ABC be callable (not\n      even via super()).\n  \n  ----------------------------------------------------------------------\n  Methods inherited from pyspark.ml.wrapper.JavaParams:\n  \n  copy(self, extra=None)\n      Creates a copy of this instance with the same uid and some\n      extra params. This implementation first calls Params.copy and\n      then make a copy of the companion Java pipeline component with\n      extra params. So both the Python wrapper and the Java pipeline\n      component get copied.\n      \n      :param extra: Extra parameters to copy to the new instance\n      :return: Copy of this instance\n  \n  ----------------------------------------------------------------------\n  Methods inherited from pyspark.ml.wrapper.JavaWrapper:\n  \n  __del__(self)\n  \n  ----------------------------------------------------------------------\n  Methods inherited from pyspark.ml.base.Estimator:\n  \n  fit(self, dataset, params=None)\n      Fits a model to the input dataset with optional parameters.\n      \n      :param dataset: input dataset, which is an instance of :py:class:`pyspark.sql.DataFrame`\n      :param params: an optional param map that overrides embedded params. If a list/tuple of\n                     param maps is given, this calls fit on each param map and returns a list of\n                     models.\n      :returns: fitted model(s)\n      \n      .. versionadded:: 1.3.0\n  \n  fitMultiple(self, dataset, paramMaps)\n      Fits a model to the input dataset for each param map in `paramMaps`.\n      \n      :param dataset: input dataset, which is an instance of :py:class:`pyspark.sql.DataFrame`.\n      :param paramMaps: A Sequence of param maps.\n      :return: A thread safe iterable which contains one model for each param map. Each\n               call to `next(modelIterator)` will return `(index, model)` where model was fit\n               using `paramMaps[index]`. `index` values may not be sequential.\n      \n      .. note:: DeveloperApi\n      .. note:: Experimental\n      \n      .. versionadded:: 2.3.0\n  \n  ----------------------------------------------------------------------\n  Methods inherited from pyspark.ml.param.Params:\n  \n  explainParam(self, param)\n      Explains a single param and returns its name, doc, and optional\n      default value and user-supplied value in a string.\n  \n  explainParams(self)\n      Returns the documentation of all params with their optionally\n      default values and user-supplied values.\n  \n  extractParamMap(self, extra=None)\n      Extracts the embedded default param values and user-supplied\n      values, and then merges them with extra values from input into\n      a flat param map, where the latter value is used if there exist\n      conflicts, i.e., with ordering: default param values &lt;\n      user-supplied values &lt; extra.\n      \n      :param extra: extra param values\n      :return: merged param map\n  \n  getOrDefault(self, param)\n      Gets the value of a param in the user-supplied param map or its\n      default value. Raises an error if neither is set.\n  \n  getParam(self, paramName)\n      Gets a param by its name.\n  \n  hasDefault(self, param)\n      Checks whether a param has a default value.\n  \n  hasParam(self, paramName)\n      Tests whether this instance contains a param with a given\n      (string) name.\n  \n  isDefined(self, param)\n      Checks whether a param is explicitly set by user or has\n      a default value.\n  \n  isSet(self, param)\n      Checks whether a param is explicitly set by user.\n  \n  set(self, param, value)\n      Sets a parameter in the embedded param map.\n  \n  ----------------------------------------------------------------------\n  Data descriptors inherited from pyspark.ml.param.Params:\n  \n  params\n      Returns all params ordered by name. The default implementation\n      uses :py:func:`dir` to get all attributes of type\n      :py:class:`Param`.\n  \n  ----------------------------------------------------------------------\n  Methods inherited from pyspark.ml.util.Identifiable:\n  \n  __repr__(self)\n      Return repr(self).\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["After the model is trained, we score it against the test dataset and view metrics."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b58128fe-a58c-4eab-b3cd-f8ed931dffea"}}},{"cell_type":"code","source":["from mmlspark.train import ComputeModelStatistics, TrainedClassifierModel\nprediction = model.transform(test)\nmetrics = ComputeModelStatistics().transform(prediction)\nmetrics.limit(10).toPandas()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ae5337f4-3427-4f70-a959-54b964c67df5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"prediction","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"education","nullable":true,"type":"string"},{"metadata":{},"name":"marital-status","nullable":true,"type":"string"},{"metadata":{},"name":"hours-per-week","nullable":true,"type":"double"},{"metadata":{"mml":{"null_exists":false,"ord":false,"score_model78062777-f022-489b-ae73-298e199a8a78":{"ScoreColumnKind":"true_labels","ScoreValueKind":"Classification"},"vals":[" <=50K"," >50K"]}},"name":"income","nullable":true,"type":"string"},{"metadata":{"mml":{"score_model78062777-f022-489b-ae73-298e199a8a78":{"ScoreColumnKind":"scores","ScoreValueKind":"Classification"}}},"name":"scores","nullable":true,"type":{"class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"fields":[{"metadata":{},"name":"type","nullable":false,"type":"byte"},{"metadata":{},"name":"size","nullable":true,"type":"integer"},{"metadata":{},"name":"indices","nullable":true,"type":{"containsNull":false,"elementType":"integer","type":"array"}},{"metadata":{},"name":"values","nullable":true,"type":{"containsNull":false,"elementType":"double","type":"array"}}],"type":"struct"},"type":"udt"}},{"metadata":{"mml":{"score_model78062777-f022-489b-ae73-298e199a8a78":{"ScoreColumnKind":"scored_probabilities","ScoreValueKind":"Classification"}}},"name":"scored_probabilities","nullable":true,"type":{"class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"fields":[{"metadata":{},"name":"type","nullable":false,"type":"byte"},{"metadata":{},"name":"size","nullable":true,"type":"integer"},{"metadata":{},"name":"indices","nullable":true,"type":{"containsNull":false,"elementType":"integer","type":"array"}},{"metadata":{},"name":"values","nullable":true,"type":{"containsNull":false,"elementType":"double","type":"array"}}],"type":"struct"},"type":"udt"}},{"metadata":{"mml":{"null_exists":false,"ord":false,"score_model78062777-f022-489b-ae73-298e199a8a78":{"ScoreColumnKind":"scored_labels","ScoreValueKind":"Classification"},"vals":[" <=50K"," >50K"]}},"name":"scored_labels","nullable":false,"type":"double"}],"type":"struct"},"tableIdentifier":null},{"name":"metrics","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"evaluation_type","nullable":true,"type":"string"},{"metadata":{},"name":"confusion_matrix","nullable":true,"type":{"class":"org.apache.spark.ml.linalg.MatrixUDT","pyClass":"pyspark.ml.linalg.MatrixUDT","sqlType":{"fields":[{"metadata":{},"name":"type","nullable":false,"type":"byte"},{"metadata":{},"name":"numRows","nullable":false,"type":"integer"},{"metadata":{},"name":"numCols","nullable":false,"type":"integer"},{"metadata":{},"name":"colPtrs","nullable":true,"type":{"containsNull":false,"elementType":"integer","type":"array"}},{"metadata":{},"name":"rowIndices","nullable":true,"type":{"containsNull":false,"elementType":"integer","type":"array"}},{"metadata":{},"name":"values","nullable":true,"type":{"containsNull":false,"elementType":"double","type":"array"}},{"metadata":{},"name":"isTransposed","nullable":false,"type":"boolean"}],"type":"struct"},"type":"udt"}},{"metadata":{},"name":"accuracy","nullable":false,"type":"double"},{"metadata":{},"name":"precision","nullable":false,"type":"double"},{"metadata":{},"name":"recall","nullable":false,"type":"double"},{"metadata":{},"name":"AUC","nullable":false,"type":"double"}],"type":"struct"},"tableIdentifier":null}],"data":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>evaluation_type</th>\n      <th>confusion_matrix</th>\n      <th>accuracy</th>\n      <th>precision</th>\n      <th>recall</th>\n      <th>AUC</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Classification</td>\n      <td>DenseMatrix([[5780.,  378.],\\n             [10...</td>\n      <td>0.825283</td>\n      <td>0.708333</td>\n      <td>0.468846</td>\n      <td>0.87245</td>\n    </tr>\n  </tbody>\n</table>\n</div>","textData":"<div class=\"ansiout\">Out[8]: </div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>evaluation_type</th>\n      <th>confusion_matrix</th>\n      <th>accuracy</th>\n      <th>precision</th>\n      <th>recall</th>\n      <th>AUC</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Classification</td>\n      <td>DenseMatrix([[5780.,  378.],\\n             [10...</td>\n      <td>0.825283</td>\n      <td>0.708333</td>\n      <td>0.468846</td>\n      <td>0.87245</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Finally, we save the model so it can be used in a scoring program."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4ba5e6a1-3215-4880-b3cf-409b7adbf50b"}}},{"cell_type":"code","source":["model.write().overwrite().save(\"AdultCensus.mml\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3b5bfd3b-6e29-4037-a4bc-0260db8da6c9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a897a042-59ff-4567-935d-1c377693a22d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0}],"metadata":{"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.6.3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python [default]","language":"python","name":"python3"},"anaconda-cloud":{},"application/vnd.databricks.v1+notebook":{"notebookName":"Classification - Adult Census.ipynb","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":1232268167943500},"pycharm":{"stem_cell":{"cell_type":"raw","source":[],"metadata":{"collapsed":false}}}},"nbformat":4,"nbformat_minor":0}
