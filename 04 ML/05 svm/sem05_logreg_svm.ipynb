{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Семинар 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (15, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Линейная классификация\n",
    "\n",
    "### Постановка задачи классификации\n",
    "\n",
    "Пусть задана обучающая выборка $X = \\left\\{ \\left( x_i, y_i \\right) \\right\\}_{i=1}^l, x_i \\in \\mathbb{X}, y_i \\in \\mathbb{Y},$ — $l$ пар объект-ответ, где\n",
    "$\\mathbb{X}$ — пространство объектов,\n",
    "$\\mathbb{Y}$ — пространство ответов.\n",
    "\n",
    "\n",
    "### Логистическая регрессия\n",
    "\n",
    "Рассмотрим в качестве верхней оценки пороговой функции потерь логистическую функцию:\n",
    "\n",
    "$$\\tilde{L}(M) = \\log (1 + \\exp(-M)).$$\n",
    "\n",
    "Таким образом, необходимо решить следующую оптимизационную задачу:\n",
    "$$\\frac{1}{l} \\sum_{i=1}^l \\tilde{L} (M_i) = \\frac{1}{l} \\sum_{i=1}^l \\log (1 + \\exp (-y_i \\langle w, x_i \\rangle)) \\to \\min_w$$\n",
    "\n",
    "Получившийся метод обучения называется **логистической регрессией**.\n",
    "\n",
    "Одно из полезных свойств логистической регрессии, которое будет изучено нами несколько позднее, — тот факт, что она позволяет предсказывать помимо метки класса ещё и вероятность принадлежности каждому из них, что может быть полезным в некоторых задачах.\n",
    "\n",
    "**Пример**: Вы работаете в банке и хотите выдавать кредиты только тем клиентам, которые вернут его с вероятностью не меньше 0.9.\n",
    "\n",
    "Попробуем сконструировать функцию потерь из других соображений.\n",
    "Если алгоритм $b(x) \\in [0, 1]$ действительно выдает вероятности, то\n",
    "они должны согласовываться с выборкой.\n",
    "С точки зрения алгоритма вероятность того, что в выборке встретится объект $x_i$ с классом $y_i$,\n",
    "равна $b(x_i)^{[y_i = +1]} (1 - b(x_i))^{[y_i = -1]}$.\n",
    "\n",
    "Исходя из этого, можно записать правдоподобие выборки (т.е. вероятность получить такую выборку\n",
    "с точки зрения алгоритма):\n",
    "$$\n",
    "    Q(a, X)\n",
    "    =\n",
    "    \\prod_{i = 1}^{\\ell}\n",
    "        b(x_i)^{[y_i = +1]} (1 - b(x_i))^{[y_i = -1]}.\n",
    "$$\n",
    "Данное правдоподобие можно использовать как функционал для обучения алгоритма --\n",
    "с той лишь оговоркой, что удобнее оптимизировать его логарифм:\n",
    "$$\n",
    "    -\\sum_{i = 1}^{\\ell} \\left(\n",
    "        [y_i = +1] \\log b(x_i)\n",
    "        +\n",
    "        [y_i = -1] \\log (1 - b(x_i))\n",
    "    \\right)\n",
    "    \\to\n",
    "    \\min\n",
    "$$\n",
    "Данная функция потерь называется логарифмической (log-loss).\n",
    "\n",
    "Мы хотим предсказывать вероятности, то есть, чтобы наш алгоритм предсказывал числа в интервале [0, 1]. Этого легко достичь, если положить $b(x) = \\sigma(\\langle w, x \\rangle)$,\n",
    "где в качестве $\\sigma$ может выступать любая монотонно неубывающая функция\n",
    "с областью значений $[0, 1]$.\n",
    "Мы будем использовать сигмоидную функцию: $\\sigma(z) = \\frac{1}{1 + \\exp(-z)}$.\n",
    "Таким образом, чем больше скалярное произведение $\\langle w, x \\rangle$,\n",
    "тем больше будет предсказанная вероятность.\n",
    "\n",
    "Подставим трансформированный ответ линейной модели в логарифмическую функцию потерь:\n",
    "\\begin{align*}\n",
    "    -\\sum_{i = 1}^{\\ell} &\\left(\n",
    "        [y_i = +1]\n",
    "        \\log \\frac{1}{1 + \\exp(-\\langle w, x_i \\rangle)}\n",
    "        +\n",
    "        [y_i = -1]\n",
    "        \\log \\frac{\\exp(-\\langle w, x_i \\rangle)}{1 + \\exp(-\\langle w, x_i \\rangle)}\n",
    "    \\right)\n",
    "    =\\\\\n",
    "    &=\n",
    "    -\\sum_{i = 1}^{\\ell} \\left(\n",
    "        [y_i = +1]\n",
    "        \\log \\frac{1}{1 + \\exp(-\\langle w, x_i \\rangle)}\n",
    "        +\n",
    "        [y_i = -1]\n",
    "        \\log \\frac{1}{1 + \\exp(\\langle w, x_i \\rangle)}\n",
    "    \\right)\n",
    "    =\\\\\n",
    "    &=\n",
    "    \\sum_{i = 1}^{\\ell} \\left(\n",
    "        [y_i = +1]\n",
    "        \\log (1 + \\exp(-\\langle w, x_i \\rangle))\n",
    "        +\n",
    "        [y_i = -1]\n",
    "        \\log (1 + \\exp(\\langle w, x_i \\rangle))\n",
    "    \\right)\n",
    "    =\\\\\n",
    "    &=\n",
    "    \\sum_{i = 1}^{\\ell}\n",
    "        \\log \\left(\n",
    "            1 + \\exp(-y_i \\langle w, x_i \\rangle)\n",
    "        \\right).\n",
    "\\end{align*}\n",
    "\n",
    "Полученная функция в точности представляет собой логистические потери,\n",
    "упомянутые в начале.\n",
    "Линейная модель классификации, настроенная путём минимизации данного функционала,\n",
    "называется логистической регрессией.\n",
    "Как видно из приведенных рассуждений, она оптимизирует\n",
    "правдоподобие выборки и дает корректные оценки вероятности принадлежности к положительному классу.\n",
    "\n",
    "\n",
    "### Пример обучения логистической регрессии\n",
    "#### Определение спама по тексту электронного письма\n",
    "\n",
    "Попробуем при помощи моделей линейной классификации построить алгоритм, отделяющий спам от нормальной почты. Для экспериментов воспользуемся небольшим набором данных с [UCI](https://archive.ics.uci.edu/ml/datasets.html). Объекты в датасете соответствуют письмам, которые описаны признаками на основе текста письма, спам — положительный пример для классификации, хорошее письмо — отрицательный пример.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_all</th>\n",
       "      <th>word_freq_3d</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_over</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_;</th>\n",
       "      <th>char_freq_(</th>\n",
       "      <th>char_freq_[</th>\n",
       "      <th>char_freq_!</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.250</td>\n",
       "      <td>5</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.147</td>\n",
       "      <td>5</td>\n",
       "      <td>78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.718</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.404</td>\n",
       "      <td>6</td>\n",
       "      <td>118</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.353</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.555</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.142</td>\n",
       "      <td>3</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
       "0            0.00                0.0           0.65           0.0   \n",
       "1            0.96                0.0           0.00           0.0   \n",
       "2            0.30                0.0           0.30           0.0   \n",
       "3            0.00                0.0           0.00           0.0   \n",
       "4            0.31                0.0           0.62           0.0   \n",
       "\n",
       "   word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
       "0           0.00            0.00               0.0                 0.0   \n",
       "1           0.32            0.00               0.0                 0.0   \n",
       "2           0.00            0.00               0.0                 0.0   \n",
       "3           0.00            0.00               0.0                 0.0   \n",
       "4           0.00            0.31               0.0                 0.0   \n",
       "\n",
       "   word_freq_order  word_freq_mail  ...  char_freq_;  char_freq_(  \\\n",
       "0              0.0             0.0  ...        0.000        0.000   \n",
       "1              0.0             0.0  ...        0.000        0.057   \n",
       "2              0.0             0.0  ...        0.102        0.718   \n",
       "3              0.0             0.0  ...        0.000        0.000   \n",
       "4              0.0             0.0  ...        0.000        0.232   \n",
       "\n",
       "   char_freq_[  char_freq_!  char_freq_$  char_freq_#  \\\n",
       "0          0.0        0.125          0.0          0.0   \n",
       "1          0.0        0.000          0.0          0.0   \n",
       "2          0.0        0.000          0.0          0.0   \n",
       "3          0.0        0.353          0.0          0.0   \n",
       "4          0.0        0.000          0.0          0.0   \n",
       "\n",
       "   capital_run_length_average  capital_run_length_longest  \\\n",
       "0                       1.250                           5   \n",
       "1                       1.147                           5   \n",
       "2                       1.404                           6   \n",
       "3                       1.555                           4   \n",
       "4                       1.142                           3   \n",
       "\n",
       "   capital_run_length_total  spam  \n",
       "0                        40     0  \n",
       "1                        78     0  \n",
       "2                       118     0  \n",
       "3                        14     0  \n",
       "4                        88     0  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_data = pd.read_csv('spam_data.csv')\n",
    "spam_data\n",
    "\n",
    "X, y = spam_data.iloc[:, :-1].values, spam_data.iloc[:, -1].values\n",
    " \n",
    "spam_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4601, 57), (4601,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение логистической регрессии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим выборку на обучающую и тестовую в отношении 80/20 и обучим логистическую регрессию при помощи объекта [LogisticRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X_train = X[:int(len(X) * 0.8)]\n",
    "y_train = y[:int(len(X) * 0.8)]\n",
    "X_test = X[int(len(X) * 0.8):]\n",
    "y_test = y[int(len(X) * 0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=3000, random_state=13)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(max_iter=3000, solver='lbfgs', random_state=13)\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "??LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_with_th(X, model, th=0.5):\n",
    "    th = min(max(0, th),1)\n",
    "    probs = model.predict_proba(X)\n",
    "    labels = np.zeros(X.shape[0])\n",
    "    labels = prob > th\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычислим долю правильных ответов при помощи соответствующей функции из модуля [sklearn.metrics](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2788\n",
       "1    1813\n",
       "Name: spam, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_data['spam'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9315217391304348\n",
      "0.7937024972855592\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_train, lr.predict(X_train)))\n",
    "print(accuracy_score(y_test, lr.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В чем проблема?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[:int(len(X) * 0.8)]\n",
    "y_train = y[:int(len(X) * 0.8)]\n",
    "X_test = X[int(len(X) * 0.8):]\n",
    "y_test = y[int(len(X) * 0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.24239130434782608, 1.0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.mean(), y_test.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.39565217391304347, 0.38762214983713356)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=13)\n",
    "y_train.mean(), y_test.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "??train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.39402173913043476, 0.3941368078175896)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=13)\n",
    "y_train.mean(), y_test.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=3000)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training\n",
    "lr = LogisticRegression(max_iter=3000, solver='lbfgs')\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9380434782608695\n",
      "0.9120521172638436\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_train, lr.predict(X_train)))\n",
    "print(accuracy_score(y_test, lr.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь будем смотреть на AUC-ROC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.979321014380702\n",
      "0.9591318858181029\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "print(roc_auc_score(y_train, lr.predict_proba(X_train)[:, 1]))\n",
    "print(roc_auc_score(y_test, lr.predict_proba(X_test)[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.96780045, 0.03219955],\n",
       "       [0.99409979, 0.00590021],\n",
       "       [0.69562986, 0.30437014],\n",
       "       ...,\n",
       "       [0.99642941, 0.00357059],\n",
       "       [0.00467549, 0.99532451],\n",
       "       [0.6575597 , 0.3424403 ]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.predict_proba(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте попробуем сделать лучше. У нашего алгоритма есть разные гиперпараметры: способ регуляризации, коэффициент регуляризации. Запустим поиск по сетке гиперпараметров, алгоритм переберет все возможные комбинации, посчитает метрику для каждого набора и выдаст лучший набор."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.00000000e-05, 1.32571137e-05, 1.75751062e-05, 2.32995181e-05,\n",
       "       3.08884360e-05, 4.09491506e-05, 5.42867544e-05, 7.19685673e-05,\n",
       "       9.54095476e-05, 1.26485522e-04, 1.67683294e-04, 2.22299648e-04,\n",
       "       2.94705170e-04, 3.90693994e-04, 5.17947468e-04, 6.86648845e-04,\n",
       "       9.10298178e-04, 1.20679264e-03, 1.59985872e-03, 2.12095089e-03,\n",
       "       2.81176870e-03, 3.72759372e-03, 4.94171336e-03, 6.55128557e-03,\n",
       "       8.68511374e-03, 1.15139540e-02, 1.52641797e-02, 2.02358965e-02,\n",
       "       2.68269580e-02, 3.55648031e-02, 4.71486636e-02, 6.25055193e-02,\n",
       "       8.28642773e-02, 1.09854114e-01, 1.45634848e-01, 1.93069773e-01,\n",
       "       2.55954792e-01, 3.39322177e-01, 4.49843267e-01, 5.96362332e-01,\n",
       "       7.90604321e-01, 1.04811313e+00, 1.38949549e+00, 1.84206997e+00,\n",
       "       2.44205309e+00, 3.23745754e+00, 4.29193426e+00, 5.68986603e+00,\n",
       "       7.54312006e+00, 1.00000000e+01])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.logspace(-5, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.00000000e-05, 2.04091429e-01, 4.08172857e-01, 6.12254286e-01,\n",
       "       8.16335714e-01, 1.02041714e+00, 1.22449857e+00, 1.42858000e+00,\n",
       "       1.63266143e+00, 1.83674286e+00, 2.04082429e+00, 2.24490571e+00,\n",
       "       2.44898714e+00, 2.65306857e+00, 2.85715000e+00, 3.06123143e+00,\n",
       "       3.26531286e+00, 3.46939429e+00, 3.67347571e+00, 3.87755714e+00,\n",
       "       4.08163857e+00, 4.28572000e+00, 4.48980143e+00, 4.69388286e+00,\n",
       "       4.89796429e+00, 5.10204571e+00, 5.30612714e+00, 5.51020857e+00,\n",
       "       5.71429000e+00, 5.91837143e+00, 6.12245286e+00, 6.32653429e+00,\n",
       "       6.53061571e+00, 6.73469714e+00, 6.93877857e+00, 7.14286000e+00,\n",
       "       7.34694143e+00, 7.55102286e+00, 7.75510429e+00, 7.95918571e+00,\n",
       "       8.16326714e+00, 8.36734857e+00, 8.57143000e+00, 8.77551143e+00,\n",
       "       8.97959286e+00, 9.18367429e+00, 9.38775571e+00, 9.59183714e+00,\n",
       "       9.79591857e+00, 1.00000000e+01])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linspace(10**(-5), 10**(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То, какая метрика будет использоваться, определяется параметром `'scoring'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['accuracy',\n",
       " 'adjusted_mutual_info_score',\n",
       " 'adjusted_rand_score',\n",
       " 'average_precision',\n",
       " 'balanced_accuracy',\n",
       " 'completeness_score',\n",
       " 'explained_variance',\n",
       " 'f1',\n",
       " 'f1_macro',\n",
       " 'f1_micro',\n",
       " 'f1_samples',\n",
       " 'f1_weighted',\n",
       " 'fowlkes_mallows_score',\n",
       " 'homogeneity_score',\n",
       " 'jaccard',\n",
       " 'jaccard_macro',\n",
       " 'jaccard_micro',\n",
       " 'jaccard_samples',\n",
       " 'jaccard_weighted',\n",
       " 'max_error',\n",
       " 'mutual_info_score',\n",
       " 'neg_brier_score',\n",
       " 'neg_log_loss',\n",
       " 'neg_mean_absolute_error',\n",
       " 'neg_mean_gamma_deviance',\n",
       " 'neg_mean_poisson_deviance',\n",
       " 'neg_mean_squared_error',\n",
       " 'neg_mean_squared_log_error',\n",
       " 'neg_median_absolute_error',\n",
       " 'neg_root_mean_squared_error',\n",
       " 'normalized_mutual_info_score',\n",
       " 'precision',\n",
       " 'precision_macro',\n",
       " 'precision_micro',\n",
       " 'precision_samples',\n",
       " 'precision_weighted',\n",
       " 'r2',\n",
       " 'recall',\n",
       " 'recall_macro',\n",
       " 'recall_micro',\n",
       " 'recall_samples',\n",
       " 'recall_weighted',\n",
       " 'roc_auc',\n",
       " 'roc_auc_ovo',\n",
       " 'roc_auc_ovo_weighted',\n",
       " 'roc_auc_ovr',\n",
       " 'roc_auc_ovr_weighted',\n",
       " 'v_measure_score']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import SCORERS\n",
    "sorted(SCORERS.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "grid_searcher = GridSearchCV(\n",
    "    LogisticRegression(max_iter=3000, solver='liblinear', random_state=13), # тут задали фикс параметры, ниже сетку\n",
    "    param_grid={\n",
    "        'C': np.logspace(-5, 1),\n",
    "        'penalty': ['l1', 'l2']\n",
    "    },\n",
    "    cv=5,\n",
    "    scoring='roc_auc'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Параметр `cv=5` говорит, что во время поиска оптимальных параметров будет использоваться кросс-валидация с 5 фолдами. Давайте вспомним, что это такое: \n",
    "\n",
    "![alt text](https://scikit-learn.org/stable/_images/grid_search_cross_validation.png)\n",
    "\n",
    "*Source: https://scikit-learn.org/stable/modules/cross_validation.html*\n",
    "\n",
    "В нашем случае, выборка будет разделена на 5 частей, и на каждой из 5 итераций часть данных будет становиться тестовой выборкой, а другая часть - обучающей. Посчитав метрики на каждой итерации, мы сможем усреднить их в конце и получить достаточно точную оценку качества нашего алгоритма."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 15.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=LogisticRegression(max_iter=3000, random_state=13,\n",
       "                                          solver='liblinear'),\n",
       "             param_grid={'C': array([1.00000000e-05, 1.32571137e-05, 1.75751062e-05, 2.32995181e-05,\n",
       "       3.08884360e-05, 4.09491506e-05, 5.42867544e-05, 7.19685673e-05,\n",
       "       9.54095476e-05, 1.26485522e-04, 1.67683294e-04, 2.22299648e-04,\n",
       "       2.94705170e-04, 3.90693994e-04, 5.17947468e-04, 6.866...\n",
       "       2.68269580e-02, 3.55648031e-02, 4.71486636e-02, 6.25055193e-02,\n",
       "       8.28642773e-02, 1.09854114e-01, 1.45634848e-01, 1.93069773e-01,\n",
       "       2.55954792e-01, 3.39322177e-01, 4.49843267e-01, 5.96362332e-01,\n",
       "       7.90604321e-01, 1.04811313e+00, 1.38949549e+00, 1.84206997e+00,\n",
       "       2.44205309e+00, 3.23745754e+00, 4.29193426e+00, 5.68986603e+00,\n",
       "       7.54312006e+00, 1.00000000e+01]),\n",
       "                         'penalty': ['l1', 'l2']},\n",
       "             scoring='roc_auc')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "grid_searcher.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на результаты лучшей модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9802989021184475\n",
      "0.9604401789152522\n"
     ]
    }
   ],
   "source": [
    "print(roc_auc_score(y_train, grid_searcher.predict_proba(X_train)[:, 1]))\n",
    "print(roc_auc_score(y_test, grid_searcher.predict_proba(X_test)[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полные результаты поиска гиперпараметров:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_C</th>\n",
       "      <th>param_penalty</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.008593</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.002798</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>l1</td>\n",
       "      <td>{'C': 1e-05, 'penalty': 'l1'}</td>\n",
       "      <td>0.747449</td>\n",
       "      <td>0.766457</td>\n",
       "      <td>0.788754</td>\n",
       "      <td>0.773601</td>\n",
       "      <td>0.730134</td>\n",
       "      <td>0.761279</td>\n",
       "      <td>0.020469</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.013790</td>\n",
       "      <td>0.001937</td>\n",
       "      <td>0.002399</td>\n",
       "      <td>0.000490</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>l2</td>\n",
       "      <td>{'C': 1e-05, 'penalty': 'l2'}</td>\n",
       "      <td>0.853920</td>\n",
       "      <td>0.845400</td>\n",
       "      <td>0.847766</td>\n",
       "      <td>0.855969</td>\n",
       "      <td>0.819283</td>\n",
       "      <td>0.844467</td>\n",
       "      <td>0.013173</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.007796</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.002998</td>\n",
       "      <td>0.000632</td>\n",
       "      <td>1.32571e-05</td>\n",
       "      <td>l1</td>\n",
       "      <td>{'C': 1.3257113655901082e-05, 'penalty': 'l1'}</td>\n",
       "      <td>0.747449</td>\n",
       "      <td>0.766457</td>\n",
       "      <td>0.788754</td>\n",
       "      <td>0.773601</td>\n",
       "      <td>0.730134</td>\n",
       "      <td>0.761279</td>\n",
       "      <td>0.020469</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.011392</td>\n",
       "      <td>0.002937</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.000490</td>\n",
       "      <td>1.32571e-05</td>\n",
       "      <td>l2</td>\n",
       "      <td>{'C': 1.3257113655901082e-05, 'penalty': 'l2'}</td>\n",
       "      <td>0.859788</td>\n",
       "      <td>0.851979</td>\n",
       "      <td>0.854817</td>\n",
       "      <td>0.863584</td>\n",
       "      <td>0.825383</td>\n",
       "      <td>0.851110</td>\n",
       "      <td>0.013471</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.008796</td>\n",
       "      <td>0.000401</td>\n",
       "      <td>0.002797</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>1.75751e-05</td>\n",
       "      <td>l1</td>\n",
       "      <td>{'C': 1.757510624854793e-05, 'penalty': 'l1'}</td>\n",
       "      <td>0.747449</td>\n",
       "      <td>0.766457</td>\n",
       "      <td>0.788754</td>\n",
       "      <td>0.773601</td>\n",
       "      <td>0.730134</td>\n",
       "      <td>0.761279</td>\n",
       "      <td>0.020469</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.053544</td>\n",
       "      <td>0.011785</td>\n",
       "      <td>0.001778</td>\n",
       "      <td>0.000392</td>\n",
       "      <td>5.68987</td>\n",
       "      <td>l2</td>\n",
       "      <td>{'C': 5.689866029018293, 'penalty': 'l2'}</td>\n",
       "      <td>0.975437</td>\n",
       "      <td>0.979017</td>\n",
       "      <td>0.967458</td>\n",
       "      <td>0.971656</td>\n",
       "      <td>0.982612</td>\n",
       "      <td>0.975236</td>\n",
       "      <td>0.005330</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.019588</td>\n",
       "      <td>0.001020</td>\n",
       "      <td>0.002202</td>\n",
       "      <td>0.000747</td>\n",
       "      <td>7.54312</td>\n",
       "      <td>l1</td>\n",
       "      <td>{'C': 7.543120063354607, 'penalty': 'l1'}</td>\n",
       "      <td>0.973566</td>\n",
       "      <td>0.975143</td>\n",
       "      <td>0.968432</td>\n",
       "      <td>0.972739</td>\n",
       "      <td>0.982194</td>\n",
       "      <td>0.974415</td>\n",
       "      <td>0.004480</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.068169</td>\n",
       "      <td>0.018043</td>\n",
       "      <td>0.002598</td>\n",
       "      <td>0.000799</td>\n",
       "      <td>7.54312</td>\n",
       "      <td>l2</td>\n",
       "      <td>{'C': 7.543120063354607, 'penalty': 'l2'}</td>\n",
       "      <td>0.975638</td>\n",
       "      <td>0.978359</td>\n",
       "      <td>0.967435</td>\n",
       "      <td>0.971788</td>\n",
       "      <td>0.982503</td>\n",
       "      <td>0.975145</td>\n",
       "      <td>0.005205</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.017791</td>\n",
       "      <td>0.003248</td>\n",
       "      <td>0.001999</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>10</td>\n",
       "      <td>l1</td>\n",
       "      <td>{'C': 10.0, 'penalty': 'l1'}</td>\n",
       "      <td>0.973504</td>\n",
       "      <td>0.974733</td>\n",
       "      <td>0.968409</td>\n",
       "      <td>0.972708</td>\n",
       "      <td>0.982047</td>\n",
       "      <td>0.974280</td>\n",
       "      <td>0.004429</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.086053</td>\n",
       "      <td>0.025654</td>\n",
       "      <td>0.002996</td>\n",
       "      <td>0.001669</td>\n",
       "      <td>10</td>\n",
       "      <td>l2</td>\n",
       "      <td>{'C': 10.0, 'penalty': 'l2'}</td>\n",
       "      <td>0.975646</td>\n",
       "      <td>0.978143</td>\n",
       "      <td>0.967612</td>\n",
       "      <td>0.972050</td>\n",
       "      <td>0.982743</td>\n",
       "      <td>0.975239</td>\n",
       "      <td>0.005162</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time      param_C  \\\n",
       "0        0.008593      0.001359         0.002798        0.000400        1e-05   \n",
       "1        0.013790      0.001937         0.002399        0.000490        1e-05   \n",
       "2        0.007796      0.001600         0.002998        0.000632  1.32571e-05   \n",
       "3        0.011392      0.002937         0.002400        0.000490  1.32571e-05   \n",
       "4        0.008796      0.000401         0.002797        0.000400  1.75751e-05   \n",
       "..            ...           ...              ...             ...          ...   \n",
       "95       0.053544      0.011785         0.001778        0.000392      5.68987   \n",
       "96       0.019588      0.001020         0.002202        0.000747      7.54312   \n",
       "97       0.068169      0.018043         0.002598        0.000799      7.54312   \n",
       "98       0.017791      0.003248         0.001999        0.000001           10   \n",
       "99       0.086053      0.025654         0.002996        0.001669           10   \n",
       "\n",
       "   param_penalty                                          params  \\\n",
       "0             l1                   {'C': 1e-05, 'penalty': 'l1'}   \n",
       "1             l2                   {'C': 1e-05, 'penalty': 'l2'}   \n",
       "2             l1  {'C': 1.3257113655901082e-05, 'penalty': 'l1'}   \n",
       "3             l2  {'C': 1.3257113655901082e-05, 'penalty': 'l2'}   \n",
       "4             l1   {'C': 1.757510624854793e-05, 'penalty': 'l1'}   \n",
       "..           ...                                             ...   \n",
       "95            l2       {'C': 5.689866029018293, 'penalty': 'l2'}   \n",
       "96            l1       {'C': 7.543120063354607, 'penalty': 'l1'}   \n",
       "97            l2       {'C': 7.543120063354607, 'penalty': 'l2'}   \n",
       "98            l1                    {'C': 10.0, 'penalty': 'l1'}   \n",
       "99            l2                    {'C': 10.0, 'penalty': 'l2'}   \n",
       "\n",
       "    split0_test_score  split1_test_score  split2_test_score  \\\n",
       "0            0.747449           0.766457           0.788754   \n",
       "1            0.853920           0.845400           0.847766   \n",
       "2            0.747449           0.766457           0.788754   \n",
       "3            0.859788           0.851979           0.854817   \n",
       "4            0.747449           0.766457           0.788754   \n",
       "..                ...                ...                ...   \n",
       "95           0.975437           0.979017           0.967458   \n",
       "96           0.973566           0.975143           0.968432   \n",
       "97           0.975638           0.978359           0.967435   \n",
       "98           0.973504           0.974733           0.968409   \n",
       "99           0.975646           0.978143           0.967612   \n",
       "\n",
       "    split3_test_score  split4_test_score  mean_test_score  std_test_score  \\\n",
       "0            0.773601           0.730134         0.761279        0.020469   \n",
       "1            0.855969           0.819283         0.844467        0.013173   \n",
       "2            0.773601           0.730134         0.761279        0.020469   \n",
       "3            0.863584           0.825383         0.851110        0.013471   \n",
       "4            0.773601           0.730134         0.761279        0.020469   \n",
       "..                ...                ...              ...             ...   \n",
       "95           0.971656           0.982612         0.975236        0.005330   \n",
       "96           0.972739           0.982194         0.974415        0.004480   \n",
       "97           0.971788           0.982503         0.975145        0.005205   \n",
       "98           0.972708           0.982047         0.974280        0.004429   \n",
       "99           0.972050           0.982743         0.975239        0.005162   \n",
       "\n",
       "    rank_test_score  \n",
       "0                97  \n",
       "1                82  \n",
       "2                97  \n",
       "3                81  \n",
       "4                97  \n",
       "..              ...  \n",
       "95                8  \n",
       "96               24  \n",
       "97               11  \n",
       "98               25  \n",
       "99                7  \n",
       "\n",
       "[100 rows x 15 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(grid_searcher.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучшие гиперпараметры:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_searcher.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучший скор модели на кросс-валидации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_searcher.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы также можем выделить лучшую модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = grid_searcher.best_estimator_\n",
    "lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценку модели на кросс-валидации мы можем получить и без перебора гиперпараметров:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cv_score = cross_val_score(lr, X_train, y_train, scoring='roc_auc', cv=5)\n",
    "print(cv_score)\n",
    "print(cv_score.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вместо перебора по сетке можно перебирать гиперпараметры, сгенерированные из заданного распределения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import uniform\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "lr = LogisticRegression(max_iter=3000, solver='liblinear', random_state=13)\n",
    "distributions = dict(C=uniform(loc=0, scale=10),\n",
    "                     penalty=['l1', 'l2'])\n",
    "clf = RandomizedSearchCV(lr, distributions, n_iter=50, cv=5, scoring='roc_auc', random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(roc_auc_score(y_train, clf.predict_proba(X_train)[:, 1]))\n",
    "print(roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для некоторых моделей из `sklearn` можно сразу применить кросс-валидацию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "lr = LogisticRegressionCV(max_iter=3000, solver='lbfgs', cv=5, random_state=13)\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.C_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(roc_auc_score(y_train, lr.predict_proba(X_train)[:, 1]))\n",
    "print(roc_auc_score(y_test, lr.predict_proba(X_test)[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM\n",
    "\n",
    "Рассмотрим теперь другой подход к построению функции потерь,\n",
    "основанный на максимизации зазора между классами.\n",
    "Будем рассматривать линейные классификаторы вида\n",
    "$$\n",
    "    a(x) = sign (\\langle w, x \\rangle + b), \\qquad w \\in R^d, b \\in R.\n",
    "$$\n",
    "\n",
    "### Разделимый случай\n",
    "Будем считать, что существуют такие параметры $w_*$ и $b_*$,\n",
    "что соответствующий им классификатор $a(x)$ не допускает ни одной ошибки\n",
    "на обучающей выборке.\n",
    "В этом случае говорят, что выборка __линейно разделима__.\n",
    "\n",
    "Пусть задан некоторый классификатор $a(x) = sign (\\langle w, x \\rangle + b)$.\n",
    "Заметим, что если одновременно умножить параметры $w$ и $b$\n",
    "на одну и ту же положительную константу,\n",
    "то классификатор не изменится.\n",
    "Распорядимся этой свободой выбора и отнормируем параметры так, что\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{eq:svmNormCond}\n",
    "    \\min_{x \\in X} | \\langle w, x \\rangle + b| = 1.\n",
    "\\end{equation}\n",
    "\n",
    "Можно показать, что расстояние от произвольной точки $x_0 \\in R^d$ до гиперплоскости,\n",
    "определяемой данным классификатором, равно\n",
    "\n",
    "$$\n",
    "    \\rho(x_0, a)\n",
    "    =\n",
    "    \\frac{\n",
    "        |\\langle w, x \\rangle + b|\n",
    "    }{\n",
    "        \\|w\\|\n",
    "    }.\n",
    "$$\n",
    "\n",
    "Тогда расстояние от гиперплоскости до ближайшего объекта обучающей выборки равно\n",
    "\n",
    "$$\n",
    "    \\min_{x \\in X}\n",
    "    \\frac{\n",
    "        |\\langle w, x \\rangle + b|\n",
    "    }{\n",
    "        \\|w\\|\n",
    "    }\n",
    "    =\n",
    "    \\frac{1}{\\|w\\|} \\min_{x \\in X} |\\langle w, x \\rangle + b|\n",
    "    =\n",
    "    \\frac{1}{\\|w\\|}.\n",
    "$$\n",
    "\n",
    "Данная величина также называется __отступом (margin)__.\n",
    "\n",
    "Таким образом, если классификатор без ошибок разделяет обучающую выборку,\n",
    "то ширина его разделяющей полосы равна $\\frac{2}{\\|w\\|}$.\n",
    "Известно, что максимизация ширины разделяющей полосы приводит\n",
    "к повышению обобщающей способности классификатора.\n",
    "Вспомним также, что на повышение обобщающей способности направлена и регуляризация,\n",
    "которая штрафует большую норму весов -- а чем больше норма весов,\n",
    "тем меньше ширина разделяющей полосы.\n",
    "\n",
    "Итак, требуется построить классификатор, идеально разделяющий обучающую выборку,\n",
    "и при этом имеющий максимальный отступ.\n",
    "Запишем соответствующую оптимизационную задачу,\n",
    "которая и будет определять метод опорных векторов для линейно разделимой выборки (hard margin support vector machine):\n",
    "\\begin{equation}\n",
    "\\label{eq:svmSep}\n",
    "    \\left\\{\n",
    "        \\begin{aligned}\n",
    "            & \\frac{1}{2} \\|w\\|^2 \\to \\min_{w, b} \\\\\n",
    "            & y_i \\left(\n",
    "                \\langle w, x_i \\rangle + b\n",
    "            \\right) \\geq 1, \\quad i = 1, \\dots, \\ell.\n",
    "        \\end{aligned}\n",
    "    \\right.\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "### Неразделимый случай\n",
    "Рассмотрим теперь общий случай, когда выборку\n",
    "невозможно идеально разделить гиперплоскостью.\n",
    "Это означает, что какие бы $w$ и $b$ мы не взяли,\n",
    "хотя бы одно из ограничений в предыдущей задаче будет нарушено:\n",
    "\n",
    "$$\n",
    "    \\exists x_i \\in X:\\\n",
    "    y_i \\left(\n",
    "        \\langle w, x_i \\rangle + b\n",
    "    \\right) < 1.\n",
    "$$\n",
    "\n",
    "Сделаем эти ограничения \"мягкими\", введя штраф $\\xi_i \\geq 0$ за их нарушение:\n",
    "\n",
    "$$\n",
    "    y_i \\left(\n",
    "        \\langle w, x_i \\rangle + b\n",
    "    \\right) \\geq 1 - \\xi_i, \\quad i = 1, \\dots, \\ell.\n",
    "$$\n",
    "\n",
    "Отметим, что если отступ объекта лежит между нулем и\n",
    "единицей ($0 \\leq y_i \\left( \\langle w, x_i \\rangle + b \\right) < 1$),\n",
    "то объект верно классифицируется, но имеет ненулевой штраф $\\xi > 0$.\n",
    "Таким образом, мы штрафуем объекты за попадание внутрь разделяющей полосы.\n",
    "\n",
    "Величина $\\frac{1}{\\|w\\|}$ в данном случае называется мягким отступом (soft margin).\n",
    "С одной стороны, мы хотим максимизировать отступ, с другой -- минимизировать\n",
    "штраф за неидеальное разделение выборки $\\sum_{i = 1}^{\\ell} \\xi_i$.\n",
    "Эти две задачи противоречат друг другу: как правило, излишняя подгонка под\n",
    "выборку приводит к маленькому отступу, и наоборот -- максимизация отступа\n",
    "приводит к большой ошибке на обучении.\n",
    "В качестве компромисса будем минимизировать взвешенную сумму двух указанных величин.\n",
    "Приходим к оптимизационной задаче,\n",
    "соответствующей методу опорных векторов для линейно неразделимой выборки (soft margin support vector machine)\n",
    "\\begin{equation}\n",
    "\\label{eq:svmUnsep}\n",
    "    \\left\\{\n",
    "        \\begin{aligned}\n",
    "            & \\frac{1}{2} \\|w\\|^2 + C \\sum_{i = 1}^{\\ell} \\xi_i \\to \\min_{w, b, \\xi} \\\\\n",
    "            & y_i \\left(\n",
    "                \\langle w, x_i \\rangle + b\n",
    "            \\right) \\geq 1 - \\xi_i, \\quad i = 1, \\dots, \\ell, \\\\\n",
    "            & \\xi_i \\geq 0, \\quad i = 1, \\dots, \\ell.\n",
    "        \\end{aligned}\n",
    "    \\right.\n",
    "\\end{equation}\n",
    "Чем больше здесь параметр $C$, тем сильнее мы будем настраиваться на обучающую выборку."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Исследуем зависимость положения разделяющей гиперплоскости в методе опорных векторов в зависимости от значения гиперпараметра $C$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сгенерируем двумерную искуственную выборку из двух различных нормальных распределений:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_size=500\n",
    "\n",
    "mean0 = [7, 5]\n",
    "cov0 = [[4, 0], [0, 1]]  # diagonal covariance\n",
    "mean1 = [0, 0]\n",
    "cov1 = [[4, 0], [0, 2]]\n",
    "data0 = np.random.multivariate_normal(mean0, cov0, class_size)\n",
    "data1 = np.random.multivariate_normal(mean1, cov1, class_size)\n",
    "data = np.vstack((data0, data1))\n",
    "y = np.hstack((-np.ones(class_size), np.ones(class_size)))\n",
    "\n",
    "plt.scatter(data0[:, 0], data0[:, 1], c='red', s=50)\n",
    "plt.scatter(data1[:, 0], data1[:, 1], c='green', s=50)\n",
    "plt.legend(['y = -1', 'y = 1'])\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-5,15])\n",
    "axes.set_ylim([-5,10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "SVM_classifier = SVC(C=0.01, kernel='linear') # changing C here\n",
    "SVM_classifier.fit(data, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_1 = SVM_classifier.coef_[0][0]\n",
    "w_2 = SVM_classifier.coef_[0][1]\n",
    "w_0 = SVM_classifier.intercept_[0]\n",
    "\n",
    "plt.scatter(data0[:, 0], data0[:, 1], c='red', s=50)\n",
    "plt.scatter(data1[:, 0], data1[:, 1], c='green', s=50)\n",
    "plt.legend(['y = -1', 'y = 1'])\n",
    "x_arr = np.linspace(-10, 15, 3000)\n",
    "plt.plot(x_arr, -(w_0 + w_1 * x_arr) / w_2)\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-5,15])\n",
    "axes.set_ylim([-5,10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data0[:, 0], data0[:, 1], c='red', s=50, label='y = -1')\n",
    "plt.scatter(data1[:, 0], data1[:, 1], c='green', s=50, label='y = +1')\n",
    "#plt.legend(['y = -1', 'y = 1'])\n",
    "x_arr = np.linspace(-10, 15, 3000)\n",
    "colors = ['red', 'orange', 'green', 'blue', 'magenta']\n",
    "\n",
    "for i, C in enumerate([0.0001, 0.01,  1, 100, 10000]):\n",
    "    SVM_classifier = SVC(C=C, kernel='linear')\n",
    "    SVM_classifier.fit(data, y)\n",
    "    w_1 = SVM_classifier.coef_[0][0]\n",
    "    w_2 = SVM_classifier.coef_[0][1]\n",
    "    w_0 = SVM_classifier.intercept_[0]\n",
    "    plt.plot(x_arr, -(w_0 + w_1 * x_arr) / w_2, color=colors[i], label='C='+str(C))\n",
    "\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-5,15])\n",
    "axes.set_ylim([-5,10])\n",
    "plt.legend(loc=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Гиперпараметр $C$ отвечает за то, что является более приоритетным для классификатора, — \"подгонка\" под обучающую выборку или максимизация ширины разделяющей полосы.\n",
    " - При больших значениях $C$ классификатор сильно настраивается на обучение, тем самым сужая разделяющую полосу.\n",
    " - При маленьких значениях $C$ классификатор расширяет разделяющую полосу, при этом допуская ошибки на некоторых объектах обучающей выборки."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
