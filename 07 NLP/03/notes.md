# заметки по семинару и ДЗ

## по домашнему заданию

Привет! Запоздало, но обещанные материалы про CNN на текстах:

на русском языке:

1. из (<https://openedu.ru/course/hse/TEXT/>) курса (<https://openedu.ru/course/hse/TEXT/>) по анализу текстовых данных, лекция (<https://courses.openedu.ru/courses/course-v1:hse+TEXT+fall_2020/courseware/a947c504d30949828ef013535d677634/8c57b76fb1054f18aa3119c5f17a10cf/1?activate_block_id=block-v1%3Ahse%2BTEXT%2Bfall_2020%2Btype%40vertical%2Bblock%400fa6b8d4c55a4d5fb1f188065733449a>) и семинар (<https://courses.openedu.ru/courses/course-v1:hse+TEXT+fall_2020/courseware/a947c504d30949828ef013535d677634/7c7fb63f763840f783f5a0ec993283e9/1?activate_block_id=block-v1%3Ahse%2BTEXT%2Bfall_2020%2Btype%40vertical%2Bblock%4096dba3daf59f484cb4394356cc6a2022>) про CNN (курс и доступ к материалам там бесплатный, но там нужно будет зарегистрироваться на платформе)

на английском языке:

1. кусочек про CNN в курсе Лены Войта (<https://lena-voita.github.io/nlp_course/text_classification.html#nn_models_cnn>)
2. лекция про CNN для текстов от коллег из Вышки, нашла в открытом доступе на youtube (<https://www.youtube.com/watch?v=wNBaNhvL4pg>), но лучше обратиться к первоисточнику на курсере (<https://ru.coursera.org/learn/language-processing?specialization=aml>)

## Лекция

Языковые модели - вероятностная конструкция, которая сравнивает вероятность фраз $$ P(дождь | на\_улице\_идет ) > P(концерт | на\_улице\_идет )$$

$P(W)$  - вероятность слова, при условии контекста

в простом случае - можно посчитать вероятности на большом корпусе слова.

### марковская модель

дано словао и его предыстория

ограничим длину предыстории и будем оакном ехать по тексту

P(на улице идет дождь) (модель униграм) = P(на)P(улице | на)P(идет| улице) P(дождь | идет)

Можно статисчтически оценить вероятности этих штук.

Проблемы:

* вероятность может быть равна 0 (можно добавить сглаживание, чтобы избежать деление на 0)

Можно это использоват для генерации предложений

### нейросетевые языковые модели

по n-1 слову историческому предсказать n-е слово.

взяли контекст и выдаем вероятности для всего корпуса (P каждого слова)

**Для решения этого** - рекурентные нейронные сети

$h_t = \sigma(Vh_{t-1} +Ux_t +b_1)$ - обновление вектора памяти

$\hat{Y_t} = softmax(Wh_t + b_2) \in \R^{|V|}$ - предсказание

Мы старые токены засовывае в h, он зависим от самого себя на прошом шаге.

### двунаправленные рекурентные нейронные сети

они смотрят текст вперед и назад

#### **Плюс**

* входы переменной длины
* механизм памяти
* количество параметров не напряму зависит от размера корпуса

#### **Минусы**

* сложно производную считать
* механизм памяти практически не работает

### продвинутые архитектуры

если простая задача с не очень большим контекстом (до 200 слов) - РНН можно - для коротких

* LSTM - хранят и долгосрочную и краткосроыную память - если последовательности длинные
* GRU (Gated recurrent unit) - в какой-то момент немного забывают - для средних

## извлечение именованных сущностей

sequence labelling - это же тип задачи и part of speech tagging - нужно чтобы каждому токену в тексте назначался некий класс

CNN-biLSTM-CRF

1. вектора слова -  это данные или BOW или предобученные модели эмбеддингов
2. плюс к ним символьные представления
3. канкатеннируем их 1 и 2
4. CNN для них
5. Прямая и обратная LSTM
6. CRF - conditional random field - продвинутый вариант на основе идей марковских сетей (как марковская последовательность, но больше связей) и она выдает уже метки POS (Part of speech)
