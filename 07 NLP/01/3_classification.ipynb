{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "3_classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6ccN1AlFhNo"
      },
      "source": [
        "## Классификация текста с использованием FastText и CNN\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1U3vnZeD8aiDg5Gh-SjnEyJyfrTHSRTkB)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJ8_zAI8FhNp"
      },
      "source": [
        "Используя данные отзывов IMDB, построим CNN для классификации документов на позитивный и негативный классы.\n",
        "\n",
        "Источник изложения: https://github.com/bentrevett/pytorch-sentiment-analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHKLq9hEFhNr"
      },
      "source": [
        "В предположении, что PyTorch уже установлен, поставим дополнительные модули и загрузим модель для токенизации:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgGxeuQjG9NI"
      },
      "source": [
        "# !pip3 install https://download.pytorch.org/whl/cpu/torch-1.0.1.post2-cp36-cp36m-linux_x86_64.whl"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kiZro9eG-bG",
        "outputId": "1bd1ea6b-9453-44f3-96c4-4437ac121ce9"
      },
      "source": [
        "!pip install torchvision"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.10.0+cu111)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.19.5)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: torch==1.9.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.9.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0->torchvision) (3.7.4.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-WRgs-VFhNt",
        "outputId": "f664a9aa-12bb-4a32-e858-884671c3f25d"
      },
      "source": [
        "!pip install torchtext"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.7/dist-packages (0.10.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext) (4.62.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext) (2.23.0)\n",
            "Requirement already satisfied: torch==1.9.0 in /usr/local/lib/python3.7/dist-packages (from torchtext) (1.9.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0->torchtext) (3.7.4.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (2.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pB7uZ5L7FhN3"
      },
      "source": [
        "# !pip3 install spacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLGeXcUjFhN8"
      },
      "source": [
        "#!python3.6 -m spacy download en\n",
        "# !python3 -m spacy download en_core_web_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaPk16PLmgnw"
      },
      "source": [
        "# import spacy\n",
        "#import en\n",
        "# en_nlp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0RJ6nnay9Sf",
        "outputId": "3d65fce3-6644-4817-ff5f-b1a22ea29e0f"
      },
      "source": [
        "! pip3 install fasttext"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[?25l\r\u001b[K     |████▊                           | 10 kB 32.1 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 20 kB 9.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 30 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 40 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 51 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 61 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 68 kB 3.0 MB/s \n",
            "\u001b[?25hCollecting pybind11>=2.2\n",
            "  Using cached pybind11-2.8.0-py2.py3-none-any.whl (207 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext) (57.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fasttext) (1.19.5)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp37-cp37m-linux_x86_64.whl size=3119048 sha256=99c27b6687c243313bf9457dbe999f6c879a389e556fa87ce5c58caa5ebb9ef1\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/ca/bf/b020d2be95f7641801a6597a29c8f4f19e38f9c02a345bab9b\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLGAJBxfFhN_"
      },
      "source": [
        "Загрузим датасет и получим из него выборку:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zclld6vmaH2"
      },
      "source": [
        "import torch"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4Yq-LOGmoOn",
        "outputId": "a999c813-7ba7-4edb-8e35-01a84a7e7516"
      },
      "source": [
        "print(torch.__version__)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.9.0+cu111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZlniJuAmp8F"
      },
      "source": [
        "SEED = 0\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BObMLa5mvQYM"
      },
      "source": [
        "### Данные"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7w84GaXumr0q",
        "outputId": "b483af83-0979-478e-ac8d-6df655035845"
      },
      "source": [
        "!wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=17uuANm7Q1CunXHfTaF7IRY9Vy7qPl5_L' -O imdb.csv"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-23 15:35:48--  https://drive.google.com/uc?export=download&id=17uuANm7Q1CunXHfTaF7IRY9Vy7qPl5_L\n",
            "Resolving drive.google.com (drive.google.com)... 64.233.189.101, 64.233.189.113, 64.233.189.102, ...\n",
            "Connecting to drive.google.com (drive.google.com)|64.233.189.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0c-44-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/49i9qovp5crofo4vdo1jk65ej3nhqllr/1635003300000/13414369628864094336/*/17uuANm7Q1CunXHfTaF7IRY9Vy7qPl5_L?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2021-10-23 15:35:52--  https://doc-0c-44-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/49i9qovp5crofo4vdo1jk65ej3nhqllr/1635003300000/13414369628864094336/*/17uuANm7Q1CunXHfTaF7IRY9Vy7qPl5_L?e=download\n",
            "Resolving doc-0c-44-docs.googleusercontent.com (doc-0c-44-docs.googleusercontent.com)... 74.125.203.132, 2404:6800:4008:c01::84\n",
            "Connecting to doc-0c-44-docs.googleusercontent.com (doc-0c-44-docs.googleusercontent.com)|74.125.203.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 66212309 (63M) [text/csv]\n",
            "Saving to: ‘imdb.csv’\n",
            "\n",
            "imdb.csv            100%[===================>]  63.14M  81.9MB/s    in 0.8s    \n",
            "\n",
            "2021-10-23 15:35:54 (81.9 MB/s) - ‘imdb.csv’ saved [66212309/66212309]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pG25VDn1vdNn"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "qedrhUuqyaX2",
        "outputId": "a456e4eb-a305-4e75-a3f9-28b00bbb3163"
      },
      "source": [
        "df = pd.read_csv('imdb.csv')\n",
        "df.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review sentiment\n",
              "0  One of the other reviewers has mentioned that ...  positive\n",
              "1  A wonderful little production. <br /><br />The...  positive\n",
              "2  I thought this was a wonderful way to spend ti...  positive\n",
              "3  Basically there's a family where a little boy ...  negative\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6j9lHpqyaX3"
      },
      "source": [
        "train, test = train_test_split(df, test_size=0.3)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRKOKoPayaX3"
      },
      "source": [
        "## FastText\n",
        "\n",
        "Чтобы обучить FastText как классифкатор, необходимо сперва записать данные в файл в следующем формате:"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "z21qL2_syaX3"
      },
      "source": [
        "__label__0 some text\n",
        "__label__1 another text\n",
        "__label__0 the last text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTn-FKXfyaX4"
      },
      "source": [
        "import fasttext"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lx8WRlYcyaX4"
      },
      "source": [
        "with open('ft_train_data.txt', 'w') as f:\n",
        "    for pair in list(zip(train['review'], train['sentiment'])):\n",
        "        text, label = pair\n",
        "        f.write(f'__label__{label} {text.lower()}\\n')\n",
        "        \n",
        "with open('ft_test_data.txt', 'w') as f:\n",
        "    for pair in list(zip(test['review'], test['sentiment'])):\n",
        "        text, label = pair\n",
        "        f.write(f'__label__{label} {text.lower()}\\n')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faYKzEJwyaX4",
        "outputId": "8f95304d-2a04-4554-fa4e-93a9c8fc3265"
      },
      "source": [
        "! head -n 3 ft_train_data.txt"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "__label__positive super speedway makes a great demo of your new dvd / bigscreen / 5.1 channel sound system. the imax camera puts you right in the race car, where you cruise around various tracks at high speed, reminiscent of the driving sequences in grand prix (if only that would appear on dvd!). i enjoy watching it again and again.<br /><br />the only minus, and why i didn't give it a 10, is some of the driving sequences look suspiciously like the film was speeded up. the soundtrack also requires a little suspension of disbelief - all you can hear in a real car is the engine. you won't hear swooshes as you go under bridges.\n",
            "__label__negative this is not an entirely bad movie. the plot (new house built next door seems to be haunted) is not bad, the mood is creepy enough, and the acting is okay. the big problem i had is that, being familiar with lara flynn boyle (from twin peaks and other shows), i couldn't get over how different she looks with her apparently new, big lips. i kept staring at them. they look so out of place on her face! they make her look completely different (and not better).<br /><br />mark-paul gosselaar, the actor who plays kim the architect who designs and pours his heart and soul into the house, does a fine job. and lara (as col) is also quite good (but those lips!) as the owner of the house next door. her husband, walker (colin ferguson) is appropriately wooden. the various characters who live in the house were also fine. i particularly liked pie (charlotte sullivan) and her husband, buddy (stephen amell), the first people to move into the house. the attempt to involve us in the overall neighborhood vibe fails, unfortunately, as the other neighbors are not particularly likable.<br /><br />for some reason the director was unable to make the \"haunted\" house particularly ominous. other movies (such as amityville horror, the legend of hell house) manage to achieve that spooky feel, but it just doesn't happen here. the closest is when col paints a depiction of the house.<br /><br />another thing that didn't work for me is the plot twist that occurs with kim, the architect. initially, he appears to be a victim of the house like the others (it has sucked him dry of inspiration), but later he seems to have joined forces with it in evil.<br /><br />overall, not a bad movie for horror fans if you can take your eyes off those big lips!\n",
            "__label__positive i wish \"that '70s show\" would come back on television. it was the greatest show ever!!! they should make episodes between the other episodes but of course that would be confusing. but i wish it would come back and make more episodes. please come back... the show was absolutely hilarious. you couldn't laugh without seeing an episode. there is a really funny part in every episode and plus the show was so much better when hyde and jackie were going out with each other. those were the best episodes. \"that '70s show is the best\".... it will be and always will be the best show ever. it was really sad when the show ended. they should make new episodes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FY8bD8mKyaX4"
      },
      "source": [
        "Теперь можем обучить модель:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L84pCJbPyaX5",
        "outputId": "f72d44c4-fd28-400e-d044-fd00a72ca352"
      },
      "source": [
        "classifier = fasttext.train_supervised('ft_train_data.txt')#, 'model')\n",
        "result = classifier.test('ft_test_data.txt')\n",
        "print('P@1:', result[1])#.precision)\n",
        "print('R@1:', result[2])#.recall)\n",
        "print('Number of examples:', result[0])#.nexamples)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P@1: 0.8788666666666667\n",
            "R@1: 0.8788666666666667\n",
            "Number of examples: 15000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJJnMdVAyaX5"
      },
      "source": [
        "Так можно получить сами предсказания модели:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORMmyUbayaX5",
        "outputId": "acda3146-8c0e-488f-ba80-c18483029198"
      },
      "source": [
        "pred = classifier.predict(list(test['review']))[0]\n",
        "\n",
        "pred[:10]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['__label__negative'],\n",
              " ['__label__negative'],\n",
              " ['__label__negative'],\n",
              " ['__label__positive'],\n",
              " ['__label__negative'],\n",
              " ['__label__negative'],\n",
              " ['__label__negative'],\n",
              " ['__label__positive'],\n",
              " ['__label__negative'],\n",
              " ['__label__negative']]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-u-Zp-5yaX5",
        "outputId": "2f929a21-d301-47ad-f198-7e9055e013e4"
      },
      "source": [
        "pred = [label[0].split('__')[-1] for label in pred]\n",
        "\n",
        "pred[:10]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['negative',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'positive',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'positive',\n",
              " 'negative',\n",
              " 'negative']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WAUUMcixIRP"
      },
      "source": [
        "## СNN\n",
        "\n",
        "### Записываем данные в удобные структуры"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yT8eYs4dxWW1"
      },
      "source": [
        "from torchtext.legacy import data"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ck6B9K1ayX7A"
      },
      "source": [
        "# классы Field и LabelField отвечают за то, как данные будут храниться и обрабатываться при считывании\n",
        "TEXT = data.Field(tokenize='spacy') # spacy -- значит, токенизацию будет делать модуль \n",
        "LABEL = data.LabelField()\n",
        "\n",
        "ds = data.TabularDataset(\n",
        "  path='imdb.csv', format='csv',\n",
        "  skip_header=True,\n",
        "  fields=[('text', TEXT),\n",
        "        ('label', LABEL)]\n",
        ")"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdpfQN9qOu0p"
      },
      "source": [
        "ds - dataset - хранит итераторы по нашим данным и меткам класса.\n",
        "\n",
        "**NB**: неважно как столбцы назывались в исходном датасете, здесь за названия отвечают строки, которые мы передали аргументу `fields`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8-CSOf1xHuV",
        "outputId": "66cd51e4-4308-4cdd-9140-65fd25f268ef"
      },
      "source": [
        "next(ds.text)[:10]"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['One',\n",
              " 'of',\n",
              " 'the',\n",
              " 'other',\n",
              " 'reviewers',\n",
              " 'has',\n",
              " 'mentioned',\n",
              " 'that',\n",
              " 'after',\n",
              " 'watching']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "e9h0nnsi0Q0-",
        "outputId": "36757778-08f1-4475-97df-cf5f1fd8dd4e"
      },
      "source": [
        "next(ds.label)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'positive'"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECxpxH2m9VY8"
      },
      "source": [
        "Строим словарь и загружаем эмбеддинги.\n",
        "\n",
        "С учётом того, что в коллекции 100К уникальных слов, и векторы получатся достаточно громоздкие, урежем коллекцию до 25К слов, для всех прочих заведя токен unk (unknown).\n",
        "\n",
        "У torchtext есть репозиторий, где хранятся некоторые словарные эмбеддинги для английского. `vectors=\"glove.6B.100d\"` значит, что крооме построения индекса слов корпуса, мы скачаем и сохраним вектора glove из этого репозитория."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9K_LTwEn78n5",
        "outputId": "d3c6c55c-87d0-49d4-91b1-546818b7e337"
      },
      "source": [
        "TEXT.build_vocab(ds, max_size=25000, vectors=\"glove.6B.100d\")\n",
        "LABEL.build_vocab(ds)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [02:42, 5.30MB/s]                           \n",
            "100%|█████████▉| 399999/400000 [00:21<00:00, 18451.10it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9omo_uS9P7V",
        "outputId": "324e92f2-30af-41d7-b1cb-67d63ce658cc"
      },
      "source": [
        "# itos == i to s == index to string\n",
        "print(TEXT.vocab.itos[:20])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<unk>', '<pad>', 'the', ',', '.', 'a', 'and', 'of', 'to', 'is', 'in', 'I', 'it', 'that', '\"', \"'s\", 'this', '-', '/><br', 'was']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNFCBfJZa98r",
        "outputId": "a6b6ed32-80e8-4b12-d088-e3b545c178ae"
      },
      "source": [
        "TEXT.vocab.itos[:20]"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<unk>',\n",
              " '<pad>',\n",
              " 'the',\n",
              " ',',\n",
              " '.',\n",
              " 'a',\n",
              " 'and',\n",
              " 'of',\n",
              " 'to',\n",
              " 'is',\n",
              " 'in',\n",
              " 'I',\n",
              " 'it',\n",
              " 'that',\n",
              " '\"',\n",
              " \"'s\",\n",
              " 'this',\n",
              " '-',\n",
              " '/><br',\n",
              " 'was']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZRu4ykFpz_q",
        "outputId": "2d5f70d3-6013-447f-80f9-1a279984bee2"
      },
      "source": [
        "# stoi == s to i == string to index\n",
        "TEXT.vocab.stoi[42]"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJl0ZOPBElSE"
      },
      "source": [
        "Разобьём обучающий сет на обучение, валидацию для настройки параметров и тест."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nuZQIq1FhOl"
      },
      "source": [
        "train, val = ds.split() # дефолтное соотношение 0.7\n",
        "val, test = val.split(split_ratio=0.5)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4_u1nkyS740",
        "outputId": "b398c830-da19-4661-8916-be068c8c91c5"
      },
      "source": [
        "print(len(train))\n",
        "print(len(val))\n",
        "print(len(test))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "35000\n",
            "7500\n",
            "7500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eT-lY0stTbWo"
      },
      "source": [
        "А теперь создадим итераторы батчей:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIJTX3ypTbrK"
      },
      "source": [
        "BATCH_SIZE  = 64\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train, val, test), \n",
        "    batch_size=BATCH_SIZE, \n",
        "    sort=True,\n",
        "    sort_key=lambda x: len(x.text), # сорируем тексты по длине, чтобы рядом оказывались предложения с одинаковой длиной и добавлялось меньше паддинга\n",
        "    repeat=False)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JtvOuVbT3i7"
      },
      "source": [
        "Заглянем внутрь батча"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQuNMEB5Tgmz",
        "outputId": "55faca6d-33a1-4bbf-d2b2-08f03a6235e2"
      },
      "source": [
        "for i, batch in enumerate(test_iterator):\n",
        "  print(batch.batch_size)\n",
        "  # pass"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kE9EDfGT_Gr",
        "outputId": "30db62ff-4178-421c-c200-ecf247b32fc8"
      },
      "source": [
        "batch.fields"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['text', 'label'])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Kvf2Hf2UEtR",
        "outputId": "24f5f02e-7705-4cba-8219-14cc955cc22b"
      },
      "source": [
        "batch.batch_size"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1oLPN8-TuA_",
        "outputId": "88a54f89-2a5c-4dee-f754-9ce1a7e84c3f"
      },
      "source": [
        "batch.text"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 3100,    29,   149,  ...,    66,    11,   580],\n",
              "        [   10,   596,   149,  ...,     9,    19,   761],\n",
              "        [    2,  2882, 22717,  ...,    37,  1088,     2],\n",
              "        ...,\n",
              "        [    2,     1,     1,  ...,     1,     1,     1],\n",
              "        [  235,     1,     1,  ...,     1,     1,     1],\n",
              "        [    4,     1,     1,  ...,     1,     1,     1]])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_TjGTb0UHDE",
        "outputId": "59bd9edf-1f3e-408c-f5e4-203fdc849dc1"
      },
      "source": [
        "batch.label"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7i8JbEnULFC"
      },
      "source": [
        "## Обучение"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTPx1IJ2SlXf"
      },
      "source": [
        "### Модель"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRr8D0t3Sl1a"
      },
      "source": [
        "import torch.nn as nn"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHEDOjrAFhP5"
      },
      "source": [
        "Для создания свёрточного слоя воспользуемся nn.Conv2d, in_channels в нашем случае один (текст), out_channels -- это число число фильтров и размер ядер всех фильтров. Каждый фильтр будет иметь размерность [n x размерность эмбеддинга], где n - размер обрабатываемой n-граммы.\n",
        "\n",
        "Важно, что предложения имели длину не меньше размера самого большого из используемых фильтров (здесь это не страшно, поскольку в используемых данных нет текстов, состоящих из пяти и менее слов)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yb5g3czaFhP6"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout_proba):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.conv_0 = nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(filter_sizes[0], embedding_dim))\n",
        "        self.conv_1 = nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(filter_sizes[1], embedding_dim))\n",
        "        self.conv_2 = nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(filter_sizes[2], embedding_dim))\n",
        "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout_proba)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        #x = [sent len, batch size]\n",
        "        x = x.permute(1, 0)\n",
        "                \n",
        "        #x = [batch size, sent len]\n",
        "        embedded = self.embedding(x)\n",
        "        #print(embedded.shape)\n",
        "                \n",
        "        #embedded = [batch size, sent len, emb dim]\n",
        "        embedded = embedded.unsqueeze(1)\n",
        "        \n",
        "        #embedded = [batch size, 1, sent len, emb dim]\n",
        "        conv_0 = self.conv_0(embedded)\n",
        "        #print(conv_0.shape)\n",
        "        conv_0 = conv_0.squeeze(3)\n",
        "        #print(conv_0.shape)\n",
        "        conved_0 = F.relu(conv_0)\n",
        "        conved_1 = F.relu(self.conv_1(embedded).squeeze(3))\n",
        "        conved_2 = F.relu(self.conv_2(embedded).squeeze(3))\n",
        "            \n",
        "        #conv_n = [batch size, n_filters, sent len - filter_sizes[n]]\n",
        "        #print(conved_0.shape)\n",
        "        pool_0 = F.max_pool1d(conved_0, conved_0.shape[2])\n",
        "        #print(pool_0.shape)\n",
        "\n",
        "        pooled_0 = pool_0.squeeze(2)\n",
        "        #print(pooled_0.shape)\n",
        "        pooled_1 = F.max_pool1d(conved_1, conved_1.shape[2]).squeeze(2)\n",
        "        pooled_2 = F.max_pool1d(conved_2, conved_2.shape[2]).squeeze(2)\n",
        "        \n",
        "        #pooled_n = [batch size, n_filters]\n",
        "        cat = self.dropout(torch.cat((pooled_0, pooled_1, pooled_2), dim=1))\n",
        "\n",
        "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
        "        return self.fc(cat)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_xPva9zFhP8"
      },
      "source": [
        "Сейчас мы можем использовать только три различных фильтра, хотелось бы больше. Вообще, можно воспользоваться `nn.ModuleList`, чтобы создать слои списком и сделать так, чтобы фильтров создавалось по количеству элементов в filter_sizes. [(Как здесь).](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/4%20-%20Convolutional%20Sentiment%20Analysis.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FenASHjKUb3p"
      },
      "source": [
        "### Вспомогательные функции"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yf2viKyOE5FF"
      },
      "source": [
        "Опишем функцию подсчёта accuracy, а также функции обучения и применения сети:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQo4PuPzFhPE"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def binary_accuracy(preds, y):\n",
        "    rounded_preds = torch.round(F.sigmoid(preds))\n",
        "    correct = (rounded_preds == y).float()\n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jepSRNlkFhPI"
      },
      "source": [
        "def train_func(model, iterator, optimizer, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch in iterator:\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        predictions = model(batch.text.cuda()).squeeze(1)\n",
        "\n",
        "        loss = criterion(predictions.float(), batch.label.float().cuda())\n",
        "        acc = binary_accuracy(predictions.float(), batch.label.float().cuda())\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss\n",
        "        epoch_acc += acc\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nb0KPBl8FhPL"
      },
      "source": [
        "def evaluate_func(model, iterator, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "            predictions = model(batch.text.cuda()).squeeze(1)\n",
        "\n",
        "            loss = criterion(predictions.float(), batch.label.float().cuda())\n",
        "            acc = binary_accuracy(predictions.float(), batch.label.float().cuda())\n",
        "\n",
        "            epoch_loss += loss\n",
        "            epoch_acc += acc\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6CzZJecUpN7"
      },
      "source": [
        "### Подготовка обучения"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Q_Yvs_gFhQB"
      },
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "N_FILTERS = 100\n",
        "FILTER_SIZES = [3,4,5]\n",
        "OUTPUT_DIM = 1\n",
        "DROPOUT_PROBA = 0.5\n",
        "\n",
        "model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT_PROBA)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-5mPhI4U0nk",
        "outputId": "0466c1ed-9964-44f8-a63e-48b2d1445aca"
      },
      "source": [
        "model # посмотрим на неё ещё раз"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNN(\n",
              "  (embedding): Embedding(25002, 100)\n",
              "  (conv_0): Conv2d(1, 100, kernel_size=(3, 100), stride=(1, 1))\n",
              "  (conv_1): Conv2d(1, 100, kernel_size=(4, 100), stride=(1, 1))\n",
              "  (conv_2): Conv2d(1, 100, kernel_size=(5, 100), stride=(1, 1))\n",
              "  (fc): Linear(in_features=300, out_features=1, bias=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVfQU3wUU4BY"
      },
      "source": [
        "Копируем скачанные эмбеддинги слов в параметры слоя `Embedding`, чботы не нужно было обучать его с нуля."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18Vk11CaZG-u",
        "outputId": "11310bbf-3d10-4d41-f09a-d38b07dd2979"
      },
      "source": [
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "model.embedding.weight.data.copy_(pretrained_embeddings)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
              "        ...,\n",
              "        [ 0.4413,  0.3325,  0.1120,  ..., -0.0686,  0.4374,  0.8717],\n",
              "        [ 0.1177,  0.1141,  0.2218,  ..., -1.0694,  0.4712, -0.7554],\n",
              "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Yqwwc5rBc5L"
      },
      "source": [
        "import torch.optim as optim"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "np-BTnydFhQF"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters()) # мы подали оптимизатору все параметры -- значит, эмбеддиги тоже будут дообучаться\n",
        "criterion = nn.BCEWithLogitsLoss() # бинарная кросс-энтропия с логитами\n",
        "\n",
        "model = model.cuda() # будем учить на gpu! =)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8LoPWRX5j2b"
      },
      "source": [
        "### замечание\n",
        "\n",
        "мы подаем несколько текстов, каждый имеет по строчкам  слова, по столбцам идут значения номеров токенов в vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmWIaqIbFhQF"
      },
      "source": [
        "### Обучение!\n",
        "\n",
        "Используя определённые ранее функции, запустим обучение с оптимизатором Adam и оценим качество на валидации и тесте:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZC7S33pFhQH",
        "outputId": "4eeaffca-e71e-47fe-f1ff-9dfb0ce680a7"
      },
      "source": [
        "N_EPOCHS = 5\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    train_loss, train_acc = train_func(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate_func(model, valid_iterator, criterion)\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc*100:.2f}%, Val. Loss: {valid_loss:.3f}, Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:652: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool1d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1805: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01, Train Loss: 0.405, Train Acc: 81.20%, Val. Loss: 0.297, Val. Acc: 87.60%\n",
            "Epoch: 02, Train Loss: 0.248, Train Acc: 89.92%, Val. Loss: 0.262, Val. Acc: 89.18%\n",
            "Epoch: 03, Train Loss: 0.174, Train Acc: 93.32%, Val. Loss: 0.257, Val. Acc: 89.15%\n",
            "Epoch: 04, Train Loss: 0.119, Train Acc: 95.75%, Val. Loss: 0.305, Val. Acc: 88.11%\n",
            "Epoch: 05, Train Loss: 0.078, Train Acc: 97.39%, Val. Loss: 0.323, Val. Acc: 88.92%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXQYQCLCFhQJ",
        "outputId": "98efa95a-536a-4753-feb0-c36819a41d87"
      },
      "source": [
        "test_loss , test_acc = evaluate_func(model, test_iterator, criterion)\n",
        "print(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1805: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loss: 0.282, Test Acc: 88.24%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mm5CmdjtY6Zj"
      },
      "source": [
        "#### Упражнение 1: как изменились эмбеддинги?\n",
        "\n",
        "Давайте проверим, произошли ли какие-то любопытные изменения в отношениях между словами."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbAon9WMY61-",
        "outputId": "05023a82-40b3-4cd9-b5e0-594ed183cac8"
      },
      "source": [
        "TEXT.vocab.vectors # старые эмбеддинги"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
              "        ...,\n",
              "        [ 0.4413,  0.3325,  0.1120,  ..., -0.0686,  0.4374,  0.8717],\n",
              "        [ 0.1177,  0.1141,  0.2218,  ..., -1.0694,  0.4712, -0.7554],\n",
              "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oej9zvPYarQK",
        "outputId": "fdca1239-3c5d-4829-9a5e-3f623701a060"
      },
      "source": [
        "model.embedding.weight.data # новые эмбеддиги"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 1.3537, -0.5994, -0.2251,  ...,  0.0064, -1.3333,  0.3274],\n",
              "        [-0.2687, -0.0785, -1.9346,  ..., -1.7024, -0.1569,  0.8511],\n",
              "        [ 0.4343, -0.0671, -1.0944,  ..., -0.3915, -0.9669,  0.3443],\n",
              "        ...,\n",
              "        [-0.5355,  1.5282,  0.3985,  ...,  0.6649, -1.7519, -1.8649],\n",
              "        [ 0.2111,  1.9653,  1.2217,  ..., -1.6369,  0.9221,  0.5238],\n",
              "        [-1.5200,  0.1436,  1.1655,  ...,  0.0534,  0.7600, -1.1566]],\n",
              "       device='cuda:0')"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8mHfM5MbhSx"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcFo6zIfbVvL"
      },
      "source": [
        "i1, i2 = TEXT.vocab.stoi['perfect'], TEXT.vocab.stoi['awful']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17bFXBwhbliQ",
        "outputId": "51531905-18f3-4c5a-8725-9b0f395ae502"
      },
      "source": [
        "cosine_similarity([\n",
        "  TEXT.vocab.vectors[i1].cpu().numpy(),\n",
        "  TEXT.vocab.vectors[i2].cpu().numpy()\n",
        "  ])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.9999999, 0.5248411],\n",
              "       [0.5248411, 0.9999996]], dtype=float32)"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUFaea6bbw8f",
        "outputId": "9d89db00-a133-403b-8c9a-330626f9ffd6"
      },
      "source": [
        "cosine_similarity([\n",
        "  model.embedding.weight.data[i1].cpu().numpy(),\n",
        "  model.embedding.weight.data[i2].cpu().numpy()\n",
        "  ])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 0.9999999, -0.086183 ],\n",
              "       [-0.086183 ,  0.9999996]], dtype=float32)"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0XE_D_MWE22"
      },
      "source": [
        "\"прекрасный\" и \"ужасный\" стали дальше друг от друга\n",
        "\n",
        "**Задание**: посмотрите на другие изменения и попробуйте их объяснить. Для наглядности можно сделать визуализацию с помощью t-sne."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVvpv9w-WFOc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICox-i-WYYPq"
      },
      "source": [
        "#### Упражнение 2: nn.ModuleList\n",
        "\n",
        "Используя его, можно легко и красиво определить столько разных сверток, сколько захотим! Вот пример:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKCFgWKdXeyO"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \n",
        "                 dropout, pad_idx):\n",
        "        \n",
        "        super().__init__()\n",
        "                \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
        "        \n",
        "        self.convs = nn.ModuleList([\n",
        "                                    nn.Conv2d(in_channels = 1, \n",
        "                                              out_channels = n_filters, \n",
        "                                              kernel_size = (fs, embedding_dim)) \n",
        "                                    for fs in filter_sizes\n",
        "                                    ])\n",
        "        \n",
        "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, text):\n",
        "                \n",
        "        #text = [batch size, sent len]\n",
        "        \n",
        "        embedded = self.embedding(text)\n",
        "                \n",
        "        #embedded = [batch size, sent len, emb dim]\n",
        "        \n",
        "        embedded = embedded.unsqueeze(1)\n",
        "        \n",
        "        #embedded = [batch size, 1, sent len, emb dim]\n",
        "        \n",
        "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
        "            \n",
        "        #conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n",
        "                \n",
        "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
        "        \n",
        "        #pooled_n = [batch size, n_filters]\n",
        "        \n",
        "        cat = self.dropout(torch.cat(pooled, dim = 1))\n",
        "\n",
        "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
        "            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSb06TT3cwO9"
      },
      "source": [
        "**Задание**: поэкспериментируйте с количеством и размером сверток. Что сработает лучше?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eO5TShOrc5OV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGNUN9TTYnxO"
      },
      "source": [
        "#### Упражнение 3: другая предобработка\n",
        "\n",
        "При загрузке данных, мы использовали `data.Field(tokenize='spacy')`.\n",
        "Попробуем заменить токенизатор `spacy` на свою функцию, которая дополнительно чистит данные от мусора."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "YkmL4MxZZ1N0",
        "outputId": "ff2fdf12-d892-451c-e5ea-c52b626308bd"
      },
      "source": [
        "# пример мусора\n",
        "ds.examples[0].text[25:40]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['is',\n",
              " 'exactly',\n",
              " 'what',\n",
              " 'happened',\n",
              " 'with',\n",
              " 'me.<br',\n",
              " '/><br',\n",
              " '/>The',\n",
              " 'first',\n",
              " 'thing',\n",
              " 'that',\n",
              " 'struck',\n",
              " 'me',\n",
              " 'about',\n",
              " 'Oz']"
            ]
          },
          "execution_count": 71,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRV729TtZfxC"
      },
      "source": [
        "Предобработка (из прошлого семинара):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnDJ5sFwZfxD"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iL2EXExRZfxF"
      },
      "source": [
        "def review_to_wordlist(review):\n",
        "    # убираем ссылки\n",
        "    review = re.sub(r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\", \" \", review)\n",
        "    # достаем сам текст\n",
        "    review_text = BeautifulSoup(review, \"lxml\").get_text()\n",
        "    # оставляем только буквенные символы\n",
        "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
        "    # приводим к нижнему регистру и разбиваем на слова по символу пробела\n",
        "    return review_text.lower().split() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXbasRI5dBfM"
      },
      "source": [
        "**Задание**: попробуйте обучить модель, используя другую предобработку. Стало ли лучше? Что если убирать стоп-слова?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4PiG-hIZfxH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sfuamkw28rgp"
      },
      "source": [
        "# Аугментация данных"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89zDRG-jIdNB"
      },
      "source": [
        "В нашем примере данные были сбалансированными, а как работать с небалансированными данными?\n",
        "\n",
        "Рассмотрим задачу распознавания тональности твитов, взятых из [Twitter Sentimental Analysis challenge](https://datahack.analyticsvidhya.com/contest/practice-problem-twitter-sentiment-analysis/).\n",
        "\n",
        "Источник изложения: https://github.com/mabusalah/Resampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLKzWFu-QWAm"
      },
      "source": [
        "Получим данные"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_WpbqWhk-a5",
        "outputId": "05aac62c-531b-406a-8b84-9fd42c26a29e"
      },
      "source": [
        "!wget --no-check-certificate \"https://drive.google.com/uc?export=download&id=1Jjuk23nMTQkfA3-3_HpevXGeupav7QLz\" -O train.csv\n",
        "!wget --no-check-certificate \"https://drive.google.com/uc?export=download&id=11FugxTRrdKqkDE_3KlfCDWRn_rbR6VxM\" -O test.csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2021-09-23 14:47:37--  https://drive.google.com/uc?export=download&id=1Jjuk23nMTQkfA3-3_HpevXGeupav7QLz\n",
            "Resolving drive.google.com (drive.google.com)... 142.251.2.139, 142.251.2.102, 142.251.2.113, ...\n",
            "Connecting to drive.google.com (drive.google.com)|142.251.2.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0s-44-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/7u8rheh5adhdiu2fhv5a3r2n5r6siiss/1632408450000/13414369628864094336/*/1Jjuk23nMTQkfA3-3_HpevXGeupav7QLz?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2021-09-23 14:47:38--  https://doc-0s-44-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/7u8rheh5adhdiu2fhv5a3r2n5r6siiss/1632408450000/13414369628864094336/*/1Jjuk23nMTQkfA3-3_HpevXGeupav7QLz?e=download\n",
            "Resolving doc-0s-44-docs.googleusercontent.com (doc-0s-44-docs.googleusercontent.com)... 142.251.2.132, 2607:f8b0:4023:c0d::84\n",
            "Connecting to doc-0s-44-docs.googleusercontent.com (doc-0s-44-docs.googleusercontent.com)|142.251.2.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/csv]\n",
            "Saving to: ‘train.csv’\n",
            "\n",
            "train.csv               [ <=>                ]   2.96M  15.8MB/s    in 0.2s    \n",
            "\n",
            "2021-09-23 14:47:39 (15.8 MB/s) - ‘train.csv’ saved [3103165]\n",
            "\n",
            "--2021-09-23 14:47:39--  https://drive.google.com/uc?export=download&id=11FugxTRrdKqkDE_3KlfCDWRn_rbR6VxM\n",
            "Resolving drive.google.com (drive.google.com)... 142.251.2.139, 142.251.2.113, 142.251.2.100, ...\n",
            "Connecting to drive.google.com (drive.google.com)|142.251.2.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-04-44-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/56lg2mialmqm3htsmr0ru5jd3qbqb4pr/1632408450000/13414369628864094336/*/11FugxTRrdKqkDE_3KlfCDWRn_rbR6VxM?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2021-09-23 14:47:40--  https://doc-04-44-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/56lg2mialmqm3htsmr0ru5jd3qbqb4pr/1632408450000/13414369628864094336/*/11FugxTRrdKqkDE_3KlfCDWRn_rbR6VxM?e=download\n",
            "Resolving doc-04-44-docs.googleusercontent.com (doc-04-44-docs.googleusercontent.com)... 142.251.2.132, 2607:f8b0:4023:c0d::84\n",
            "Connecting to doc-04-44-docs.googleusercontent.com (doc-04-44-docs.googleusercontent.com)|142.251.2.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1635543 (1.6M) [text/csv]\n",
            "Saving to: ‘test.csv’\n",
            "\n",
            "test.csv            100%[===================>]   1.56M  7.83MB/s    in 0.2s    \n",
            "\n",
            "2021-09-23 14:47:41 (7.83 MB/s) - ‘test.csv’ saved [1635543/1635543]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILRsSOuWJGFP",
        "outputId": "542da2b1-6ee5-4c9a-b48b-ffcef8d05f73"
      },
      "source": [
        "import pandas as pd\n",
        "test = pd.read_csv('test.csv')\n",
        "print(\"Test Set:\"% test.columns, test.shape, len(test))\n",
        "train = pd.read_csv('train.csv')\n",
        "print(\"Training Set:\"% train.columns, train.shape, len(train))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Set: (17197, 2) 17197\n",
            "Training Set: (31962, 3) 31962\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "sBtAsQDNJPXX",
        "outputId": "45bbcd2a-d511-455d-c96e-c404e6337a38"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>label</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>@user when a father is dysfunctional and is s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>bihday your majesty</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>#model   i love u take with u all the time in ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>factsguide: society now    #motivation</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  label                                              tweet\n",
              "0   1      0   @user when a father is dysfunctional and is s...\n",
              "1   2      0  @user @user thanks for #lyft credit i can't us...\n",
              "2   3      0                                bihday your majesty\n",
              "3   4      0  #model   i love u take with u all the time in ...\n",
              "4   5      0             factsguide: society now    #motivation"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "XV-4lGQVJRc2",
        "outputId": "4bdbc8db-e250-4c9d-a592-b017e95c7fef"
      },
      "source": [
        "test.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>31963</td>\n",
              "      <td>#studiolife #aislife #requires #passion #dedic...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>31964</td>\n",
              "      <td>@user #white #supremacists want everyone to s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>31965</td>\n",
              "      <td>safe ways to heal your #acne!!    #altwaystohe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>31966</td>\n",
              "      <td>is the hp and the cursed child book up for res...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>31967</td>\n",
              "      <td>3rd #bihday to my amazing, hilarious #nephew...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      id                                              tweet\n",
              "0  31963  #studiolife #aislife #requires #passion #dedic...\n",
              "1  31964   @user #white #supremacists want everyone to s...\n",
              "2  31965  safe ways to heal your #acne!!    #altwaystohe...\n",
              "3  31966  is the hp and the cursed child book up for res...\n",
              "4  31967    3rd #bihday to my amazing, hilarious #nephew..."
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xarXsO4DJU8Y"
      },
      "source": [
        "Итак, посмотрим, какой процент от общей выборки занимают позитивные и негативные примеры."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eT7Rz3MNJTVP",
        "outputId": "5b4c657d-4435-4015-c586-fcf3444684e2"
      },
      "source": [
        "print(\"Positive: \", train.label.value_counts()[0]/len(train)*100,\"%\")\n",
        "print(\"Negative: \", train.label.value_counts()[1]/len(train)*100,\"%\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Positive:  92.98542018647143 %\n",
            "Negative:  7.014579813528565 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VB6Jaz-mJ1zR"
      },
      "source": [
        "93% vs. 7% - данные определенно несбалансированны, что, в свою очередь, негативно влияет на точность предсказания.\n",
        "Для начала поработаем с исходными данными и оценим точность классификации.\n",
        "Начнем с предобработки данных: уберем из твитов числа, html/xml-тэги, специальные символы."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCmZHO1kJfpP"
      },
      "source": [
        "import re\n",
        "from bs4 import BeautifulSoup #для работы с html/xml-тэгами\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "porter=PorterStemmer()\n",
        "tok = WordPunctTokenizer()\n",
        "pat1 = r'@[A-Za-z0-9]+'\n",
        "pat2 = r'https?://[A-Za-z0-9./]+'\n",
        "combined_pat = r'|'.join((pat1, pat2))\n",
        "\n",
        "def tweet_cleaner(text):\n",
        "    soup = BeautifulSoup(text, 'lxml')\n",
        "    souped = soup.get_text()\n",
        "    stripped = re.sub(combined_pat, '', souped)\n",
        "    try:\n",
        "        clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
        "    except:\n",
        "        clean = stripped\n",
        "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", clean)\n",
        "    lower_case = letters_only.lower()\n",
        "\n",
        "    words = tok.tokenize(lower_case)\n",
        "    \n",
        "    stem_sentence=[]\n",
        "    for word in words:\n",
        "        stem_sentence.append(porter.stem(word))\n",
        "        stem_sentence.append(\" \")\n",
        "    words=\"\".join(stem_sentence).strip()\n",
        "    return words\n",
        "\n",
        "nums = [0,len(train)]\n",
        "clean_tweet_texts = []\n",
        "for i in range(nums[0],nums[1]):\n",
        "    clean_tweet_texts.append(tweet_cleaner(train['tweet'][i]))\n",
        "    \n",
        "nums = [0,len(test)]\n",
        "test_tweet_texts = []\n",
        "\n",
        "for i in range(nums[0],nums[1]):\n",
        "    test_tweet_texts.append(tweet_cleaner(test['tweet'][i])) \n",
        "    \n",
        "train_clean = pd.DataFrame(clean_tweet_texts,columns=['tweet'])\n",
        "train_clean['label'] = train.label\n",
        "train_clean['id'] = train.id\n",
        "test_clean = pd.DataFrame(test_tweet_texts,columns=['tweet'])\n",
        "test_clean['id'] = test.id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkQ5fDVUM9EU"
      },
      "source": [
        "Разделим данные на обучающие и проверочные."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d23SW3-tMdKk"
      },
      "source": [
        "from sklearn import model_selection, preprocessing, metrics, linear_model, svm\n",
        "\n",
        "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(train_clean['tweet'],train_clean['label'])\n",
        "encoder = preprocessing.LabelEncoder()\n",
        "train_y = encoder.fit_transform(train_y)\n",
        "valid_y = encoder.fit_transform(valid_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tfj1ZjQANVSK"
      },
      "source": [
        "Рассчитаем TF-IDF признаки."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNw9U1WnNKfq"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "\n",
        "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=100000)\n",
        "tfidf_vect.fit(train_clean['tweet'])\n",
        "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
        "xvalid_tfidf =  tfidf_vect.transform(valid_x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8TxxultNlOq"
      },
      "source": [
        "Точность в качестве метрики работает хорошо только на сбалансированных наборах данных, поэтому для оценки результатов работы  алгоритма будем использовать F1-метрику."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cx29ZstgNXqb"
      },
      "source": [
        "def train_model(classifier, feature_vector_train, label, feature_vector_valid):\n",
        "    classifier.fit(feature_vector_train, label)\n",
        "\n",
        "    predictions = classifier.predict(feature_vector_valid)    \n",
        "\n",
        "    return metrics.f1_score(valid_y,predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YEjiEUDNrLj"
      },
      "source": [
        "Для начала обучим лог-регрессию."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BG3DifE-Nn-I",
        "outputId": "3105c05d-8737-447d-b758-1c2ac59a6530"
      },
      "source": [
        "accuracyORIGINAL = train_model(linear_model.LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial'),xtrain_tfidf, train_y, xvalid_tfidf)\n",
        "print (\"Logistic regression Baseline, WordLevel TFIDF: \", accuracyORIGINAL)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic regression Baseline, WordLevel TFIDF:  0.5401459854014599\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIaiuRviW5pS"
      },
      "source": [
        "Попробуйте использовать обычный счетчик слов для извлечения признаков."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vsr0wT4NW2LW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0baBnaYN7a0"
      },
      "source": [
        "Как видно, результат оставляет желать лучшего.\n",
        "\n",
        "Что можно сделать с данными?\n",
        "\n",
        "Было бы неплохо как-то увеличить  количество негативных примеров, или же уменьшить количество положительных. Для этого существуют различные техники аугментации данных. \n",
        "В Python для этих целей есть библиотека imblearn (imbalanced-learn)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6zNqmnUN1WB",
        "outputId": "d2a9ec59-cd71-4727-ae69-22dc283ef5cf"
      },
      "source": [
        "from imblearn.over_sampling import BorderlineSMOTE, SMOTE, ADASYN, SMOTENC, RandomOverSampler\n",
        "from imblearn.under_sampling import (RandomUnderSampler, \n",
        "                                    NearMiss, \n",
        "                                    InstanceHardnessThreshold,\n",
        "                                    CondensedNearestNeighbour,\n",
        "                                    EditedNearestNeighbours,\n",
        "                                    RepeatedEditedNearestNeighbours,\n",
        "                                    AllKNN,\n",
        "                                    NeighbourhoodCleaningRule,\n",
        "                                    OneSidedSelection,\n",
        "                                    TomekLinks)\n",
        "from imblearn.combine import SMOTEENN, SMOTETomek\n",
        "from imblearn.pipeline import make_pipeline"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efoIi6euYMQs"
      },
      "source": [
        "Итак, в качестве инструментов для аугментации рассмотрим: under-sampling, over-sampling и их комбинацию."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqhD2dodYwJl"
      },
      "source": [
        "**Under-sampling** уравновешивает данные за счет уменьшения размера  превалирующего класса.\n",
        "Этот метод разумно использовать, когда количество данных достаточно велико, иначе есть риск остаться и вовсе без обучающих примеров.\n",
        "\n",
        "Итак, логика действия довольно проста: мы просто случайным образом убираем лишние экземпляры из превалирующего класса.\n",
        "\n",
        "Так как в нашем примере лишь 7% всех твитов имеют негативную окраску, уравновешивание позитивного набора с этими 7-ю процентами вряд ли обеспечит хороший результат.\n",
        "\n",
        "Попробуем..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQVfIvf0aTrp",
        "outputId": "a44e1060-6699-4747-b00c-16d8b720ffeb"
      },
      "source": [
        "rus = RandomUnderSampler(random_state=0, replacement=True)\n",
        "rus_xtrain_tfidf, rus_train_y = rus.fit_sample(xtrain_tfidf, train_y)\n",
        "accuracyrus = train_model(linear_model.LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial'),rus_xtrain_tfidf, rus_train_y, xvalid_tfidf)\n",
        "print (\"Logistic regressio RUS, WordLevel TFIDF: \", accuracyrus)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic regressio RUS, WordLevel TFIDF:  0.5184033177812338\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnwqxfGeaa9R"
      },
      "source": [
        "Действительно, все стало только хуже.\n",
        "\n",
        "Попробуем другие алгоритмы **under-sampling**.\n",
        "\n",
        "Например, **NearMiss**. Данный алгоритм выбирает, какие экземпляры нужно оставить в превалирующем классе на основании некоторых эвристик. Существует три варианта данного алгоритма:\n",
        "\n",
        "**NearMiss-1** оставляет те экземпляры из превалирующего класса, для которых среднее расстояние до *k* ближайших соседей из миноритарного класса будет наименьшим.\n",
        "\n",
        "**NearMiss-2** оставляет те экземпляры из превалирующего класса, для которых среднее расстояние до *k* самых дальних соседей из миноритарного класса будет наименьшим.\n",
        "\n",
        "**NearMiss-3** состоит из двух шагов: сначала, для каждого экземпляра из миноритарного класса выбирается *k* ближайших соседей из превалирующего класса, затем, из большего класса выбираются те экземпляры, для которых среднее расстояние до *k* ближайших соседей максимальное.\n",
        "\n",
        "![](https://glemaitre.github.io/imbalanced-learn/_images/sphx_glr_plot_nearmiss_001.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSlgmHmOd9tU",
        "outputId": "c252464b-3a48-48ef-b04f-5bb95d8e32f0"
      },
      "source": [
        "for sampler in (NearMiss(version=1),NearMiss(version=2),NearMiss(version=3)):\n",
        "    nm_xtrain_tfidf, nm_train_y = sampler.fit_sample(xtrain_tfidf, train_y)\n",
        "    accuracysm = train_model(linear_model.LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial'),nm_xtrain_tfidf, nm_train_y, xvalid_tfidf)\n",
        "    print (\"Logistic regression NearMiss(version= {0}), WordLevel TFIDF: \".format(sampler.version), accuracysm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic regression NearMiss(version= 1), WordLevel TFIDF:  0.3019441069258809\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic regression NearMiss(version= 2), WordLevel TFIDF:  0.5128205128205128\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/imblearn/under_sampling/_prototype_selection/_nearmiss.py:194: UserWarning: The number of the samples to be selected is larger than the number of samples available. The balancing ratio cannot be ensure and all samples will be returned.\n",
            "  warnings.warn('The number of the samples to be selected is larger'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic regression NearMiss(version= 3), WordLevel TFIDF:  0.3129129129129129\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9nNg2irexcc"
      },
      "source": [
        "**Edited Nearest Neighbor (ENN)**\n",
        "\n",
        "ENN удаляет из большего класса элемент, если класс его ближайшего соседа отличается от его собственного."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFjd6PzjfWEU",
        "outputId": "3e13c72d-77e9-4ee7-e27d-5a73585847b8"
      },
      "source": [
        "enn_xtrain_tfidf, enn_train_y = EditedNearestNeighbours().fit_sample(xtrain_tfidf, train_y)\n",
        "accuracy = train_model(linear_model.LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial'),enn_xtrain_tfidf, enn_train_y, xvalid_tfidf)\n",
        "print (\"Logistic regression {0}, WordLevel TFIDF: \", accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic regression {0}, WordLevel TFIDF:  0.5518072289156627\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VcHHV4jgZ50"
      },
      "source": [
        "Как вы поняли, при применении **Under-samplin**g техник новые данные не генерируются, в отличие от **Over-sampling**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CHYj7jW7z5q"
      },
      "source": [
        "# Over-sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODhcR4G672lq"
      },
      "source": [
        "Итак, когда данных недостаточно или количество экземпляров в миноритарном классе очень мало применяется **Over-sampling**. \n",
        "\n",
        "При применении этой техники балансировка данных происходит за счет увеличения количества экземпляров в миноритарном классе. Новые элементы генерируются за счет: повторения, бутстрэппинга, SMOTE (Synthetic Minority Over-Sampling Technique) или ADASYN (Adaptive synthetic sampling)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ea9aYY4mxsqo"
      },
      "source": [
        "**Random Over-sampling**: случайным образом дублируются некоторые элементы из миноритарного класса."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-6Ivm0ZOCOt",
        "outputId": "b8a90861-3ef1-4f1f-e460-0aaf5d37edea"
      },
      "source": [
        "#Random Over Sampling\n",
        "ros = RandomOverSampler(random_state=777)\n",
        "ros_xtrain_tfidf, ros_train_y = ros.fit_sample(xtrain_tfidf, train_y)\n",
        "accuracyROS = train_model(linear_model.LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial'),ros_xtrain_tfidf, ros_train_y, xvalid_tfidf)\n",
        "print (\"Logistic regression ROS, WordLevel TFIDF: \", accuracyROS)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic regression ROS, WordLevel TFIDF:  0.6687354538401862\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXp1gpdDz5p5"
      },
      "source": [
        "**SMOTE Over-sampling**\n",
        "\n",
        "Алгоритм SMOTE основан на идее генерации некоторого количества искусственных примеров, которые были бы «похожи» на имеющиеся в миноритарном классе, но при этом не дублировали их.\n",
        "\n",
        "Для создания новой записи находят разность $d=X_b-X_a,$ где $ X_b, X_a -$ векторы признаков «соседних» примеров $a$ и $b$ из миноритарного класса. \n",
        "\n",
        "Их находят, используя алгоритм ближайшего соседа (*KNN*). В данном случае необходимо и достаточно для примера $b$ получить набор из $k$ соседей, из которого в дальнейшем будет выбрана запись $a$. Остальные шаги алгоритма *KNN* не требуются.\n",
        "\n",
        "Далее из $d$ путем умножения каждого его элемента на случайное число в интервале (0, 1) получают $\\hat{d}$. Вектор признаков нового примера вычисляется путем сложения $X_a$ и $\\hat{d}$. \n",
        "\n",
        "Алгоритм **SMOTE** позволяет задавать количество записей, которое необходимо искусственно сгенерировать. Степень сходства примеров $a$ и $b$ можно регулировать путем изменения значения $k$ (числа ближайших соседей).\n",
        "\n",
        "![](https://hsto.org/getpro/habr/post_images/c57/e7e/f4f/c57e7ef4f8711ad2eda881651a027867.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWzqbIWsOEg1",
        "outputId": "53b6d551-384a-4189-e4c6-22689dba3eea"
      },
      "source": [
        "sm = SMOTE(random_state=777, ratio = 1.0)\n",
        "sm_xtrain_tfidf, sm_train_y = sm.fit_sample(xtrain_tfidf, train_y)\n",
        "accuracySMOTE = train_model(linear_model.LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial'),sm_xtrain_tfidf, sm_train_y, xvalid_tfidf)\n",
        "print (\"Logistic regression SMOTE, WordLevel TFIDF: \", accuracySMOTE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic regression SMOTE, WordLevel TFIDF:  0.6593233674272228\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gpJkvdb10Sg"
      },
      "source": [
        "Итак, по сравнению с **Random Over-sampling** разница небольшая.\n",
        "\n",
        "Проверьте результаты **Random Over-sampling** и **SMOTE Over-sampling** для реальных тестовых данных (*test_clean*)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7V-VoCAO7qw0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Jz0o7k82YoX"
      },
      "source": [
        "Следующий алгоритм **ASMO: Adaptive synthetic minority oversampling**.\n",
        "\n",
        "\n",
        "\n",
        "Сгенерировать искусственные записи в пределах отдельных кластеров на основе всех классов. Для каждого примера миноритарного класса находят m ближайших соседей, и на основе них (также как в SMOTE) создаются новые записи.\n",
        "\n",
        "1.   Если для каждого $i$-ого примера миноритарного класса из $k$ ближайших соседей $g$ ($g\\leq k$) принадлежит к мажоритарному, то набор данных считается «рассеянным». В этом случае используют алгоритм **ASMO**, иначе применяют **SMOTE** (как правило, $g$ задают равным 20).\n",
        "2.   Используя только примеры миноритарного класса, выделить несколько кластеров (например, алгоритмом $k$-means).\n",
        "3.   Сгенерировать искусственные записи в пределах отдельных кластеров на основе всех классов. Для каждого примера миноритарного класса находят m ближайших соседей, и на основе них (также как в **SMOTE**) создаются новые записи.\n",
        "\n",
        "![](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcQdTzjHBZ_9At5GIDRpF2AAw9hU1jzcVE5uwA&usqp=CAU)\n",
        "\n",
        "Такая модификация алгоритма **SMOTE** делает его более адаптивным к различным наборам данных с несбалансированными классами."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mClRDnFM1weo",
        "outputId": "d299f772-e119-42a4-b6e0-4f40a15520ce"
      },
      "source": [
        "ad = ADASYN(random_state=777, ratio = 1.0)\n",
        "ad_xtrain_tfidf, ad_train_y = ad.fit_sample(xtrain_tfidf, train_y)\n",
        "accuracyADASYN = train_model(linear_model.LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial'),ad_xtrain_tfidf, ad_train_y, xvalid_tfidf)\n",
        "print (\"Logistic regression ADASYN, WordLevel TFIDF: \", accuracyADASYN)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic regression ADASYN, WordLevel TFIDF:  0.6555639666919001\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHRcvIRP4fx7"
      },
      "source": [
        "И опять проверим на реальных тестовых примерах."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTzMbjvnRyFD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "br9FjH077ix0"
      },
      "source": [
        "# Комбинация **Under-** и **Over-sampling**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-RCI42f42Hi"
      },
      "source": [
        "В *imblearn* реализованы две возможные комбинации:\n",
        "\n",
        "\n",
        "1.   **SMOTE** + **ENN**\n",
        "2.   **SMOTE** + **Tomek Link Removal** (Пара двух ближайших соседей, которые принадлежат разным классам называется *Tomek link*. Under-sampling заключается в удалении всех таких элементов из мажоритарного класса)\n",
        "\n",
        "Подробнее: https://imbalanced-learn.readthedocs.io/en/stable/api.html#module-imblearn.combine\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pg1YBvT4-xP",
        "outputId": "fd5e0b9a-f1ae-47cb-9745-3cd348917748"
      },
      "source": [
        "se = SMOTEENN(random_state=42)\n",
        "se_xtrain_tfidf, se_train_y = se.fit_sample(xtrain_tfidf, train_y)\n",
        "accuracy = train_model(linear_model.LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial'),se_xtrain_tfidf, se_train_y, xvalid_tfidf)\n",
        "print (\"Logistic regression SMOTEENN: \", accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic regression SMOTEENN:  0.5139720558882237\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Gd9dCEd5aDs"
      },
      "source": [
        "Первый метод сработал плохо. Оцените работу второго подхода."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkZofkXo5RG2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X52CbSUs3jkK"
      },
      "source": [
        "# Аугментация текстовых данных"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YuQhjhhp0KM"
      },
      "source": [
        "Для аугментации тектсовых данных будем применять библиотеку [nlpaug](https://github.com/makcedward/nlpaug).\n",
        "\n",
        "Библиотека предоставляет функционал для различных типов аугментации текста, в том числе: добавление опечаток, замена слов синонимами, вставка дополнительных слов и т.д. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjEpJ6Yz3r5D",
        "outputId": "423acdee-a711-402b-caac-7e43f2d72e75"
      },
      "source": [
        "!pip install numpy requests nlpaug"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (2.23.0)\n",
            "Requirement already satisfied: nlpaug in /usr/local/lib/python3.7/dist-packages (1.1.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests) (2.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuXPGhVT5JkI",
        "outputId": "5eca651b-62ef-4a35-eb36-53176edbd3c4"
      },
      "source": [
        "text = 'The quick brown fox jumps over the lazy dog .'\n",
        "print(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The quick brown fox jumps over the lazy dog .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihSt3nkBrKQ2"
      },
      "source": [
        "Аугментируем пример с помощью замены слов на основе векторной модели предсавления слов Glove.\n",
        "\n",
        "Загрузим и инициализируем модель:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldMd_LBUbJVD"
      },
      "source": [
        "from nlpaug.util.file.download import DownloadUtil\n",
        "\n",
        "\n",
        "DownloadUtil.download_glove(model_name='glove.6B', dest_dir='.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbuG44538kFO"
      },
      "source": [
        "import nlpaug.augmenter.word as naw\n",
        "\n",
        "aug = naw.WordEmbsAug(\n",
        "    model_type='glove', model_path='glove.6B.100d.txt',\n",
        "    action=\"substitute\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3fAiWg6rju0"
      },
      "source": [
        "Аугментируем наш пример:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqsycH7me1F5",
        "outputId": "c2e4eeae-b49d-4bd0-e776-038671905c8a"
      },
      "source": [
        "augmented_text = aug.augment(text)\n",
        "print(\"Original:\")\n",
        "print(text)\n",
        "print(\"Augmented Text:\")\n",
        "print(augmented_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original:\n",
            "The quick brown fox jumps over the lazy dog .\n",
            "Augmented Text:\n",
            "The quick brown shown turns over the lazy monster.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLv4TNy6rphX"
      },
      "source": [
        "Попробуем аугментировать негативные твиты для классификатора. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RtQJC8X64BW"
      },
      "source": [
        "def get_negative_tweets(train_x, train_y):\n",
        "  x_neg_train = []\n",
        "  for tweet, label in zip(train_x, train_y):\n",
        "    if label == 1:\n",
        "      x_neg_train.append(tweet)\n",
        "  return x_neg_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFbQsRzx74gC",
        "outputId": "31d7071c-156b-4168-bdf3-d7176799bf27"
      },
      "source": [
        "neg_train_x = get_negative_tweets(train_x, train_y)\n",
        "neg_train_x[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['um whi a girl and not a boy or mayb just gender neutral teenag',\n",
              " 'clearli just humour attempt by liber media to undermin trump racebait won t work trump is not a',\n",
              " 'w prop jesu wa not black nor wa he white those who promot the hate biggotri need to be call vile scum',\n",
              " 'thi moron is lead the us down a path that ha so much potenti to damag and destroy the world',\n",
              " 'techjunkiejh the daili show unpack against blackwomen and asianmen on datingapp',\n",
              " 'you might be a libtard if libtard sjw liber polit',\n",
              " 'you might be a libtard if libtard sjw liber polit',\n",
              " 'allahsoil not all muslim hate america emirati in word',\n",
              " 'if they say their comment are just a joke or smth it will be just their opinion i don t think so it s a',\n",
              " 'riyadh is renown for some of the deadliest traffic in the world in word']"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDV3Xzht8F9G",
        "outputId": "00447728-5101-4a12-a4bf-9e8e2c05b961"
      },
      "source": [
        "neg_train_x_aug = aug.augment(neg_train_x)\n",
        "neg_train_x_aug[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['um whi to her and not a boy or mayb just feminism extremely teenag',\n",
              " 'clearli just humour attempt by jahrbuch media up undermin trump racebait trophy t did trump actually not a',\n",
              " \"w prop jesu wa believe black unless wa he white those thought promot the hate biggotri n't to one call awful scum\",\n",
              " 'thi arka where lead the coming down a turn that ha so much potenti out damag and destroy the record',\n",
              " 'techjunkiejh another daili show individualize against blackwomen way asianmen on datingapp',\n",
              " 'tell might be a libtard if libtard js03 poupée polit',\n",
              " 'feel might be long libtard if libtard shn liber polit',\n",
              " 'allahsoil not it muslim remember america emirati in word',\n",
              " 'if they say their comment are just it joke or smth its time be just up opinion mind don t think so you price a',\n",
              " 'riyadh is admired for some an the deadliest traffic with even world in word']"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klAMj1LYtGCm"
      },
      "source": [
        "Добавим аугментированные данные к обучающей выборке:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uuhelg3BhkZY"
      },
      "source": [
        "train_x_aug = train_x.append(pd.Series(neg_train_x_aug))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-iiMiuJtN81"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "neg_train_y_aug = [1] * len(neg_train_x_aug)\n",
        "train_y_aug = np.concatenate((train_y, neg_train_y_aug), axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "790P21a6tXYE"
      },
      "source": [
        "Перемешаем обучающие данные:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14V3GHFBlnFY",
        "outputId": "c3302308-6f89-4166-9815-1813974dfc9b"
      },
      "source": [
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "indexes = list(range(0, len(train_x_aug)))\n",
        "random.shuffle(indexes)\n",
        "\n",
        "train_x_aug_shuffled = pd.Series([list(train_x_aug)[i] for i in tqdm(indexes)])\n",
        "train_y_aug_shuffled = train_y_aug[indexes]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 25640/25640 [00:56<00:00, 456.65it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhKtfS3fn63X"
      },
      "source": [
        "#tweets = pd.concat([train_x1, valid_x])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VN1H_4IMujB8"
      },
      "source": [
        "Векторизуем полученные тексты:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlgH2zwpgdaH"
      },
      "source": [
        "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=100000)\n",
        "tfidf_vect.fit(pd.concat([train_x_aug_shuffled, valid_x]))\n",
        "xtrain_tfidf_aug =  tfidf_vect.transform(train_x_aug_shuffled)\n",
        "xvalid_tfidf_aug =  tfidf_vect.transform(valid_x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yYm3fbAunUp"
      },
      "source": [
        "Обучим и оценим модель:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnbYZIGEkiKS",
        "outputId": "cba02701-0d99-4eec-e6ed-ae4d09b845e8"
      },
      "source": [
        "accuracyTEXTAUG = train_model(linear_model.LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial'),xtrain_tfidf_aug, train_y_aug_shuffled, xvalid_tfidf_aug)\n",
        "print (\"Logistic regression with augmented texts, WordLevel TFIDF: \", accuracyTEXTAUG)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic regression with augmented texts, WordLevel TFIDF:  0.42416107382550333\n"
          ]
        }
      ]
    }
  ]
}