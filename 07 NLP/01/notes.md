# большая задача - курс на kaggle 
## время
примерно 2 недели после 

# особенности обработки языка
большие объемы
## можно строит языковые модели

## предобраотка текстовых данных
### что с ними делать?
просто - длинная строка, с ними не очень приятно работать

## нормализацию
### подставы
* от задачи зависит - может быть не нужно удалять заглавные буквы - если много имен
* если пунктуация - иногда убирают, но зависит от задачи - может быть для тональности не нужно
* возможно надо порабоать с самыми частыми или самыми редкими словами

### regular expressions
use them

### лемматизация
это алгоритмический процесс нахождения леммы слова в зависимости от его значения
### стемминг 
к псевдооснове приводить - горозда быстрее, но немного ошибочный
в realtime возможно его использовать есть смысл


# порядок предобработки

1. к одному региструк
2. удалить небуквенное
3. делить на слова или токены
4. удалиение стоп-слов
5. нормализация
6. удаление слишком частых слов
7. векторизация

# задача классификации запрещенных объявлений по описанию
* удалили числа - оказалось не помогло
* выкинуть не существительные


# Feature creaton 
* содержит слово Х - бинарный признак
* bag of words - считать число слов в объекте
    * можно попробовать регуляризацию на ней 
* TF-IDF - считаем сколкьо раз свтретились слова в тексте
    * заменим число на $tfidf_{wd} = tf_{wd} \cdot\log{\frac{|D|}{df_w}}$
    * можно брать с высоким значенимем слова - и например только их оставлять
## учет связи между 
    * считать bag-of-words, TF-IDF на н граммах
    * нграммы которые популярные - есть смысл оставлять их 





