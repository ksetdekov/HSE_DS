# seminar 1

## большая задача - курс на kaggle

### время

примерно 2 недели после

## особенности обработки языка

большие объемы

### можно строит языковые модели

### предобраотка текстовых данных

#### что с ними делать?

просто - длинная строка, с ними не очень приятно работать

### нормализацию

#### подставы

* от задачи зависит - может быть не нужно удалять заглавные буквы - если много имен
* если пунктуация - иногда убирают, но зависит от задачи - может быть для тональности не нужно
* возможно надо порабоать с самыми частыми или самыми редкими словами

#### regular expressions

use them

#### лемматизация

это алгоритмический процесс нахождения леммы слова в зависимости от его значения

#### стемминг

к псевдооснове приводить - горозда быстрее, но немного ошибочный
в realtime возможно его использовать есть смысл

## порядок предобработки

1. к одному региструк
2. удалить небуквенное
3. делить на слова или токены
4. удалиение стоп-слов
5. нормализация
6. удаление слишком частых слов
7. векторизация

## задача классификации запрещенных объявлений по описанию

* удалили числа - оказалось не помогло
* выкинуть не существительные

## Feature creaton

* содержит слово Х - бинарный признак
* bag of words - считать число слов в объекте
  * можно попробовать регуляризацию на ней
* TF-IDF - считаем сколкьо раз свтретились слова в тексте
  * заменим число на $tfidf_{wd} = tf_{wd} \cdot\log{\frac{|D|}{df_w}}$
  * можно брать с высоким значенимем слова - и например только их оставлять

### учет связи между

    * считать bag-of-words, TF-IDF на н граммах
    * нграммы которые популярные - есть смысл оставлять их 

## Векторные представления слов

### близость, близость слов

* манхеттонское рассктояние
* евклидово расстояние
* косинусное расстояние - это как раз использутся часто

### onehot encodint

норм, но если добавляются слова - нужно менять

### svd на основе onehot encoding

стало лучше.
не взаимно ортогонально. близость как-то учитывается

### модель word2vec и ее аналоги

word2vec подходы к обучению:

1. Continuous BOW слово по его контексту
2. skip-gram - контекст по слову

CBOW - 2 слойная нейросеть - n мерный вектор -число слов нашей коллекции.
берем на вход

#### problems (word2vec +  Global Vectors, представляет собой модель распределенного представления слов)

* out-of-vocabulary vords
* не смотрит на морфологии

### FastText

смотрим на векторное представления последовательностей символов - Нграмм

развитие word2vec

## выводы

что такое хорошо для векторов слов?

* внутренние
* оценки в вакууме
  * близость по аналогиии
  * согласно близости по размеченным синонимам

## варианты по ДЗ

### дз по 1.3

* попробовать разные леммизаторы
* logreg
* trees
* попробовать отбросить слова по частотному анализу
* посмотреть коэффициенты регуляризации
