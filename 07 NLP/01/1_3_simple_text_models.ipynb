{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1_3_simple_text_models.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wTjLOlc8ib0"
      },
      "source": [
        "<h1><center>Простые векторные модели текста</center></h1>\n",
        "\n",
        "<img src=\"pipeline_vec.png\" alt=\"pipeline.png\" style=\"width: 400px;\"/>\n",
        "\n",
        "### Задача: классификация твитов по тональности\n",
        "\n",
        "В этом занятии мы познакомимся с распространенной задачей в анализе текстов: с классификацией текстов на классы.\n",
        "\n",
        "В рассмотренном тут примере классов будет два: положительный и отрицательный, такую постановку этой задачи обычно называют классификацией по тональности или sentiment analysis.\n",
        "\n",
        "Классификацию по тональности используют, например, в рекомендательных системах и при анализе отзывов клиентов, чтобы понять, понравилось ли людям кафе, кино, etc.\n",
        "\n",
        "Более подробно мы рассмотрим данную задачу и познакомимся с более сложными методами её решения в семинаре 3, а здесь разберем простые подходы, основанные на методе мешка слов.\n",
        "\n",
        "У нас есть [данные постов в твиттере](http://study.mokoron.com/), про из которых каждый указано, как он эмоционально окрашен: положительно или отрицательно. \n",
        "\n",
        "**Задача**: построить модель, которая по тексту поста предсказывает его эмоциональную окраску.\n",
        "\n",
        "\n",
        "Скачиваем данные: [положительные](https://drive.google.com/file/d/1mW_fUtYmRF19AXVySU0gJOIgx0-1EFgD/view?usp=sharing), [отрицательные](https://drive.google.com/file/d/1ZnsFuf-yfO3UEHlIpk7TTqfKkEMdm1EQ/view?usp=sharing)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PuDVGp4O8ib1",
        "outputId": "b2dfb55f-2e7a-4970-fe8b-9fddbea4bead"
      },
      "source": [
        "# если у вас линукс / мак / collab или ещё какая-то среда, в которой работает wget, можно так:\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1mW_fUtYmRF19AXVySU0gJOIgx0-1EFgD' -O positive.csv\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1ZnsFuf-yfO3UEHlIpk7TTqfKkEMdm1EQ' -O negative.csv"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-16 14:39:43--  https://docs.google.com/uc?export=download&id=1mW_fUtYmRF19AXVySU0gJOIgx0-1EFgD\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.195.102, 74.125.195.139, 74.125.195.113, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.195.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-04-94-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/8f98nt1rspfi1p895kg1upeu1snmg81j/1634395125000/10227726563468148216/*/1mW_fUtYmRF19AXVySU0gJOIgx0-1EFgD?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2021-10-16 14:39:44--  https://doc-04-94-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/8f98nt1rspfi1p895kg1upeu1snmg81j/1634395125000/10227726563468148216/*/1mW_fUtYmRF19AXVySU0gJOIgx0-1EFgD?e=download\n",
            "Resolving doc-04-94-docs.googleusercontent.com (doc-04-94-docs.googleusercontent.com)... 74.125.142.132, 2607:f8b0:400e:c08::84\n",
            "Connecting to doc-04-94-docs.googleusercontent.com (doc-04-94-docs.googleusercontent.com)|74.125.142.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 26233379 (25M) [text/csv]\n",
            "Saving to: ‘positive.csv’\n",
            "\n",
            "positive.csv        100%[===================>]  25.02M  95.6MB/s    in 0.3s    \n",
            "\n",
            "2021-10-16 14:39:44 (95.6 MB/s) - ‘positive.csv’ saved [26233379/26233379]\n",
            "\n",
            "--2021-10-16 14:39:45--  https://docs.google.com/uc?export=download&id=1ZnsFuf-yfO3UEHlIpk7TTqfKkEMdm1EQ\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.142.138, 74.125.142.101, 74.125.142.102, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.142.138|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0k-94-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/v0v6f3id2hgkanh6t5klku5nc9au07rg/1634395125000/10227726563468148216/*/1ZnsFuf-yfO3UEHlIpk7TTqfKkEMdm1EQ?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2021-10-16 14:39:46--  https://doc-0k-94-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/v0v6f3id2hgkanh6t5klku5nc9au07rg/1634395125000/10227726563468148216/*/1ZnsFuf-yfO3UEHlIpk7TTqfKkEMdm1EQ?e=download\n",
            "Resolving doc-0k-94-docs.googleusercontent.com (doc-0k-94-docs.googleusercontent.com)... 74.125.142.132, 2607:f8b0:400e:c08::84\n",
            "Connecting to doc-0k-94-docs.googleusercontent.com (doc-0k-94-docs.googleusercontent.com)|74.125.142.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 24450101 (23M) [text/csv]\n",
            "Saving to: ‘negative.csv’\n",
            "\n",
            "negative.csv        100%[===================>]  23.32M  88.9MB/s    in 0.3s    \n",
            "\n",
            "2021-10-16 14:39:47 (88.9 MB/s) - ‘negative.csv’ saved [24450101/24450101]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPp8_2Sy8ib5"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import *\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "pd.set_option('display.max_columns', None)  \n",
        "pd.set_option('display.expand_frame_repr', False)\n",
        "pd.set_option('max_colwidth', 800)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsmSQOE98ib8"
      },
      "source": [
        "positive = pd.read_csv('positive.csv', sep=';', usecols=[3], names=['text'])\n",
        "positive['label'] = ['positive'] * len(positive)\n",
        "negative = pd.read_csv('negative.csv', sep=';', usecols=[3], names=['text'])\n",
        "negative['label'] = ['negative'] * len(negative)\n",
        "df = positive.append(negative)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "znc9rKWk8ib-",
        "outputId": "777808f8-c82f-4321-cefe-a51fdb545d83"
      },
      "source": [
        "df.sample(5)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7066</th>\n",
              "      <td>Сеструха второй палец ломает. Бляя, опять:((</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49624</th>\n",
              "      <td>Все очень нравятся, но нужно выбрать) http://t.co/wZs4zkTdVS http://t.co/OyNbBgID0r</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94008</th>\n",
              "      <td>@_ash_tan_ да и там не нужна! Жизнь-боль((((</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108320</th>\n",
              "      <td>@assaron Типа того :) Они уже не в первый раз пытаются договориться, но там куча технических сложностей, помимо политических.</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90828</th>\n",
              "      <td>Перебирала фото и нашла кое-что.\\nВремя пускать слюни, сладкоежки Х) http://t.co/fVdhrWFRAU</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                 text     label\n",
              "7066                                                                                     Сеструха второй палец ломает. Бляя, опять:((  negative\n",
              "49624                                             Все очень нравятся, но нужно выбрать) http://t.co/wZs4zkTdVS http://t.co/OyNbBgID0r  positive\n",
              "94008                                                                                    @_ash_tan_ да и там не нужна! Жизнь-боль((((  negative\n",
              "108320  @assaron Типа того :) Они уже не в первый раз пытаются договориться, но там куча технических сложностей, помимо политических.  positive\n",
              "90828                                     Перебирала фото и нашла кое-что.\\nВремя пускать слюни, сладкоежки Х) http://t.co/fVdhrWFRAU  positive"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZC-cyLUizc0"
      },
      "source": [
        "Воспользуемся функцией для предобработки текста, которую мы написали в прошлом семинаре:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCSqGM_qkNxT",
        "outputId": "99d8eccb-bd05-4d97-d3a5-1756a4c31e00"
      },
      "source": [
        "!pip3 install pymorphy2"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymorphy2\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[?25l\r\u001b[K     |██████                          | 10 kB 21.5 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 20 kB 28.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 30 kB 30.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 40 kB 21.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 51 kB 9.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 55 kB 2.5 MB/s \n",
            "\u001b[?25hCollecting pymorphy2-dicts-ru<3.0,>=2.4\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.2 MB 16.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/dist-packages (from pymorphy2) (0.6.2)\n",
            "Collecting dawg-python>=0.7.1\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Installing collected packages: pymorphy2-dicts-ru, dawg-python, pymorphy2\n",
            "Successfully installed dawg-python-0.7.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3-GnhDsizc0",
        "outputId": "d58e5a06-0fc2-43a2-b50c-d399a9627911"
      },
      "source": [
        "import re\n",
        "from pymorphy2 import MorphAnalyzer\n",
        "from functools import lru_cache\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "m = MorphAnalyzer()\n",
        "regex = re.compile(\"[А-Яа-яA-z]+\")\n",
        "\n",
        "def words_only(text, regex=regex):\n",
        "    try:\n",
        "        return regex.findall(text.lower())\n",
        "    except:\n",
        "        return []"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdjN1W6aizc1"
      },
      "source": [
        "@lru_cache(maxsize=128)\n",
        "def lemmatize_word(token, pymorphy=m):\n",
        "    return pymorphy.parse(token)[0].normal_form\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    return [lemmatize_word(w) for w in text]\n",
        "\n",
        "\n",
        "mystopwords = stopwords.words('russian') \n",
        "def remove_stopwords(lemmas, stopwords = mystopwords):\n",
        "    return [w for w in lemmas if not w in stopwords and len(w) > 3]\n",
        "\n",
        "def clean_text(text):\n",
        "    tokens = words_only(text)\n",
        "    lemmas = lemmatize_text(tokens)\n",
        "    \n",
        "    return ' '.join(remove_stopwords(lemmas))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "HxZ5eFC6izc1",
        "outputId": "bd2f7887-9315-4d4d-9070-647e1aba3e84"
      },
      "source": [
        "from multiprocessing import Pool\n",
        "from tqdm import tqdm\n",
        "\n",
        "with Pool(4) as p:\n",
        "    lemmas = list(tqdm(p.imap(clean_text, df['text']), total=len(df)))\n",
        "    \n",
        "df['lemmas'] = lemmas\n",
        "df.sample(5)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 226834/226834 [07:31<00:00, 501.99it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>lemmas</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>55503</th>\n",
              "      <td>@makeshka да. Причем относительно только самого себя ;)</td>\n",
              "      <td>positive</td>\n",
              "      <td>makeshka причём относительно</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10074</th>\n",
              "      <td>@kristinadorozh1 несколько минут назад закончили разговаривать по скайпу,так что я уже в курсе)</td>\n",
              "      <td>positive</td>\n",
              "      <td>kristinadorozh несколько минута назад закончить разговаривать скайп курс</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66602</th>\n",
              "      <td>Что сегодня за день вообще..терракты,расстрелы,политспоры,кражи...еще и бадун вот(</td>\n",
              "      <td>negative</td>\n",
              "      <td>сегодня день вообще терракт расстрел политспорый кража бадун</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68070</th>\n",
              "      <td>RT @porsheo_o: @Xaibull Я заметил Хуярит не по детский</td>\n",
              "      <td>negative</td>\n",
              "      <td>porsheo_o xaibull заметить хуярить детский</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6271</th>\n",
              "      <td>xxx: у меня сексуальное / расстройство / yyy: Ты серьезно? / xxx: серьезней некуда( / xxx: никто не дает! / расстраиваюсь...</td>\n",
              "      <td>negative</td>\n",
              "      <td>сексуальный расстройство серьёзно серьёзный некуда никто давать расстраиваться</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                               text     label                                                                          lemmas\n",
              "55503                                                                       @makeshka да. Причем относительно только самого себя ;)  positive                                                    makeshka причём относительно\n",
              "10074                               @kristinadorozh1 несколько минут назад закончили разговаривать по скайпу,так что я уже в курсе)  positive        kristinadorozh несколько минута назад закончить разговаривать скайп курс\n",
              "66602                                            Что сегодня за день вообще..терракты,расстрелы,политспоры,кражи...еще и бадун вот(  negative                    сегодня день вообще терракт расстрел политспорый кража бадун\n",
              "68070                                                                        RT @porsheo_o: @Xaibull Я заметил Хуярит не по детский  negative                                      porsheo_o xaibull заметить хуярить детский\n",
              "6271   xxx: у меня сексуальное / расстройство / yyy: Ты серьезно? / xxx: серьезней некуда( / xxx: никто не дает! / расстраиваюсь...  negative  сексуальный расстройство серьёзно серьёзный некуда никто давать расстраиваться"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQJPe5D5izc2"
      },
      "source": [
        "Разбиваем на train и test:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3MD0bex8icC"
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(df.lemmas, df.label)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ppAtTFc8icE"
      },
      "source": [
        "## Мешок слов (Bag of Words, BoW)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gJLFKQ38icE"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression \n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AMGIJ8C8icH"
      },
      "source": [
        "... Но сперва пару слов об n-граммах. Что такое n-граммы:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0-y2A6k8icH"
      },
      "source": [
        "from nltk import ngrams"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_lgEYzY8icJ",
        "outputId": "aac397d9-73dd-4b1f-b6bc-61fa50ba6437"
      },
      "source": [
        "sent = 'Если б мне платили каждый раз'.split()\n",
        "list(ngrams(sent, 1)) # униграммы"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Если',), ('б',), ('мне',), ('платили',), ('каждый',), ('раз',)]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDRi-68C8icM",
        "outputId": "f0cc8130-c21f-43f5-e2fc-57067cfc6cc7"
      },
      "source": [
        "list(ngrams(sent, 2)) # биграммы"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Если', 'б'),\n",
              " ('б', 'мне'),\n",
              " ('мне', 'платили'),\n",
              " ('платили', 'каждый'),\n",
              " ('каждый', 'раз')]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaUeKBDh8icO",
        "outputId": "e2f80184-49ec-4d85-85ca-d39a8390d24f"
      },
      "source": [
        "list(ngrams(sent, 3)) # триграммы"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Если', 'б', 'мне'),\n",
              " ('б', 'мне', 'платили'),\n",
              " ('мне', 'платили', 'каждый'),\n",
              " ('платили', 'каждый', 'раз')]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z325_XfX8icS",
        "outputId": "7dccf868-aea8-439a-8d4e-a2d53f9ce3b4"
      },
      "source": [
        "list(ngrams(sent, 5)) # ... пентаграммы?"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Если', 'б', 'мне', 'платили', 'каждый'),\n",
              " ('б', 'мне', 'платили', 'каждый', 'раз')]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80h0e8FJ8icV"
      },
      "source": [
        "Итак, мы хотим преобразовать наши обработанные данные в вектора с помощью мешка слов. Мешок слов можно строить как для отдельных слов (лемм в нашем случае), так и для n-грамм, и это может улучшать качество. \n",
        "\n",
        "Объект `CountVectorizer` делает простую вещь:\n",
        "* строит для каждого документа (каждой пришедшей ему строки) вектор размерности `n`, где `n` -- количество слов или n-грам во всём корпусе\n",
        "* заполняет каждый i-тый элемент количеством вхождений слова в данный документ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SQaMJbl8icW"
      },
      "source": [
        "vec = CountVectorizer(ngram_range=(1, 1)) # строим BoW для слов\n",
        "bow = vec.fit_transform(x_train) "
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6eZOyAf8icY"
      },
      "source": [
        "ngram_range отвечает за то, какие n-граммы мы используем в качестве признаков:<br/>\n",
        "ngram_range=(1, 1) -- униграммы<br/>\n",
        "ngram_range=(3, 3) -- триграммы<br/>\n",
        "ngram_range=(1, 3) -- униграммы, биграммы и триграммы.\n",
        "\n",
        "В vec.vocabulary_ лежит словарь: соответствие слов и их индексов в словаре:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8cncS9M8icY",
        "outputId": "fada71fe-25f2-4682-8c50-824fe24854ac"
      },
      "source": [
        "list(vec.vocabulary_.items())[:10]"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('знать', 113708),\n",
              " ('готовиться', 104951),\n",
              " ('задание', 111440),\n",
              " ('nika__av', 56469),\n",
              " ('самый', 148926),\n",
              " ('главное', 104168),\n",
              " ('остаться', 133946),\n",
              " ('дело', 106616),\n",
              " ('починить', 141719),\n",
              " ('телефон', 156299)]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojNG2Tbsizc8",
        "outputId": "819fad62-1105-429e-ac26-5b93ae25ff78"
      },
      "source": [
        "bow[0]"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<1x168764 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 3 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZb2wUmSizc8"
      },
      "source": [
        "Теперь у нас есть вектора, на которых можно обучать модели! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1Rq60E68ica",
        "outputId": "defb792b-271a-4db4-f5b8-9be0480b8b3e"
      },
      "source": [
        "clf = LogisticRegression(random_state=42, max_iter=500)\n",
        "clf.fit(bow, y_train)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=500,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdajOFq0izc8"
      },
      "source": [
        "Посмотрим на качество классификации на тестовой выборке. Для этого выведем classification_report из модуля [sklearn.metrics](https://scikit-learn.org/stable/modules/classes.html#sklearn-metrics-metrics)\n",
        "\n",
        "В качестве целевой метрики качества будем рассматривать macro average f1-score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xf8gqHSD8icc",
        "outputId": "68977945-2f1b-4f01-c604-6c81d1ce5ec4"
      },
      "source": [
        "pred = clf.predict(vec.transform(x_test))\n",
        "print(classification_report(pred, y_test))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.74      0.73      0.74     28491\n",
            "    positive       0.74      0.75      0.74     28218\n",
            "\n",
            "    accuracy                           0.74     56709\n",
            "   macro avg       0.74      0.74      0.74     56709\n",
            "weighted avg       0.74      0.74      0.74     56709\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdoG6YCr8icf"
      },
      "source": [
        "Попробуем сделать то же самое для триграмм:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GUDAWcz8icg",
        "outputId": "6f1fb942-cb57-4fd8-eff7-e04dc24a52b5"
      },
      "source": [
        "vec = CountVectorizer(ngram_range=(3, 3))\n",
        "bow = vec.fit_transform(x_train)\n",
        "clf = LogisticRegression(random_state=42, max_iter = 300)\n",
        "clf.fit(bow, y_train)\n",
        "pred = clf.predict(vec.transform(x_test))\n",
        "print(classification_report(pred, y_test))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.97      0.53      0.69     51602\n",
            "    positive       0.15      0.85      0.26      5107\n",
            "\n",
            "    accuracy                           0.56     56709\n",
            "   macro avg       0.56      0.69      0.47     56709\n",
            "weighted avg       0.90      0.56      0.65     56709\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yebYXfuAizc9"
      },
      "source": [
        "Видим, что качество существенно хуже. Ниже мы поймем, почему это так."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PTszK9h8ick"
      },
      "source": [
        "## TF-IDF векторизация"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQ9Td4bw8icm"
      },
      "source": [
        "`TfidfVectorizer` делает то же, что и `CountVectorizer`, но в качестве значений – tf-idf каждого слова.\n",
        "\n",
        "Как считается tf-idf:\n",
        "\n",
        "TF (term frequency) – относительная частотность слова в документе:\n",
        "$$ TF(t,d) = \\frac{n_t}{\\sum_k n_k} $$\n",
        "\n",
        "`t` -- слово (term), `d` -- документ, $n_t$ -- количество вхождений слова, $n_k$ -- количество вхождений остальных слов\n",
        "\n",
        "IDF (inverse document frequency) – обратная частота документов, в которых есть это слово:\n",
        "$$ IDF(t, D) = \\mbox{log} \\frac{|D|}{|{d : t \\in d}|} $$\n",
        "\n",
        "`t` -- слово (term), `D` -- коллекция документов\n",
        "\n",
        "Перемножаем их:\n",
        "$$TFIDF(t,d,D) = TF(t,d) \\times IDF(i, D)$$\n",
        "\n",
        "Ключевая идея этого подхода – если слово часто встречается в одном документе, но в целом по корпусу встречается в небольшом \n",
        "количестве документов, у него высокий TF-IDF."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8B_Q8qP8icm"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tn-S--vi8ico",
        "outputId": "c3b1aa99-3df5-4cf0-cfd7-ad598e061659"
      },
      "source": [
        "vec = TfidfVectorizer(ngram_range=(1, 1))\n",
        "bow = vec.fit_transform(x_train)\n",
        "clf = LogisticRegression(random_state=42, max_iter = 500)\n",
        "clf.fit(bow, y_train)\n",
        "pred = clf.predict(vec.transform(x_test))\n",
        "print(classification_report(pred, y_test))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.70      0.75      0.73     26369\n",
            "    positive       0.77      0.72      0.75     30340\n",
            "\n",
            "    accuracy                           0.74     56709\n",
            "   macro avg       0.74      0.74      0.74     56709\n",
            "weighted avg       0.74      0.74      0.74     56709\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXglD7lb8icq"
      },
      "source": [
        "В этот раз получилось хуже, чем с помощью простого CountVectorizer, то есть использование tf-idf не дало улучшений в качестве. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETq8X_Tb8idz"
      },
      "source": [
        "## О важности эксплоративного анализа\n",
        "\n",
        "Иногда в ходе стандартного препроцессинга теряются важные признаки. Посмотрим, что будет если не убирать пунктуацию?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "fN0CrFgjizc_",
        "outputId": "5aa71fb7-1252-427c-a664-468f28147a3d"
      },
      "source": [
        "df.sample()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>lemmas</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>16930</th>\n",
              "      <td>Бляяя печаль ( я умудрился забыть наушники, походу снова буду по кольцу завтра катать</td>\n",
              "      <td>negative</td>\n",
              "      <td>бляять печаль умудриться забыть наушник поход снова кольцо завтра катать</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                        text     label                                                                    lemmas\n",
              "16930  Бляяя печаль ( я умудрился забыть наушники, походу снова буду по кольцу завтра катать  negative  бляять печаль умудриться забыть наушник поход снова кольцо завтра катать"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "BJZYhFx_izc_",
        "outputId": "2355218c-ca9e-46a7-ba0f-e8dd0bf186fc"
      },
      "source": [
        "df['new_lemmas'] = df.text.apply(lambda x: x.lower())\n",
        "df.sample(3)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>lemmas</th>\n",
              "      <th>new_lemmas</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>87798</th>\n",
              "      <td>@Nina_Leni_W у меня и так настроение не очень, в теперь и это:(</td>\n",
              "      <td>negative</td>\n",
              "      <td>nina_leni_w настроение очень</td>\n",
              "      <td>@nina_leni_w у меня и так настроение не очень, в теперь и это:(</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50214</th>\n",
              "      <td>@Ivp76Natacha76 меня ма не отпустит:( я уроки не сделала:(((</td>\n",
              "      <td>negative</td>\n",
              "      <td>natacha отпустить урок сделать</td>\n",
              "      <td>@ivp76natacha76 меня ма не отпустит:( я уроки не сделала:(((</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89330</th>\n",
              "      <td>@juliamayko @and_Possum Да графиня, по-моему, кончает регулярно. Только мы неудовлетворенными остаемся(((</td>\n",
              "      <td>negative</td>\n",
              "      <td>juliamayko and_possum графиня кончать регулярно неудовлетворённый оставаться</td>\n",
              "      <td>@juliamayko @and_possum да графиня, по-моему, кончает регулярно. только мы неудовлетворенными остаемся(((</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                            text     label                                                                        lemmas                                                                                                 new_lemmas\n",
              "87798                                            @Nina_Leni_W у меня и так настроение не очень, в теперь и это:(  negative                                                  nina_leni_w настроение очень                                            @nina_leni_w у меня и так настроение не очень, в теперь и это:(\n",
              "50214                                               @Ivp76Natacha76 меня ма не отпустит:( я уроки не сделала:(((  negative                                                natacha отпустить урок сделать                                               @ivp76natacha76 меня ма не отпустит:( я уроки не сделала:(((\n",
              "89330  @juliamayko @and_Possum Да графиня, по-моему, кончает регулярно. Только мы неудовлетворенными остаемся(((  negative  juliamayko and_possum графиня кончать регулярно неудовлетворённый оставаться  @juliamayko @and_possum да графиня, по-моему, кончает регулярно. только мы неудовлетворенными остаемся((("
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37Fr0IE7izc_"
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(df.new_lemmas, df.label)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2wKRf0KogZF",
        "outputId": "d6687aae-332c-47bd-98a3-62bfeb3ffba7"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ue0DUsX18id0",
        "outputId": "94257611-c7b1-468e-8563-68e66261f738"
      },
      "source": [
        "from nltk import word_tokenize\n",
        "\n",
        "vec = TfidfVectorizer(ngram_range=(1, 1), tokenizer=word_tokenize)\n",
        "bow = vec.fit_transform(x_train)\n",
        "clf = LogisticRegression(random_state=42, max_iter = 300)\n",
        "clf.fit(bow, y_train)\n",
        "pred = clf.predict(vec.transform(x_test))\n",
        "print(classification_report(pred, y_test))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       1.00      1.00      1.00     27940\n",
            "    positive       1.00      1.00      1.00     28769\n",
            "\n",
            "    accuracy                           1.00     56709\n",
            "   macro avg       1.00      1.00      1.00     56709\n",
            "weighted avg       1.00      1.00      1.00     56709\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wymeD7-B8id3"
      },
      "source": [
        "Как можно видеть, если оставить пунктуацию, то все метрики равны 1. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SehmRUiTizdA",
        "outputId": "8329b169-8499-4b15-f73b-8d6cc7cb75e2"
      },
      "source": [
        "len(vec.vocabulary_), len(clf.coef_[0])"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(266555, 266555)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGKuAvYR8id3",
        "outputId": "94c3ccd7-c5ef-46ab-e9de-e8c06a624e70"
      },
      "source": [
        "importances = list(zip(vec.vocabulary_, clf.coef_[0]))\n",
        "importances[0]"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('ну', 0.03588456855305283)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3K9uJ5-NizdB",
        "outputId": "5de97df1-ed6b-4a7d-ca4d-6d531d192481"
      },
      "source": [
        "sorted_importances = sorted(importances, key = lambda x: -x[1])\n",
        "sorted_importances[:10]"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('лучшие', 58.780667426514725),\n",
              " ('_honey_bitch_', 27.180527319648036),\n",
              " ('!', 12.59377111063971),\n",
              " ('била', 10.845916489229035),\n",
              " ('соболезнование', 9.16521786618053),\n",
              " ('норм.я', 8.032252276567048),\n",
              " ('бутылку', 7.735863469448331),\n",
              " ('@', 5.394435209535867),\n",
              " ('указа', 5.118003898803474),\n",
              " ('любимку', 4.874465978465524)]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QxmioaZ8id8"
      },
      "source": [
        "Посмотрим, как один из наиболее значительных токенов справится с классификацией безо всякого машинного обучения:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17LPHPGR8id9",
        "outputId": "d41bdffe-8f20-4a44-e615-01a2ef34d469"
      },
      "source": [
        "cool_token = ')'\n",
        "pred = ['positive' if cool_token in tweet else 'negative' for tweet in x_test]\n",
        "print(classification_report(pred, y_test))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       1.00      0.85      0.92     32943\n",
            "    positive       0.83      1.00      0.91     23766\n",
            "\n",
            "    accuracy                           0.91     56709\n",
            "   macro avg       0.91      0.92      0.91     56709\n",
            "weighted avg       0.93      0.91      0.91     56709\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcSeIR5RizdC"
      },
      "source": [
        "Можно видеть, что это уже позволяет достаточно хорошо классифицировать тексты."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7VjSCog8id_"
      },
      "source": [
        "## Символьные n-граммы\n",
        "\n",
        "Теперь в качестве признаком используем, например, униграммы символов:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSO-k4wA8id_",
        "outputId": "8d78bd4d-cd9b-4871-8aee-12cd60e9df56"
      },
      "source": [
        "vec = CountVectorizer(analyzer='char', ngram_range=(1, 1))\n",
        "bow = vec.fit_transform(x_train)\n",
        "clf = LogisticRegression(random_state=42)\n",
        "clf.fit(bow, y_train)\n",
        "pred = clf.predict(vec.transform(x_test))\n",
        "print(classification_report(pred, y_test))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.99      1.00      1.00     27855\n",
            "    positive       1.00      0.99      1.00     28854\n",
            "\n",
            "    accuracy                           1.00     56709\n",
            "   macro avg       1.00      1.00      1.00     56709\n",
            "weighted avg       1.00      1.00      1.00     56709\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9i6JwqL-8ieE"
      },
      "source": [
        "Таким образом, становится понятно, почему на этих данных качество классификации 1. Так или иначе, на символах классифицировать тоже можно.\n",
        "\n",
        "Ещё одна замечательная особенность символьных признаков: токенизация и лемматизация не нужна, можно использовать такой подход для языков, у которых нет готовых анализаторов."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LG1GBwcTizdC"
      },
      "source": [
        "## Итоги\n",
        "\n",
        " На этом занятии мы\n",
        "* познакомились с задачей бинарной классификации текстов.\n",
        "\n",
        "* научились строить простые признаки на основе метода \"мешка слов\" с помощью библиотеки sklearn: CountVectorizer и TfidfVectorizer.\n",
        "\n",
        "* использовали для классификации линейную модель логистической регрессии.\n",
        "\n",
        "* поняли, что многое зависит от подхода к предобработки текста и от признаков, которые используются в модели.\n",
        "\n",
        "* увидели, что в некоторых задачах важно использование каждого символа из текста, в том числе пунктуации.\n",
        "\n",
        "На следующих занятиях мы рассмотрим более сложные модели построения признаков и классификации текстов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4e33FzdmizdC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}