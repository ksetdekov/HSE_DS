# общее
1. использовать класс для создания сетей
2. посмотреть, что такое spark.pickle

# эвристики в обучении
## функции активации


1. сигмоида

    * у нее положительный знак, значит вход в следующий слой положительный и градиент приведет к изменению всех весов или в + или в -
    * вычислять ее дорого

2. $\tanh$

    * центрирован относительно нуля
    * похож на сигмойду
    * минус, что возможно затухания гардиента $f'(x) = 1 - f(x)^2$

3. ReLU

    * не затухает
    * быстро делать
    * быстро сходится
    * если занулить все нейроны, могут умиреть нейроны (если инициализировать $w_0$ большим числом) - это смерть нейрона

4. Leaky ReLU

    * как ReLU в + и в - $f(y) = ay$
    * производная может быть любого знака
    * важно $a \neq 1$


Подход - брать ReLU, если плохо - брать Leaky ReLU. Для сверточных сетей - $\tanh$

## инициализация весов
### нулями
    
плохо
1. или не будет двигаться (релу и $\tanh$)
2. или будет только на константу двигаться ($sigmoid$)

### лучше
 маленькими рандомными числами из
какого-то распределения (нормальное, равномерное)

1. Хотим чтобы дисперсия на вход слоя и после слоя (до активации) были равны и равны константе
2. вывыдем из это требование
3. варианты:
    
    1. равномерное:
        
        * будет затухание
    2. равномерное, уномноженное на $\sqrt{3}$

        * норм будет
    3. Инициализация Ксавье / Глоро (http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf) для симметричного

    4. Xe Kaiming инициализация для несимметричных весов