# Важное по нейронкам:
1. стандартизировать фичи
2. Sigmoid, как уже упоминалось, она может приводить к обнулению градиентов, и сеть перестают хорошо обучаться. Поэтому можно попробовать другие нелинейные функции, например, ReLU. Но наиболее остро проблема с Sigmoid ощущается именно в глубоких нейронных сетях. 
  1. ReLu
  2. LeakyReLU - не занулит фичу очень разреженную
3. поиграть с числом нейронов и слоев
  1. два слоя, и количество нейронов не меньше двух. (Chester, D.L. (1990), "Why Two Hidden Layers are Better than One"
  2. про число нейронов в слое:
  
    1. Количество нейронов в скрытом слое: в промежутке сколько входных характеристик, и сколько выходных значений (Blum, A. (1992), Neural Networks in C++, NY: Wiley)
    2. (Number of inputs + outputs) * (2/3)
    3. Количество нейроной в скрытом слое не должно быть больше, чем в два раза размера входного тензора. Swingler, K. (1996), Applying Neural Networks: A Practical Guide, London: Academic Press.
    4. Зависит от количество наблюдений в обучающей выборке. Для данных размером от 150 до 2500 элементов 20 нейронов достаточно для одного скрытого слоя.   
    
4. использовать torch.cuda
