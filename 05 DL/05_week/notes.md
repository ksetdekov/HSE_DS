# Батч нормализация

Batch norm - все факторы стандартизированные на каждом слое

После этого прибавляем $\beta \cdot \hat{h} + \gamma$ после каждого фактора на вход следующего

## трюки
* с батч-нормализацией нужно уменьшить силу регуляризацию
* не забывайте преемешивать обучающую выборку перед каждой эпохой чтобы получать разные батчи

# Dropout

https://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf - статья

на Forward pass умножаем выход после функции активации зануляем.

# Дргуие евристики
## предобучение - предобучить части сети
## динамическо наращивание сети
учим на первой эпохе в мало нейронов, добавляем нейроны после стабилизации
## прореживание сети
начать с большого числа нейронов, удалять незначимые по каком-нибудь критерию

## early stopping
## регуляризация


