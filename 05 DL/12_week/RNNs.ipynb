{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOOLQMEgt6KJ"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://github.com/NktBkzn/HSE_DL_2021/blob/master/12_week/RNNs.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nje_LngItrLy"
      },
      "source": [
        "- Ноутбук заимствован с курса [dlcourse.ai](https://dlcourse.ai)\n",
        "- Решение ноутбука взято [отсюда](https://github.com/omega1996/dlcourse/blob/master/assignments/assignment6/RNNs.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiPbD_lTXUMd"
      },
      "source": [
        "# Задание 6: Рекуррентные нейронные сети (RNNs)\n",
        "\n",
        "Это задание адаптиповано из Deep NLP Course at ABBYY (https://github.com/DanAnastasyev/DeepNLP-Course) с разрешения автора - Даниила Анастасьева. Спасибо ему огромное!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P59NYU98GCb9",
        "outputId": "4490de6c-f3d4-4d76-9838-54fbcff75e2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim==4.1.2 in c:\\users\\ksetd\\anaconda3\\lib\\site-packages (4.1.2)\n",
            "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\ksetd\\anaconda3\\lib\\site-packages (from gensim==4.1.2) (1.7.1)\n",
            "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\ksetd\\anaconda3\\lib\\site-packages (from gensim==4.1.2) (1.21.3)\n",
            "Requirement already satisfied: Cython==0.29.23 in c:\\users\\ksetd\\anaconda3\\lib\\site-packages (from gensim==4.1.2) (0.29.23)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\ksetd\\anaconda3\\lib\\site-packages (from gensim==4.1.2) (5.2.1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution -illow (c:\\users\\ksetd\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -illow (c:\\users\\ksetd\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -illow (c:\\users\\ksetd\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -illow (c:\\users\\ksetd\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -illow (c:\\users\\ksetd\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -illow (c:\\users\\ksetd\\anaconda3\\lib\\site-packages)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting nltk==3.6.3\n",
            "  Downloading nltk-3.6.3-py3-none-any.whl (1.5 MB)\n",
            "Requirement already satisfied: tqdm in c:\\users\\ksetd\\anaconda3\\lib\\site-packages (from nltk==3.6.3) (4.61.0)\n",
            "Requirement already satisfied: regex in c:\\users\\ksetd\\anaconda3\\lib\\site-packages (from nltk==3.6.3) (2021.4.4)\n",
            "Requirement already satisfied: click in c:\\users\\ksetd\\anaconda3\\lib\\site-packages (from nltk==3.6.3) (7.1.2)\n",
            "Requirement already satisfied: joblib in c:\\users\\ksetd\\anaconda3\\lib\\site-packages (from nltk==3.6.3) (1.0.1)\n",
            "Installing collected packages: nltk\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.6.2\n",
            "    Uninstalling nltk-3.6.2:\n",
            "      Successfully uninstalled nltk-3.6.2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution -illow (c:\\users\\ksetd\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -illow (c:\\users\\ksetd\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -illow (c:\\users\\ksetd\\anaconda3\\lib\\site-packages)\n",
            "    WARNING: Ignoring invalid distribution -illow (c:\\users\\ksetd\\anaconda3\\lib\\site-packages)\n",
            "ERROR: Could not install packages due to an OSError: [WinError 5] Отказано в доступе: 'C:\\\\Users\\\\ksetd\\\\AppData\\\\Local\\\\Temp\\\\pip-uninstall-radzi3g1\\\\nltk.exe'\n",
            "Consider using the `--user` option or check the permissions.\n",
            "\n",
            "WARNING: Ignoring invalid distribution -illow (c:\\users\\ksetd\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -illow (c:\\users\\ksetd\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -illow (c:\\users\\ksetd\\anaconda3\\lib\\site-packages)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting scikit-learn==1.0.1\n",
            "  Downloading scikit_learn-1.0.1-cp38-cp38-win_amd64.whl (7.2 MB)\n",
            "Requirement already satisfied: joblib>=0.11 in c:\\users\\ksetd\\anaconda3\\lib\\site-packages (from scikit-learn==1.0.1) (1.0.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\ksetd\\anaconda3\\lib\\site-packages (from scikit-learn==1.0.1) (2.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\ksetd\\anaconda3\\lib\\site-packages (from scikit-learn==1.0.1) (1.7.1)\n",
            "Requirement already satisfied: numpy>=1.14.6 in c:\\users\\ksetd\\anaconda3\\lib\\site-packages (from scikit-learn==1.0.1) (1.21.3)\n",
            "Installing collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 0.24.2\n",
            "    Uninstalling scikit-learn-0.24.2:\n",
            "      Successfully uninstalled scikit-learn-0.24.2\n",
            "Successfully installed scikit-learn-1.0.1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution -illow (c:\\users\\ksetd\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -illow (c:\\users\\ksetd\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -illow (c:\\users\\ksetd\\anaconda3\\lib\\site-packages)\n",
            "    WARNING: Ignoring invalid distribution -illow (c:\\users\\ksetd\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -illow (c:\\users\\ksetd\\anaconda3\\lib\\site-packages)\n",
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "perceptilabs 0.12.25 requires Django==3.2, but you have django 3.1.13 which is incompatible.\n",
            "perceptilabs 0.12.25 requires numpy<=1.19.2,>=1.18.5, but you have numpy 1.21.3 which is incompatible.\n",
            "perceptilabs 0.12.25 requires pillow==7.0.0, but you have pillow 8.3.2 which is incompatible.\n",
            "perceptilabs 0.12.25 requires pytz>=2020.1, but you have pytz 2019.3 which is incompatible.\n",
            "WARNING: Ignoring invalid distribution -illow (c:\\users\\ksetd\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -illow (c:\\users\\ksetd\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -illow (c:\\users\\ksetd\\anaconda3\\lib\\site-packages)\n"
          ]
        }
      ],
      "source": [
        "!pip install gensim==4.1.2\n",
        "!pip install nltk==3.6.3\n",
        "!pip install scikit-learn==1.0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1TvJm34-fd5_",
        "outputId": "81928b78-445f-4b6a-96d7-307f3f69d300"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'3.6.3'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8sVtGHmA9aBM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    from torch.cuda import FloatTensor, LongTensor\n",
        "else:\n",
        "    from torch import FloatTensor, LongTensor\n",
        "\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6CNKM3b4hT1"
      },
      "source": [
        "# Рекуррентные нейронные сети (RNNs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_XkoGNQUeGm"
      },
      "source": [
        "## POS Tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFEtWrS_4rUs"
      },
      "source": [
        "Мы рассмотрим применение рекуррентных сетей к задаче sequence labeling (последняя картинка).\n",
        "\n",
        "![RNN types](http://karpathy.github.io/assets/rnn/diags.jpeg)\n",
        "\n",
        "*From [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)*\n",
        "\n",
        "Самые популярные примеры для такой постановки задачи - Part-of-Speech Tagging и Named Entity Recognition.\n",
        "\n",
        "Мы порешаем сейчас POS Tagging для английского.\n",
        "\n",
        "Будем работать с таким набором тегов:\n",
        "- ADJ - adjective (new, good, high, ...)\n",
        "- ADP - adposition (on, of, at, ...)\n",
        "- ADV - adverb (really, already, still, ...)\n",
        "- CONJ - conjunction (and, or, but, ...)\n",
        "- DET - determiner, article (the, a, some, ...)\n",
        "- NOUN - noun (year, home, costs, ...)\n",
        "- NUM - numeral (twenty-four, fourth, 1991, ...)\n",
        "- PRT - particle (at, on, out, ...)\n",
        "- PRON - pronoun (he, their, her, ...)\n",
        "- VERB - verb (is, say, told, ...)\n",
        "- . - punctuation marks (. , ;)\n",
        "- X - other (ersatz, esprit, dunno, ...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPIkKdFlHB-X"
      },
      "source": [
        "Скачаем данные:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TiA2dGmgF1rW",
        "outputId": "8843d1b3-2076-4186-f2f9-7ae59a373799"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to\n",
            "[nltk_data]     C:\\Users\\ksetd\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package universal_tagset to\n",
            "[nltk_data]     C:\\Users\\ksetd\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "nltk.download('brown')\n",
        "nltk.download('universal_tagset')\n",
        "\n",
        "data = nltk.corpus.brown.tagged_sents(tagset='universal')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTNDocatrWAv",
        "outputId": "e8669b78-55e7-4283-80e1-cc89519b43b9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(57340, 25, 43)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(data), len(data[0]), len(data[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d93g_swyJA_V"
      },
      "source": [
        "Пример размеченного предложения:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QstS4NO0L97c",
        "outputId": "5f11fdd6-f5b4-4d80-d0ae-ec891daaadd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The            \tDET\n",
            "Fulton         \tNOUN\n",
            "County         \tNOUN\n",
            "Grand          \tADJ\n",
            "Jury           \tNOUN\n",
            "said           \tVERB\n",
            "Friday         \tNOUN\n",
            "an             \tDET\n",
            "investigation  \tNOUN\n",
            "of             \tADP\n",
            "Atlanta's      \tNOUN\n",
            "recent         \tADJ\n",
            "primary        \tNOUN\n",
            "election       \tNOUN\n",
            "produced       \tVERB\n",
            "``             \t.\n",
            "no             \tDET\n",
            "evidence       \tNOUN\n",
            "''             \t.\n",
            "that           \tADP\n",
            "any            \tDET\n",
            "irregularities \tNOUN\n",
            "took           \tVERB\n",
            "place          \tNOUN\n",
            ".              \t.\n"
          ]
        }
      ],
      "source": [
        "for word, tag in data[0]:\n",
        "    print('{:15}\\t{}'.format(word, tag))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epdW8u_YXcAv"
      },
      "source": [
        "Построим разбиение на train/val/test - наконец-то, всё как у нормальных людей.\n",
        "\n",
        "На train будем учиться, по val - подбирать параметры и делать всякие early stopping, а на test - принимать модель по ее финальному качеству."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTai8Ta0lgwL",
        "outputId": "93fa1ec8-ab03-46bf-c7a4-d33819c606fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Words count in train set: 739769\n",
            "Words count in val set: 130954\n",
            "Words count in test set: 290469\n"
          ]
        }
      ],
      "source": [
        "train_data, test_data = train_test_split(data, test_size=0.25, random_state=42)\n",
        "train_data, val_data = train_test_split(train_data, test_size=0.15, random_state=42)\n",
        "\n",
        "print('Words count in train set:', sum(len(sent) for sent in train_data))\n",
        "print('Words count in val set:', sum(len(sent) for sent in val_data))\n",
        "print('Words count in test set:', sum(len(sent) for sent in test_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eChdLNGtXyP0"
      },
      "source": [
        "Построим маппинги из слов в индекс и из тега в индекс:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCjwwDs6Zq9x",
        "outputId": "df7e014b-3817-4f32-f2b4-23bbd67e4c41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique words in train = 45441. Tags = {'DET', 'X', 'ADP', 'NOUN', 'CONJ', 'ADJ', 'ADV', 'PRT', 'NUM', '.', 'PRON', 'VERB'}\n"
          ]
        }
      ],
      "source": [
        "words = {word for sample in train_data for word, tag in sample}\n",
        "word2ind = {word: ind + 1 for ind, word in enumerate(words)}  # ind + 1 to leave 0 idx blank for <pad>\n",
        "word2ind['<pad>'] = 0\n",
        "\n",
        "tags = {tag for sample in train_data for word, tag in sample}\n",
        "tag2ind = {tag: ind + 1 for ind, tag in enumerate(tags)}\n",
        "tag2ind['<pad>'] = 0\n",
        "\n",
        "print('Unique words in train = {}. Tags = {}'.format(len(word2ind), tags))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "1nGShrtNr59m"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'conclusion': 1,\n",
              " 'tracts': 2,\n",
              " 'chump': 3,\n",
              " 'Chadroe': 4,\n",
              " 'passions': 5,\n",
              " 'fellows': 6,\n",
              " 'single': 7,\n",
              " 'lamb': 8,\n",
              " 'ethically': 9,\n",
              " 'Seaboard': 10,\n",
              " 'eluding': 11,\n",
              " 'Scotch-and-soda': 12,\n",
              " 'prescriptive': 13,\n",
              " \"Jannequin's\": 14,\n",
              " 'doctor': 15,\n",
              " 'boulder': 16,\n",
              " 'filing': 17,\n",
              " 'immobility': 18,\n",
              " 'imperturbable': 19,\n",
              " 'wintered': 20,\n",
              " 'implantation': 21,\n",
              " 'downpayment': 22,\n",
              " 'bouanahsha': 23,\n",
              " 'unlawful': 24,\n",
              " 'eyebrows': 25,\n",
              " 'underwriter': 26,\n",
              " 'Benched': 27,\n",
              " 'skirts': 28,\n",
              " 'maturation': 29,\n",
              " 'hyacinths': 30,\n",
              " 'Taos': 31,\n",
              " 'user': 32,\n",
              " 'frictions': 33,\n",
              " 'dragged': 34,\n",
              " 'dack-rihs': 35,\n",
              " 'consideration': 36,\n",
              " 'autopsy': 37,\n",
              " 'tiredly': 38,\n",
              " 'road': 39,\n",
              " 'Remington': 40,\n",
              " 'perfidious': 41,\n",
              " 'Charlotte': 42,\n",
              " \"batter's\": 43,\n",
              " 'axe': 44,\n",
              " 'Death': 45,\n",
              " 'Lucian': 46,\n",
              " 'venerable': 47,\n",
              " 'voiced': 48,\n",
              " 'swept': 49,\n",
              " 'adaptable': 50,\n",
              " 'Singing': 51,\n",
              " 'proposition': 52,\n",
              " 'motor': 53,\n",
              " 'overpowered': 54,\n",
              " 'law-unto-itself': 55,\n",
              " 'burlesques': 56,\n",
              " 'blood-chilling': 57,\n",
              " 'relationship': 58,\n",
              " 'Chromium': 59,\n",
              " 'clouded': 60,\n",
              " 'Ky.': 61,\n",
              " 'Raymondville': 62,\n",
              " 'Entries': 63,\n",
              " 'zoomed': 64,\n",
              " 'manure': 65,\n",
              " 'Abe': 66,\n",
              " 'sportsman': 67,\n",
              " 'veneer': 68,\n",
              " 'consolidation': 69,\n",
              " 'showcase': 70,\n",
              " 'shortstop': 71,\n",
              " 'cautions': 72,\n",
              " 'fetch': 73,\n",
              " 'Bankhead': 74,\n",
              " 'mythologies': 75,\n",
              " 'valiant': 76,\n",
              " 'coolest': 77,\n",
              " 'stampede': 78,\n",
              " 'fellers': 79,\n",
              " 'sensor': 80,\n",
              " 'wished': 81,\n",
              " 'precipitate': 82,\n",
              " 'deadlock': 83,\n",
              " 'serial': 84,\n",
              " 'M.P.': 85,\n",
              " 'imperfections': 86,\n",
              " 'Morley': 87,\n",
              " 'Yamabe': 88,\n",
              " 'stiffer': 89,\n",
              " 'comedy': 90,\n",
              " 'perforated': 91,\n",
              " 'Drag': 92,\n",
              " 'Apollo': 93,\n",
              " \"Mendelssohn's\": 94,\n",
              " 'ah': 95,\n",
              " \"Burnham's\": 96,\n",
              " 'Francie': 97,\n",
              " 'coffeecup': 98,\n",
              " 'Ignazio': 99,\n",
              " 'gai': 100,\n",
              " 'cantaloupe': 101,\n",
              " 'unforeseen': 102,\n",
              " 'Gog': 103,\n",
              " 'resigned': 104,\n",
              " 'obtaine': 105,\n",
              " 'modes': 106,\n",
              " 'Priam': 107,\n",
              " 'raised': 108,\n",
              " 'Spiritual': 109,\n",
              " 'reserve': 110,\n",
              " 'speaking': 111,\n",
              " 'soothingly': 112,\n",
              " 'vegetable': 113,\n",
              " 'Throne': 114,\n",
              " 'Hausman': 115,\n",
              " 'Khasi': 116,\n",
              " 'dodge': 117,\n",
              " 'Spade': 118,\n",
              " 'cheated': 119,\n",
              " 'animized': 120,\n",
              " 'Rifle': 121,\n",
              " 'Piccadilly': 122,\n",
              " 'Miglia': 123,\n",
              " 'numerological': 124,\n",
              " 'Urge': 125,\n",
              " 'perversely': 126,\n",
              " 'Selkirk': 127,\n",
              " 'Saabye': 128,\n",
              " 'puzzling': 129,\n",
              " 'slowest': 130,\n",
              " 'chute': 131,\n",
              " 'Marcel': 132,\n",
              " 'deft': 133,\n",
              " 'Protectorate': 134,\n",
              " 'patrols': 135,\n",
              " '1951': 136,\n",
              " 'upheld': 137,\n",
              " 'diethylaminoethyl': 138,\n",
              " 'explanations': 139,\n",
              " 'University': 140,\n",
              " 'snug-fitting': 141,\n",
              " 'rinse': 142,\n",
              " 'polymers': 143,\n",
              " 'woke': 144,\n",
              " 'scimitar-wielding': 145,\n",
              " 'Adventures': 146,\n",
              " 'Harburg': 147,\n",
              " \"Colorado's\": 148,\n",
              " 'posts': 149,\n",
              " '2100': 150,\n",
              " \"Marx's\": 151,\n",
              " 'multi-valued': 152,\n",
              " 'verandah': 153,\n",
              " 'academies': 154,\n",
              " 'Purchase': 155,\n",
              " 'Presumably': 156,\n",
              " 'perpetrated': 157,\n",
              " 'decisive': 158,\n",
              " 'Times-Picayune': 159,\n",
              " 'twirled': 160,\n",
              " 'Religious': 161,\n",
              " 'kissed': 162,\n",
              " 'Rabbi': 163,\n",
              " 'sabre': 164,\n",
              " 'coup-proof': 165,\n",
              " 'usefulness': 166,\n",
              " 'eventually': 167,\n",
              " 'concur': 168,\n",
              " 'carbohydrate': 169,\n",
              " 'thankful': 170,\n",
              " 'abreast': 171,\n",
              " 'weakening': 172,\n",
              " 'inch': 173,\n",
              " 'operative': 174,\n",
              " 'pinnacle': 175,\n",
              " 'Laue': 176,\n",
              " 'mark': 177,\n",
              " 'Plus': 178,\n",
              " 'leaks': 179,\n",
              " 'day-to-day': 180,\n",
              " \"Manchester's\": 181,\n",
              " 'hoop': 182,\n",
              " 'ashen': 183,\n",
              " 'miracle': 184,\n",
              " 'light-transmitting': 185,\n",
              " 'Osaka': 186,\n",
              " 'marriage': 187,\n",
              " 'stabilized': 188,\n",
              " 'thirteenth-century': 189,\n",
              " 'pre-drilled': 190,\n",
              " 'consoles': 191,\n",
              " '15.0': 192,\n",
              " 'queen': 193,\n",
              " 'convictions': 194,\n",
              " 'calculable': 195,\n",
              " 'Layton': 196,\n",
              " 'require': 197,\n",
              " 'earns': 198,\n",
              " 'sprang': 199,\n",
              " 'splashy': 200,\n",
              " 'compromises': 201,\n",
              " 'irreverent': 202,\n",
              " 'carrot': 203,\n",
              " 'injustice': 204,\n",
              " 'prancing': 205,\n",
              " '4:30': 206,\n",
              " 'fill-in': 207,\n",
              " 'workers': 208,\n",
              " 'urgent': 209,\n",
              " \"Hardee's\": 210,\n",
              " 'Buchanan': 211,\n",
              " 'demythologize': 212,\n",
              " 'homopolymers': 213,\n",
              " 'saleslady': 214,\n",
              " 'guttural': 215,\n",
              " 'some': 216,\n",
              " 'Karen': 217,\n",
              " 'registrations': 218,\n",
              " 'frame': 219,\n",
              " 'allocable': 220,\n",
              " 'awed': 221,\n",
              " 'lacquer': 222,\n",
              " 'Greek': 223,\n",
              " 'communicator': 224,\n",
              " 'iniquitous': 225,\n",
              " 'revolts': 226,\n",
              " 'prodigious': 227,\n",
              " 'prolusions': 228,\n",
              " \"syndicate's\": 229,\n",
              " 'nutritive': 230,\n",
              " 'raucously': 231,\n",
              " 'Geroge': 232,\n",
              " 'bumble-bee': 233,\n",
              " 'disloyalty': 234,\n",
              " 'bouffant': 235,\n",
              " 'volley-ball': 236,\n",
              " \"Madden's\": 237,\n",
              " 'humanist': 238,\n",
              " 'Expressways': 239,\n",
              " '1943': 240,\n",
              " 'thickening': 241,\n",
              " 'watching': 242,\n",
              " 'Interview': 243,\n",
              " 'Commonwealth': 244,\n",
              " 'hazy': 245,\n",
              " 'behaves': 246,\n",
              " 'jumbled': 247,\n",
              " 'Ivies': 248,\n",
              " 'presaged': 249,\n",
              " 'tensile': 250,\n",
              " 'love-in-action': 251,\n",
              " 'tiers': 252,\n",
              " 'mystics': 253,\n",
              " '75': 254,\n",
              " '1290': 255,\n",
              " 'socializes': 256,\n",
              " 'offend': 257,\n",
              " 'congressional': 258,\n",
              " '46,000': 259,\n",
              " 'Greenville': 260,\n",
              " 'specimen': 261,\n",
              " 'oversimplified': 262,\n",
              " 'rigging': 263,\n",
              " 'contemplates': 264,\n",
              " 'puissant': 265,\n",
              " 'upstate': 266,\n",
              " 'world': 267,\n",
              " 'Dragnet': 268,\n",
              " \"M.E.'s\": 269,\n",
              " 'merge': 270,\n",
              " 'pact': 271,\n",
              " 'Culture': 272,\n",
              " 'squirmed': 273,\n",
              " 'Muse': 274,\n",
              " 'Ohio': 275,\n",
              " 'Controls': 276,\n",
              " 'sinusoids': 277,\n",
              " 'impart': 278,\n",
              " '$10.8': 279,\n",
              " 'creations': 280,\n",
              " 'philosophic': 281,\n",
              " 'arising': 282,\n",
              " 'arteries': 283,\n",
              " 'Expressway': 284,\n",
              " 'drawings': 285,\n",
              " 'tireless': 286,\n",
              " 'Bathing': 287,\n",
              " 'reporting': 288,\n",
              " 'repentance': 289,\n",
              " 'Abraham': 290,\n",
              " 'advertising-conscious': 291,\n",
              " 'Hidden': 292,\n",
              " 'pecs': 293,\n",
              " 'intuitively': 294,\n",
              " 'missions': 295,\n",
              " 'wiping': 296,\n",
              " 'Tims': 297,\n",
              " 'Jerry': 298,\n",
              " 'Siepi': 299,\n",
              " 'ad': 300,\n",
              " 'Henrietta': 301,\n",
              " 'Micelles': 302,\n",
              " 'metals': 303,\n",
              " 'cowboy': 304,\n",
              " 'access': 305,\n",
              " 'Ephesians': 306,\n",
              " 'tack': 307,\n",
              " 'depravities': 308,\n",
              " 'blundered': 309,\n",
              " 'Coleman': 310,\n",
              " 'questioning': 311,\n",
              " 'averting': 312,\n",
              " 'one-time': 313,\n",
              " 'Mando': 314,\n",
              " 'Rules': 315,\n",
              " 'reinforcements': 316,\n",
              " 'bleeps': 317,\n",
              " 'monumental': 318,\n",
              " 'bother': 319,\n",
              " 'Graphic': 320,\n",
              " 'transferee': 321,\n",
              " 'bubbling': 322,\n",
              " 'seduced': 323,\n",
              " 'boldness': 324,\n",
              " 'parentage': 325,\n",
              " 'divert': 326,\n",
              " 'ingratiating': 327,\n",
              " 'Boogie': 328,\n",
              " 'reoriented': 329,\n",
              " 'niggers': 330,\n",
              " 'Denverite': 331,\n",
              " 'bargain-priced': 332,\n",
              " 'Bodenheim': 333,\n",
              " 'unclear': 334,\n",
              " 'plastic': 335,\n",
              " 'budget-wise': 336,\n",
              " 'clothier': 337,\n",
              " 'Hawaiian': 338,\n",
              " 'show-offy': 339,\n",
              " 'craters': 340,\n",
              " 'man-to-man': 341,\n",
              " 'Kawecki': 342,\n",
              " 'chirping': 343,\n",
              " 'LP': 344,\n",
              " 'dawn': 345,\n",
              " 'rhinestones': 346,\n",
              " 'incidence': 347,\n",
              " 'repeater': 348,\n",
              " 'executors': 349,\n",
              " 'Melancholy': 350,\n",
              " 'Says': 351,\n",
              " \"Chavez'\": 352,\n",
              " 'speechless': 353,\n",
              " \"diplomat's\": 354,\n",
              " 'electoral': 355,\n",
              " 'senilis': 356,\n",
              " 'badmen': 357,\n",
              " 'extra-thick': 358,\n",
              " 'Lorenz': 359,\n",
              " '16.7': 360,\n",
              " 'Resistance': 361,\n",
              " 'replies': 362,\n",
              " 'personified': 363,\n",
              " 'One-armed': 364,\n",
              " 'Invictus': 365,\n",
              " 'bunt': 366,\n",
              " 'vacationland': 367,\n",
              " 'vehicle': 368,\n",
              " 'families': 369,\n",
              " 'Courcy': 370,\n",
              " '185th': 371,\n",
              " 'Thakhek': 372,\n",
              " 'experimenter': 373,\n",
              " 'turnout': 374,\n",
              " 'scourge': 375,\n",
              " 'meets': 376,\n",
              " 'Deauville': 377,\n",
              " 'Fraud': 378,\n",
              " 'cranberries': 379,\n",
              " 'souffle': 380,\n",
              " 'Shah': 381,\n",
              " 'catered': 382,\n",
              " 'Matching': 383,\n",
              " 'Hints': 384,\n",
              " 'bestes': 385,\n",
              " 'Vapor': 386,\n",
              " 'sallying': 387,\n",
              " 'literally': 388,\n",
              " 'Repnin': 389,\n",
              " 'intonaco': 390,\n",
              " 'directorate': 391,\n",
              " 'bankers': 392,\n",
              " 'dishes': 393,\n",
              " 'mails': 394,\n",
              " 'Hayek': 395,\n",
              " 'pushed': 396,\n",
              " 'brandishing': 397,\n",
              " 'parapets': 398,\n",
              " 'Brew': 399,\n",
              " '42': 400,\n",
              " \"Puttin'\": 401,\n",
              " 'hopped': 402,\n",
              " 'Awake': 403,\n",
              " 'harassed': 404,\n",
              " 'psychotherapeutic': 405,\n",
              " 'Iglehart': 406,\n",
              " 'Coble': 407,\n",
              " 'Jansenist': 408,\n",
              " '21-2': 409,\n",
              " 'Tamiris-Daniel': 410,\n",
              " 'Marmon': 411,\n",
              " 'chills': 412,\n",
              " 'otter': 413,\n",
              " 'closer': 414,\n",
              " 'Depicted': 415,\n",
              " 'superseded': 416,\n",
              " 'emerging': 417,\n",
              " 'Arden': 418,\n",
              " 'cherishing': 419,\n",
              " 'Danbury': 420,\n",
              " 'Eating': 421,\n",
              " 'installed': 422,\n",
              " 'volunteering': 423,\n",
              " 'bounding': 424,\n",
              " 'lithe': 425,\n",
              " \"Giants'\": 426,\n",
              " 'Northeastern': 427,\n",
              " 'Huggins': 428,\n",
              " 'Lavoisier': 429,\n",
              " 'friendship': 430,\n",
              " 'prevayle': 431,\n",
              " 'braced': 432,\n",
              " 'mostly': 433,\n",
              " 'parades': 434,\n",
              " \"Crouch's\": 435,\n",
              " 'Vocal': 436,\n",
              " 'Pilots': 437,\n",
              " 'Torrid-Mighty': 438,\n",
              " 'Norfolk': 439,\n",
              " 'Rawson': 440,\n",
              " 'sinews': 441,\n",
              " 'Works': 442,\n",
              " 'reverse': 443,\n",
              " 'turtleneck': 444,\n",
              " 'sadistic': 445,\n",
              " '173': 446,\n",
              " 'minter': 447,\n",
              " 'referent': 448,\n",
              " 'Canyon': 449,\n",
              " \"Foundation's\": 450,\n",
              " '$43,000': 451,\n",
              " 'cytolysis': 452,\n",
              " '387': 453,\n",
              " 'beneath': 454,\n",
              " \"soul's\": 455,\n",
              " 'smug': 456,\n",
              " 'familarity': 457,\n",
              " 'eject': 458,\n",
              " 'commentators': 459,\n",
              " 'Weird': 460,\n",
              " 'Santa': 461,\n",
              " 'starched': 462,\n",
              " 'limp-looking': 463,\n",
              " 'enthusiastic': 464,\n",
              " 'primarily': 465,\n",
              " \"David's\": 466,\n",
              " 'convicted': 467,\n",
              " 'Findings': 468,\n",
              " 'hunger': 469,\n",
              " 'warren': 470,\n",
              " 'vouchers': 471,\n",
              " 'beef-feeding': 472,\n",
              " 'seven-stories': 473,\n",
              " 'golfing': 474,\n",
              " 'calibers': 475,\n",
              " 'Kwango': 476,\n",
              " 'rotary': 477,\n",
              " 'shotshells': 478,\n",
              " 'collage': 479,\n",
              " 'commemorates': 480,\n",
              " 'Sox': 481,\n",
              " 'continuum': 482,\n",
              " 'chipping': 483,\n",
              " 'Buddy': 484,\n",
              " 'vehicular': 485,\n",
              " \"Benson's\": 486,\n",
              " 'Paulus': 487,\n",
              " 'Hear': 488,\n",
              " 'magnetism': 489,\n",
              " 'doubts': 490,\n",
              " 'below': 491,\n",
              " 'fellowship': 492,\n",
              " 'Progressivism': 493,\n",
              " 'Sulphur': 494,\n",
              " 'hovered': 495,\n",
              " 'miners': 496,\n",
              " 'Harrow': 497,\n",
              " 'Ich': 498,\n",
              " 'interaction': 499,\n",
              " 'overweight': 500,\n",
              " 'Gloom': 501,\n",
              " 'ribcage': 502,\n",
              " 'Artur': 503,\n",
              " 'quarreling': 504,\n",
              " 'imposing': 505,\n",
              " 'Ocean': 506,\n",
              " 'Readiness': 507,\n",
              " 'Hamburger': 508,\n",
              " '220-degrees': 509,\n",
              " 'Kemm': 510,\n",
              " 'cooling': 511,\n",
              " \"don't-know's\": 512,\n",
              " 'shipment': 513,\n",
              " 'interrelationship': 514,\n",
              " 'initially': 515,\n",
              " 'primly': 516,\n",
              " 'absorbed': 517,\n",
              " 'vibrato': 518,\n",
              " 'encyclopedic': 519,\n",
              " 'biophysical': 520,\n",
              " 'Schonberg': 521,\n",
              " \"World's\": 522,\n",
              " '160-degrees-F': 523,\n",
              " 'seaports': 524,\n",
              " 'It': 525,\n",
              " 'wiry': 526,\n",
              " 'memorabilia': 527,\n",
              " 'Bel-Air': 528,\n",
              " 'diffusing': 529,\n",
              " ',': 530,\n",
              " 'outflow': 531,\n",
              " 'Soeren': 532,\n",
              " 'interprets': 533,\n",
              " 'Newspapermen': 534,\n",
              " 'Provisional': 535,\n",
              " 'time-servers': 536,\n",
              " 'fibers': 537,\n",
              " 'ensue': 538,\n",
              " 'distastefully': 539,\n",
              " 'stark': 540,\n",
              " 'built': 541,\n",
              " 'rhythmic': 542,\n",
              " 'Lodley': 543,\n",
              " 'underfoot': 544,\n",
              " 'turnover': 545,\n",
              " 'differed': 546,\n",
              " 'Las': 547,\n",
              " 'whole': 548,\n",
              " 'braiding': 549,\n",
              " 'flamboyantly': 550,\n",
              " \"Brenner's\": 551,\n",
              " 'Macintosh': 552,\n",
              " 'terminate': 553,\n",
              " 'tanned': 554,\n",
              " 'peculiar': 555,\n",
              " 'merchandise': 556,\n",
              " 'puberty': 557,\n",
              " 'slowly-mending': 558,\n",
              " 'true': 559,\n",
              " 'fabricating': 560,\n",
              " 'appetite': 561,\n",
              " 'opted': 562,\n",
              " 'directrices': 563,\n",
              " 'joke': 564,\n",
              " 'mimesis': 565,\n",
              " 'Still': 566,\n",
              " 'cancer-ridden': 567,\n",
              " 'clamped': 568,\n",
              " 'Liverpool': 569,\n",
              " 'Grossman': 570,\n",
              " 'cleaner': 571,\n",
              " '$67,000': 572,\n",
              " 'confrontation': 573,\n",
              " 'packaged': 574,\n",
              " 'Church': 575,\n",
              " 'tabloids': 576,\n",
              " 'dumb': 577,\n",
              " \"B-52's\": 578,\n",
              " 'Language': 579,\n",
              " 'spotty': 580,\n",
              " 'Eli': 581,\n",
              " 'runners': 582,\n",
              " \"Dean's\": 583,\n",
              " 'low': 584,\n",
              " 'Outlays': 585,\n",
              " 'past-fantasy': 586,\n",
              " 'knots': 587,\n",
              " 'salubrious': 588,\n",
              " 'Konstantin': 589,\n",
              " \"Can't\": 590,\n",
              " 'smithereens': 591,\n",
              " '372': 592,\n",
              " 'Elmer': 593,\n",
              " 'Needham': 594,\n",
              " '$50': 595,\n",
              " 'pedigree': 596,\n",
              " 'Hanoverian': 597,\n",
              " 'softly': 598,\n",
              " 'dirty': 599,\n",
              " 'meats': 600,\n",
              " 'theorizing': 601,\n",
              " 'Savage': 602,\n",
              " 'east': 603,\n",
              " 'preamble': 604,\n",
              " 'unselfish': 605,\n",
              " 'alive': 606,\n",
              " 'heard': 607,\n",
              " 'waving': 608,\n",
              " 'flamed': 609,\n",
              " 'flora': 610,\n",
              " 'overshot': 611,\n",
              " 'explains': 612,\n",
              " 'pinkish-white': 613,\n",
              " 'flops': 614,\n",
              " 'ex-singer': 615,\n",
              " 'terrace': 616,\n",
              " 'dominates': 617,\n",
              " 'patch': 618,\n",
              " 'technically': 619,\n",
              " 'Gargle': 620,\n",
              " 'vainly': 621,\n",
              " 'cant': 622,\n",
              " 'Entry': 623,\n",
              " 'padding': 624,\n",
              " 'stresses': 625,\n",
              " 'skirmishes': 626,\n",
              " 'effectuate': 627,\n",
              " \"Muller's\": 628,\n",
              " 'connexion': 629,\n",
              " 'no-hit': 630,\n",
              " 'quickie': 631,\n",
              " 'glove': 632,\n",
              " '1565': 633,\n",
              " 'derisively': 634,\n",
              " '$10.00': 635,\n",
              " 'Ehlers': 636,\n",
              " 'gastrocnemius': 637,\n",
              " '6:35': 638,\n",
              " 'gumming': 639,\n",
              " 'catalog': 640,\n",
              " 'Simon': 641,\n",
              " 'hops': 642,\n",
              " 'Calmer': 643,\n",
              " 'less-developed': 644,\n",
              " 'primaries': 645,\n",
              " 'phrase': 646,\n",
              " 'Lenygon': 647,\n",
              " 'girlishly': 648,\n",
              " 'seventy-foot': 649,\n",
              " 'Penna.': 650,\n",
              " 'uniqueness': 651,\n",
              " 'lobe': 652,\n",
              " 'hay': 653,\n",
              " 'hailed': 654,\n",
              " 'deceive': 655,\n",
              " '505': 656,\n",
              " 'galvanism': 657,\n",
              " 'enters': 658,\n",
              " 'Detectives': 659,\n",
              " 'Atty.': 660,\n",
              " 'mile-long': 661,\n",
              " 'selves': 662,\n",
              " 'Elijah': 663,\n",
              " 'Ave.': 664,\n",
              " '2:33.2': 665,\n",
              " 'Spelling': 666,\n",
              " 'haint': 667,\n",
              " 'invade': 668,\n",
              " \"Flagler's\": 669,\n",
              " 'annual': 670,\n",
              " 'interchanges': 671,\n",
              " 'Nonsense': 672,\n",
              " 'visited': 673,\n",
              " 'job-': 674,\n",
              " 'resplendent': 675,\n",
              " 'rejects': 676,\n",
              " 'Pe': 677,\n",
              " 'witness': 678,\n",
              " 'toast': 679,\n",
              " 'plied': 680,\n",
              " 'moisten': 681,\n",
              " 'clenched': 682,\n",
              " 'Tobacco': 683,\n",
              " 'loneliness': 684,\n",
              " 'worth': 685,\n",
              " 'wobbling': 686,\n",
              " 'Arnolphe': 687,\n",
              " 'Pass': 688,\n",
              " \"Max's\": 689,\n",
              " 'Enemies': 690,\n",
              " 'inherits': 691,\n",
              " 'deteriorate': 692,\n",
              " \"Gyp'll\": 693,\n",
              " 'insisting': 694,\n",
              " 'Dak.': 695,\n",
              " 'East-West': 696,\n",
              " 'lifeguards': 697,\n",
              " 'Emotional': 698,\n",
              " 'Purvis': 699,\n",
              " 'chair': 700,\n",
              " 'Smokers': 701,\n",
              " 'Dragonetti': 702,\n",
              " 'vestige': 703,\n",
              " 'long-vanished': 704,\n",
              " 'shrubs': 705,\n",
              " 'broadest': 706,\n",
              " 'predecessor': 707,\n",
              " 'Separate': 708,\n",
              " 'pacifier': 709,\n",
              " 'Hughes': 710,\n",
              " 'donate': 711,\n",
              " 'dissection': 712,\n",
              " 'Oh-the-pain-of-it': 713,\n",
              " 'intentionally': 714,\n",
              " 'involuntarily': 715,\n",
              " \"Harbor's\": 716,\n",
              " 'nigh': 717,\n",
              " 'whoever': 718,\n",
              " 'mid-September': 719,\n",
              " 'uniquely': 720,\n",
              " 'skyjacked': 721,\n",
              " 'clotheshorse': 722,\n",
              " 'reproduction': 723,\n",
              " 'Edw': 724,\n",
              " '220-yard': 725,\n",
              " 'Dodgers': 726,\n",
              " 'mais': 727,\n",
              " 'Pennell': 728,\n",
              " 'grapefruit': 729,\n",
              " 'anxieties': 730,\n",
              " '$450,000': 731,\n",
              " 'scheming': 732,\n",
              " 'ready': 733,\n",
              " 'Peep': 734,\n",
              " 'singular': 735,\n",
              " 'accusing': 736,\n",
              " 'chives': 737,\n",
              " 'hammered': 738,\n",
              " 'excommunicated': 739,\n",
              " 'bred': 740,\n",
              " 'drab': 741,\n",
              " 'variety': 742,\n",
              " 'dextrous-fingered': 743,\n",
              " 'drugstore': 744,\n",
              " 'lamplight': 745,\n",
              " 'the': 746,\n",
              " 'Gibbs': 747,\n",
              " 'conference': 748,\n",
              " 'counts': 749,\n",
              " 'wharves': 750,\n",
              " 'pleased': 751,\n",
              " 'Rudy': 752,\n",
              " 'Statistically': 753,\n",
              " \"Cunard's\": 754,\n",
              " 'grammatical': 755,\n",
              " 'casts': 756,\n",
              " 'religionists': 757,\n",
              " 'Animals': 758,\n",
              " 'gorges': 759,\n",
              " 'sub-interval': 760,\n",
              " 'magnolia': 761,\n",
              " 'pupil': 762,\n",
              " 'Claude': 763,\n",
              " 'brother': 764,\n",
              " 'non-time': 765,\n",
              " 'indelibly': 766,\n",
              " 'digs': 767,\n",
              " 'bleachers': 768,\n",
              " 'measure': 769,\n",
              " 'situated': 770,\n",
              " 'hymselfe': 771,\n",
              " 'p': 772,\n",
              " 'Hatching': 773,\n",
              " 'cabins': 774,\n",
              " 'impaling': 775,\n",
              " 'luminous': 776,\n",
              " 'neighbourhood': 777,\n",
              " 'est': 778,\n",
              " 'conspiracy': 779,\n",
              " 'Ransy': 780,\n",
              " 'colonists': 781,\n",
              " 'dead-weight': 782,\n",
              " 'assayed': 783,\n",
              " 'justice': 784,\n",
              " 'understanding': 785,\n",
              " 'Assuming': 786,\n",
              " 'mg': 787,\n",
              " 'codetermines': 788,\n",
              " 'Tatler': 789,\n",
              " 'distortable': 790,\n",
              " 'abnormally': 791,\n",
              " 'Florence': 792,\n",
              " 'Barnes': 793,\n",
              " 'whereof': 794,\n",
              " 'skiddy': 795,\n",
              " 'balance': 796,\n",
              " 'Maximilian': 797,\n",
              " 'catastrophic': 798,\n",
              " 'commended': 799,\n",
              " 'SAKOS': 800,\n",
              " 'appeals': 801,\n",
              " 'Buzz': 802,\n",
              " 'creed': 803,\n",
              " 'guess': 804,\n",
              " 'Lead': 805,\n",
              " 'dot': 806,\n",
              " 'logging': 807,\n",
              " 'pleats': 808,\n",
              " 'glide-bombed': 809,\n",
              " 'succor': 810,\n",
              " 'high-salaried': 811,\n",
              " 'Wycombe': 812,\n",
              " 'Erlenmeyer': 813,\n",
              " 'pall': 814,\n",
              " 'Martian': 815,\n",
              " 'ribbon': 816,\n",
              " 'lance': 817,\n",
              " 'abandon': 818,\n",
              " '$227.72': 819,\n",
              " 'Whipple': 820,\n",
              " \"Class'\": 821,\n",
              " 'diarrhoea': 822,\n",
              " 'Ritz': 823,\n",
              " 'traineeships': 824,\n",
              " 'Cominform': 825,\n",
              " 'six-point': 826,\n",
              " 'Scott': 827,\n",
              " 'backers': 828,\n",
              " 'similitude': 829,\n",
              " 'reforms': 830,\n",
              " 'dilatation': 831,\n",
              " 'malevolence': 832,\n",
              " 'chiefly': 833,\n",
              " 'yell': 834,\n",
              " 'thereby': 835,\n",
              " 'nettlesome': 836,\n",
              " 'radiation-produced': 837,\n",
              " 'boxcars': 838,\n",
              " 'fishpond': 839,\n",
              " 'impetuous': 840,\n",
              " 'soil': 841,\n",
              " 'contradict': 842,\n",
              " 'cultural': 843,\n",
              " 'Djakarta': 844,\n",
              " 'three-dice': 845,\n",
              " 'Kizzie': 846,\n",
              " 'flared': 847,\n",
              " 'Collinsville': 848,\n",
              " 'gulps': 849,\n",
              " 'Hannibal': 850,\n",
              " 'Bernhardt': 851,\n",
              " 'Blakey': 852,\n",
              " 'Can': 853,\n",
              " 'Bagley': 854,\n",
              " 'plumbing': 855,\n",
              " 'migratory': 856,\n",
              " 'morsels': 857,\n",
              " 'Lieutenant': 858,\n",
              " 'flaming': 859,\n",
              " 'beckons': 860,\n",
              " 'triangles': 861,\n",
              " 'Kan.': 862,\n",
              " 'Messengers': 863,\n",
              " 'outcome': 864,\n",
              " 'rocket-bombs': 865,\n",
              " 'thermostat': 866,\n",
              " 'reserved': 867,\n",
              " 'hindered': 868,\n",
              " 'spider': 869,\n",
              " 'pilgrimage': 870,\n",
              " \"sentry's\": 871,\n",
              " 'Creepers': 872,\n",
              " 'agony': 873,\n",
              " 'Nowacki': 874,\n",
              " 'sternly': 875,\n",
              " 'Dewey': 876,\n",
              " 'counterman': 877,\n",
              " \"meetin'\": 878,\n",
              " 'liveried': 879,\n",
              " 'bailing': 880,\n",
              " 'cellist': 881,\n",
              " 'washed-out': 882,\n",
              " 'babel': 883,\n",
              " 'Synthetic': 884,\n",
              " 'galvanizing': 885,\n",
              " 'Bodin': 886,\n",
              " 'overwhelm': 887,\n",
              " 'Quickly': 888,\n",
              " 'yours': 889,\n",
              " '450': 890,\n",
              " 'Irene': 891,\n",
              " 'penetrate': 892,\n",
              " \"sewer's\": 893,\n",
              " 'disintegrating': 894,\n",
              " 'giggling': 895,\n",
              " 'Hawaii': 896,\n",
              " 'dominantly': 897,\n",
              " 'subtends': 898,\n",
              " 'blowing': 899,\n",
              " \"doesn't\": 900,\n",
              " 'avaricious': 901,\n",
              " 'gang': 902,\n",
              " '200-odd': 903,\n",
              " 'nationalistic': 904,\n",
              " 'Bridewell': 905,\n",
              " 'veining': 906,\n",
              " 'individuals': 907,\n",
              " 'warless': 908,\n",
              " 'cunning': 909,\n",
              " 'historians': 910,\n",
              " 'islands': 911,\n",
              " 'devotional': 912,\n",
              " 'Sarasota': 913,\n",
              " 'defeated': 914,\n",
              " 'triumphantly': 915,\n",
              " 'Dec.': 916,\n",
              " 'manages': 917,\n",
              " 'antagonism': 918,\n",
              " \"buyin'\": 919,\n",
              " '470-degrees-C': 920,\n",
              " 'Grecian': 921,\n",
              " 'concealing': 922,\n",
              " 'Stations': 923,\n",
              " 'Sleepers': 924,\n",
              " 'mater': 925,\n",
              " 'obtaining': 926,\n",
              " 'work-weary': 927,\n",
              " 'Heaven': 928,\n",
              " 'ritual': 929,\n",
              " 'positioned': 930,\n",
              " 'horselike': 931,\n",
              " 'reviewing': 932,\n",
              " 'Numbers': 933,\n",
              " 'lob-scuse': 934,\n",
              " 'roared': 935,\n",
              " 'disadvantage': 936,\n",
              " 'Flier': 937,\n",
              " 'simplicitude': 938,\n",
              " 'assuredly': 939,\n",
              " 'maneuvered': 940,\n",
              " 'writhing': 941,\n",
              " 'supposedly': 942,\n",
              " 'Travancore': 943,\n",
              " 'disastrous': 944,\n",
              " 'invoke': 945,\n",
              " 'embargo': 946,\n",
              " \"o'\": 947,\n",
              " 'raked': 948,\n",
              " 'scheduled': 949,\n",
              " 'chew': 950,\n",
              " \"You'd\": 951,\n",
              " 'subtitled': 952,\n",
              " 'quivered': 953,\n",
              " 'limbo': 954,\n",
              " 'disproportionately': 955,\n",
              " 'convert': 956,\n",
              " 'Indication': 957,\n",
              " 'pungent': 958,\n",
              " 'price-cutting': 959,\n",
              " 'three-hour': 960,\n",
              " 'cruelest': 961,\n",
              " 'dissented': 962,\n",
              " 'callousness': 963,\n",
              " 'anti-personality': 964,\n",
              " 'manor': 965,\n",
              " 'Sanchez': 966,\n",
              " \"respondent's\": 967,\n",
              " 'ditch': 968,\n",
              " 'equivalence': 969,\n",
              " 'fireman': 970,\n",
              " 'abound': 971,\n",
              " 'vest': 972,\n",
              " '62-year-old': 973,\n",
              " 'collective-bargaining': 974,\n",
              " 'conceits': 975,\n",
              " 'disharmony': 976,\n",
              " 'Antigone': 977,\n",
              " 'dualism': 978,\n",
              " 'Their': 979,\n",
              " 'insistent': 980,\n",
              " 'cabana': 981,\n",
              " 'oilcloth': 982,\n",
              " 'thatt': 983,\n",
              " 'frighten': 984,\n",
              " 'Elsinore': 985,\n",
              " 'windows': 986,\n",
              " 'Anaconda': 987,\n",
              " 'misjudged': 988,\n",
              " 'fragmentation': 989,\n",
              " '30-year': 990,\n",
              " 'throttle': 991,\n",
              " 'brigades': 992,\n",
              " 'incorruptible': 993,\n",
              " 'bungalow': 994,\n",
              " 'rotating': 995,\n",
              " 'Hays': 996,\n",
              " 'Attopeu': 997,\n",
              " 'recall': 998,\n",
              " 'Shamrock': 999,\n",
              " 'wherever': 1000,\n",
              " ...}"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word2ind"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "URC1B2nvPGFt",
        "outputId": "36833855-9a87-47ed-b2e2-7fdd67ec2413"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmkAAAEvCAYAAAAemFY+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAeP0lEQVR4nO3de7SldX3f8fcnTMIiTSFcRkMYyCAXFVhmEqbIippiCDC6bMAsqEMTGVuaUQptJZcVSdJicdGKCZkskoALy4RLI5dAFeqC6FRiNC0Cg6JcFBiEyMgUCMNCEgEz+O0f+3dwz2HPmTPnnDnnd47v11p7nWd/n+f3+N24L5/5Pc+zd6oKSZIk9eWH5roBSZIkvZIhTZIkqUOGNEmSpA4Z0iRJkjpkSJMkSeqQIU2SJKlDi+a6gZm2zz771NKlS+e6DUmSpO266667/q6qFo9at+BC2tKlS1m/fv1ctyFJkrRdSf52W+s83ClJktQhQ5okSVKHDGmSJEkdMqRJkiR1yJAmSZLUIUOaJElShwxpkiRJHTKkSZIkdWi7IS3J2iRPJrl3qHZtkrvb7dEkd7f60iTPD6376NCYI5Pck2RDkouSpNV3bfvbkOT2JEuHxqxK8lC7rZrJBy5JktSzyfziwOXAnwBXjhWq6l1jy0kuBJ4d2v7hqlo2Yj+XAKuBLwI3AyuAW4DTgWeq6uAkK4ELgHcl2Qs4F1gOFHBXkpuq6plJPzpJkqR5arszaVX1eWDzqHVtNuxfAldPtI8k+wK7V9VtVVUMAt9JbfWJwBVt+Xrg2LbfE4B1VbW5BbN1DIKdJEnSgjfd3+58C/BEVT00VDswyZeBbwO/V1VfAPYDNg5ts7HVaH8fA6iqLUmeBfYero8YI3VpzboHpzX+7OMOnaFOJEnz3XRD2qlsPYu2CTigqp5OciTwySSHAxkxttrfba2baMxWkqxmcCiVAw44YJKtS5Ik9WvKV3cmWQT8MnDtWK2qXqyqp9vyXcDDwKEMZsGWDA1fAjzeljcC+w/tcw8Gh1dfro8Ys5WqurSqllfV8sWLF0/1IUmSJHVjOl/B8YvA16vq5cOYSRYn2aUtvwY4BPhGVW0CnktydDvf7DTgxjbsJmDsys2TgVvbeWufBo5PsmeSPYHjW02SJGnB2+7hziRXA8cA+yTZCJxbVZcBK3nlBQM/D5yXZAvwEvC+qhq76OAMBleK7sbgqs5bWv0y4KokGxjMoK0EqKrNST4E3Nm2O29oX5IkSQvadkNaVZ26jfp7RtRuAG7YxvbrgSNG1F8ATtnGmLXA2u31KEmStND4iwOSJEkdMqRJkiR1yJAmSZLUIUOaJElShwxpkiRJHTKkSZIkdciQJkmS1CFDmiRJUocMaZIkSR0ypEmSJHXIkCZJktQhQ5okSVKHDGmSJEkdMqRJkiR1yJAmSZLUIUOaJElShwxpkiRJHTKkSZIkdciQJkmS1CFDmiRJUocMaZIkSR0ypEmSJHXIkCZJktQhQ5okSVKHDGmSJEkdMqRJkiR1yJAmSZLUIUOaJElSh7Yb0pKsTfJkknuHah9M8q0kd7fb24fWnZNkQ5IHkpwwVD8yyT1t3UVJ0uq7Jrm21W9PsnRozKokD7Xbqhl71JIkSZ2bzEza5cCKEfU1VbWs3W4GSHIYsBI4vI25OMkubftLgNXAIe02ts/TgWeq6mBgDXBB29dewLnAG4GjgHOT7LnDj1CSJGke2m5Iq6rPA5snub8TgWuq6sWqegTYAByVZF9g96q6raoKuBI4aWjMFW35euDYNst2ArCuqjZX1TPAOkaHRUmSpAVnOueknZXkq+1w6NgM137AY0PbbGy1/dry+PpWY6pqC/AssPcE+5IkSVrwphrSLgEOApYBm4ALWz0jtq0J6lMds5Ukq5OsT7L+qaeemqBtSZKk+WFKIa2qnqiql6rqe8DHGJwzBoPZrv2HNl0CPN7qS0bUtxqTZBGwB4PDq9va16h+Lq2q5VW1fPHixVN5SJIkSV2ZUkhr55iNeScwduXnTcDKdsXmgQwuELijqjYBzyU5up1vdhpw49CYsSs3TwZubeetfRo4Psme7XDq8a0mSZK04C3a3gZJrgaOAfZJspHBFZfHJFnG4PDjo8B7AarqviTXAfcDW4Azq+qltqszGFwpuhtwS7sBXAZclWQDgxm0lW1fm5N8CLizbXdeVU32AgZJkqR5bbshrapOHVG+bILtzwfOH1FfDxwxov4CcMo29rUWWLu9HiVJkhYaf3FAkiSpQ4Y0SZKkDhnSJEmSOmRIkyRJ6pAhTZIkqUOGNEmSpA4Z0iRJkjpkSJMkSeqQIU2SJKlDhjRJkqQOGdIkSZI6ZEiTJEnqkCFNkiSpQ4Y0SZKkDhnSJEmSOmRIkyRJ6pAhTZIkqUOGNEmSpA4Z0iRJkjpkSJMkSeqQIU2SJKlDhjRJkqQOGdIkSZI6ZEiTJEnqkCFNkiSpQ4Y0SZKkDhnSJEmSOmRIkyRJ6tB2Q1qStUmeTHLvUO33k3w9yVeTfCLJj7f60iTPJ7m73T46NObIJPck2ZDkoiRp9V2TXNvqtydZOjRmVZKH2m3VTD5wSZKknk1mJu1yYMW42jrgiKp6A/AgcM7Quoeralm7vW+ofgmwGjik3cb2eTrwTFUdDKwBLgBIshdwLvBG4Cjg3CR77sBjkyRJmre2G9Kq6vPA5nG1z1TVlnb3i8CSifaRZF9g96q6raoKuBI4qa0+EbiiLV8PHNtm2U4A1lXV5qp6hkEwHB8WJUmSFqSZOCft3wC3DN0/MMmXk/x1kre02n7AxqFtNrba2LrHAFrwexbYe7g+YowkSdKCtmg6g5P8LrAF+PNW2gQcUFVPJzkS+GSSw4GMGF5ju9nGuonGjO9jNYNDqRxwwAGTfwCSJEmdmvJMWjuR/x3Ar7RDmFTVi1X1dFu+C3gYOJTBLNjwIdElwONteSOwf9vnImAPBodXX66PGLOVqrq0qpZX1fLFixdP9SFJkiR1Y0ohLckK4LeBX6qq7wzVFyfZpS2/hsEFAt+oqk3Ac0mObuebnQbc2IbdBIxduXkycGsLfZ8Gjk+yZ7tg4PhWkyRJWvC2e7gzydXAMcA+STYyuOLyHGBXYF37Jo0vtis5fx44L8kW4CXgfVU1dtHBGQyuFN2NwTlsY+exXQZclWQDgxm0lQBVtTnJh4A723bnDe1LkiRpQdtuSKuqU0eUL9vGtjcAN2xj3XrgiBH1F4BTtjFmLbB2ez1KkiQtNP7igCRJUocMaZIkSR0ypEmSJHXIkCZJktQhQ5okSVKHDGmSJEkdMqRJkiR1aFq/3SlJktSrNesenNb4s487dIY6mRpn0iRJkjpkSJMkSeqQIU2SJKlDhjRJkqQOGdIkSZI6ZEiTJEnqkCFNkiSpQ4Y0SZKkDhnSJEmSOmRIkyRJ6pAhTZIkqUOGNEmSpA4Z0iRJkjpkSJMkSeqQIU2SJKlDhjRJkqQOGdIkSZI6ZEiTJEnqkCFNkiSpQ4Y0SZKkDm03pCVZm+TJJPcO1fZKsi7JQ+3vnkPrzkmyIckDSU4Yqh+Z5J627qIkafVdk1zb6rcnWTo0ZlX733goyaoZe9SSJEmdm8xM2uXAinG1DwCfrapDgM+2+yQ5DFgJHN7GXJxklzbmEmA1cEi7je3zdOCZqjoYWANc0Pa1F3Au8EbgKODc4TAoSZK0kG03pFXV54HN48onAle05SuAk4bq11TVi1X1CLABOCrJvsDuVXVbVRVw5bgxY/u6Hji2zbKdAKyrqs1V9QywjleGRUmSpAVpquekvbqqNgG0v69q9f2Ax4a229hq+7Xl8fWtxlTVFuBZYO8J9vUKSVYnWZ9k/VNPPTXFhyRJktSPmb5wICNqNUF9qmO2LlZdWlXLq2r54sWLJ9WoJElSz6Ya0p5ohzBpf59s9Y3A/kPbLQEeb/UlI+pbjUmyCNiDweHVbe1LkiRpwZtqSLsJGLvachVw41B9Zbti80AGFwjc0Q6JPpfk6Ha+2Wnjxozt62Tg1nbe2qeB45Ps2S4YOL7VJEmSFrxF29sgydXAMcA+STYyuOLyw8B1SU4HvgmcAlBV9yW5Drgf2AKcWVUvtV2dweBK0d2AW9oN4DLgqiQbGMygrWz72pzkQ8Cdbbvzqmr8BQySJEkL0nZDWlWduo1Vx25j+/OB80fU1wNHjKi/QAt5I9atBdZur0dJkqSFxl8ckCRJ6pAhTZIkqUOGNEmSpA5t95w0SZI089ase3Ba488+7tAZ6kS9ciZNkiSpQ4Y0SZKkDnm4U93yUIAk6QeZM2mSJEkdMqRJkiR1yJAmSZLUIUOaJElShwxpkiRJHTKkSZIkdciQJkmS1CG/J22KpvMdXn5/lyRJ2h5n0iRJkjpkSJMkSeqQIU2SJKlDhjRJkqQOGdIkSZI6ZEiTJEnqkCFNkiSpQ4Y0SZKkDhnSJEmSOmRIkyRJ6pAhTZIkqUOGNEmSpA5NOaQleW2Su4du307y/iQfTPKtofrbh8ack2RDkgeSnDBUPzLJPW3dRUnS6rsmubbVb0+ydFqPVpIkaZ6YckirqgeqallVLQOOBL4DfKKtXjO2rqpuBkhyGLASOBxYAVycZJe2/SXAauCQdlvR6qcDz1TVwcAa4IKp9itJkjSfzNThzmOBh6vqbyfY5kTgmqp6saoeATYARyXZF9i9qm6rqgKuBE4aGnNFW74eOHZslk2SJGkhm6mQthK4euj+WUm+mmRtkj1bbT/gsaFtNrbafm15fH2rMVW1BXgW2HuGepYkSerWtENakh8Bfgn4i1a6BDgIWAZsAi4c23TE8JqgPtGY8T2sTrI+yfqnnnpq8s1LkiR1aiZm0t4GfKmqngCoqieq6qWq+h7wMeCott1GYP+hcUuAx1t9yYj6VmOSLAL2ADaPb6CqLq2q5VW1fPHixTPwkCRJkubWTIS0Uxk61NnOMRvzTuDetnwTsLJdsXkggwsE7qiqTcBzSY5u55udBtw4NGZVWz4ZuLWdtyZJkrSgLZrO4CQ/ChwHvHeo/JEkyxgclnx0bF1V3ZfkOuB+YAtwZlW91MacAVwO7Abc0m4AlwFXJdnAYAZt5XT6lSRJmi+mFdKq6juMO5G/qt49wfbnA+ePqK8HjhhRfwE4ZTo9SpIkzUf+4oAkSVKHDGmSJEkdMqRJkiR1yJAmSZLUIUOaJElShwxpkiRJHTKkSZIkdciQJkmS1CFDmiRJUocMaZIkSR0ypEmSJHXIkCZJktQhQ5okSVKHDGmSJEkdMqRJkiR1yJAmSZLUIUOaJElShwxpkiRJHTKkSZIkdciQJkmS1CFDmiRJUocMaZIkSR0ypEmSJHXIkCZJktQhQ5okSVKHDGmSJEkdMqRJkiR1aNFcNyBJO2LNugenNf7s4w6doU4kaeea1kxakkeT3JPk7iTrW22vJOuSPNT+7jm0/TlJNiR5IMkJQ/Uj2342JLkoSVp91yTXtvrtSZZOp19JkqT5YiYOd761qpZV1fJ2/wPAZ6vqEOCz7T5JDgNWAocDK4CLk+zSxlwCrAYOabcVrX468ExVHQysAS6YgX4lSZK6tzPOSTsRuKItXwGcNFS/pqperKpHgA3AUUn2BXavqtuqqoArx40Z29f1wLFjs2ySJEkL2XRDWgGfSXJXktWt9uqq2gTQ/r6q1fcDHhsau7HV9mvL4+tbjamqLcCzwN7T7FmSJKl7071w4E1V9XiSVwHrknx9gm1HzYDVBPWJxmy940FAXA1wwAEHTNyxJEnSPDCtmbSqerz9fRL4BHAU8EQ7hEn7+2TbfCOw/9DwJcDjrb5kRH2rMUkWAXsAm0f0cWlVLa+q5YsXL57OQ5IkSerClENakn+S5J+OLQPHA/cCNwGr2margBvb8k3AynbF5oEMLhC4ox0SfS7J0e18s9PGjRnb18nAre28NUmSpAVtOoc7Xw18op3Hvwj4eFX9ZZI7geuSnA58EzgFoKruS3IdcD+wBTizql5q+zoDuBzYDbil3QAuA65KsoHBDNrKafQrSZI0b0w5pFXVN4CfHlF/Gjh2G2POB84fUV8PHDGi/gIt5EmSJP0g8WehJEmSOmRIkyRJ6pAhTZIkqUOGNEmSpA4Z0iRJkjpkSJMkSeqQIU2SJKlDhjRJkqQOGdIkSZI6ZEiTJEnqkCFNkiSpQ4Y0SZKkDhnSJEmSOmRIkyRJ6pAhTZIkqUOGNEmSpA4tmusGJGmhW7PuwWmNP/u4Q2eoE0nziTNpkiRJHTKkSZIkdciQJkmS1CFDmiRJUocMaZIkSR0ypEmSJHXIr+CQJEmTMp2vk/GrZHacM2mSJEkdMqRJkiR1yJAmSZLUIUOaJElSh6Yc0pLsn+SvknwtyX1J/mOrfzDJt5Lc3W5vHxpzTpINSR5IcsJQ/cgk97R1FyVJq++a5NpWvz3J0mk8VkmSpHljOjNpW4DfqKrXA0cDZyY5rK1bU1XL2u1mgLZuJXA4sAK4OMkubftLgNXAIe22otVPB56pqoOBNcAF0+hXkiRp3phySKuqTVX1pbb8HPA1YL8JhpwIXFNVL1bVI8AG4Kgk+wK7V9VtVVXAlcBJQ2OuaMvXA8eOzbJJkiQtZDNyTlo7DPkzwO2tdFaSryZZm2TPVtsPeGxo2MZW268tj69vNaaqtgDPAnvPRM+SJEk9m3ZIS/JjwA3A+6vq2wwOXR4ELAM2AReObTpieE1Qn2jM+B5WJ1mfZP1TTz21Yw9AkiSpQ9P6xYEkP8wgoP15Vf1PgKp6Ymj9x4BPtbsbgf2Hhi8BHm/1JSPqw2M2JlkE7AFsHt9HVV0KXAqwfPnyV4Q4SdvmN4hLUp+mc3VngMuAr1XVHw7V9x3a7J3AvW35JmBlu2LzQAYXCNxRVZuA55Ic3fZ5GnDj0JhVbflk4NZ23pokSdKCNp2ZtDcB7wbuSXJ3q/0OcGqSZQwOSz4KvBegqu5Lch1wP4MrQ8+sqpfauDOAy4HdgFvaDQYh8KokGxjMoK2cRr+SJEnzxpRDWlX9DaPPGbt5gjHnA+ePqK8HjhhRfwE4Zao9SpIkzVf+4oAkSVKHDGmSJEkdMqRJkiR1yJAmSZLUoWl9T5okaWHy+/OkuedMmiRJUocMaZIkSR0ypEmSJHXIkCZJktQhQ5okSVKHDGmSJEkdMqRJkiR1yJAmSZLUIUOaJElShwxpkiRJHTKkSZIkdciQJkmS1CFDmiRJUocWzXUDkiRN15p1D05r/NnHHTpDnUgzx5k0SZKkDhnSJEmSOmRIkyRJ6pAhTZIkqUOGNEmSpA4Z0iRJkjpkSJMkSeqQIU2SJKlDhjRJkqQOzYuQlmRFkgeSbEjygbnuR5IkaWfrPqQl2QX4U+BtwGHAqUkOm9uuJEmSdq7uQxpwFLChqr5RVd8FrgFOnOOeJEmSdqr58APr+wGPDd3fCLxxjnqZ16bzA8T++LAkSbMrVTXXPUwoySnACVX1b9v9dwNHVdW/H9pmNbC63X0t8MCsN/pK+wB/N9dN7ID51i/Y82yYb/2CPc+G+dYv2PNsmW8999DvT1XV4lEr5sNM2kZg/6H7S4DHhzeoqkuBS2ezqe1Jsr6qls91H5M13/oFe54N861fsOfZMN/6BXueLfOt5977nQ/npN0JHJLkwCQ/AqwEbprjniRJknaq7mfSqmpLkrOATwO7AGur6r45bkuSJGmn6j6kAVTVzcDNc93HDurq8OskzLd+wZ5nw3zrF+x5Nsy3fsGeZ8t867nrfru/cECSJOkH0Xw4J02SJOkHjiFtCpK8lOTuJPcl+UqSX0/yQ23dMUmebevHbu8aWv5/Sb41dP9H5vBx7J/kkSR7tft7tvs/NVc9DUvyziSV5HXt/tIkzyf5cpKvJbkjyaqh7d+T5Kn23/X+JL+2E3urJBcO3f/NJB8cur86ydfb7Y4kbx5a92iSfYbuH5PkU0OP4XtJ3jC0/t4kS2ew959Ick2Sh9t/p5uTHJrk8CS3JnkwyUNJ/lOSTKav8Y9pZ5vic+NPZqu/qfbangu3jRu/KMkTSfadxZ7H3uPuTfIXSX50RP1/JfnxJLe32jeHXn93z+RzdoI+t/k6THJ5kpPHbf/37e/SNvZDQ+v2SfKPc/U86dGOPA+Gxkz5fWQG+v1ckhPG1d7f3uOez9afy6e19Y8muSfJV5P8dYY+/4Ye51eSfCnJz81EnzvCkDY1z1fVsqo6HDgOeDtw7tD6L7T1Y7drx5aBjwJrhtZ9dw76B6CqHgMuAT7cSh8GLq2qv52rnsY5FfgbBlf0jnm4qn6mql7f6mcn+ddD669t/52PAf5rklfvpN5eBH55VDBJ8g7gvcCbq+p1wPuAjyf5iUnueyPwuzPW6da9BfgE8LmqOqiqDgN+B3g1g6umP1xVhwI/Dfwc8O9mo68pmMpzY67sSK+fB5aM+9D6ReDeqto0Ww3z/fe4I4DvMngOj69vBs6sqje219x/pr3+2u3RWehzm6/DSfgG8I6h+6cAXpS2tUk/DwCS7Mbcvo9czdavM9r9/8bgNTf8uXzl0DZvrao3AJ8Dfm+oPvY4fxo4p+1nVhnSpqmqnmTwRbpnjf1rYZ5ZAxyd5P3Am4ELJ958diT5MeBNwOm88kUHQFV9A/h14D+MWPck8DCws2YFtzA44fTsEet+G/itqvq71suXgCtob2ST8Cng8CSvnYlGx3kr8I9V9dGxQlXdDRwK/J+q+kyrfQc4C/jALPU1adN9bsymHe21qr4H/AXwrqFNVjL48JkrXwAOHlG/jcEvwsyliV6H2/M88LUkY9+R9S7guplqbAGazPPgXzG37yPXA+9IsisMZkyBn2QQDCdjouf07sAz021wRxnSZkB7k/0h4FWt9JZx06oHzWF7E6qqfwR+i0FYe/9czuyNcxLwl1X1ILA5yc9uY7svAa8bX0zyGuA1wIad1iH8KfArSfYYVz8cuGtcbX2rT8b3gI8wmOGaaUfwyt5gRM9V9TDwY0l2n4W+dsRJTOO5MctOYsd7fXk2oH3YvB24YSf3OVKSRcDbgHvG1XcBjqWP76zc1utwMq4BViZZArzEuC9K18AOPA/m9H2kqp4G7gBWtNJK4FqggIPGfS6/ZcQuVgCfHLq/W9v268B/Bz40YsxOZUibOcOzaOMPdz48Z11NztuATQw+wHtxKoM3UNrfU7ex3fjZy3cluZvBB917q2rzzmkPqurbwJVMbrYmDN4oGPq71e7G3f84gxnOA6fe4Q4Z7m+84fps9zXKVJ8bc2GHe62qOxl8qL2WwWvzi1U12/+C3629jtYD3wQuG1d/GtgLWDfLfb3CBK/DybzO/pLBKSunMvgw19Z29HnQw/vI8CHP4Vno8Yc7vzA05q+SPMng1IKPD9XHDne+jkGAu3K2j5jNi+9J612btXkJeBJ4/Ry3s0OSLGPwJnU08DdJrpnlc19G9bQ38AvAEUmKwZcYF3DxiM1/Bvja0P1rq+qsnd/ly/6IwSzInw3V7geOBG4dqv1sq8PgjW1Pvv97cXsx7rfj2pc4X8jg0OlMug84eRv1nx8utOf131fVc2PvSzuxr0mZ5nNjVk2z12sYfMC8nrk51Pl8O89sZL3NWn2KwSH8i2a1s9H+iFe+DsdeZwBkcIHU+NfZd5PcBfwGg1mgf7HTO51fdvR50MP7yCeBP2yz1rtV1ZcmcWHCW4F/AC4HzmNw+sFWquq2du7jYgaf9bPCmbRpSrKYwcUAf1Lz7Evn2r8ILmFwmPObwO8DfzC3XQGDEHFlVf1UVS2tqv2BRxj8buvL2gvvD4A/nv0WB9pM3XUMzjka8xHggvYhPRaE38P3P5w/B7y7rdsF+FXgr0bs/nIG/7Ib+cO7U3QrsGuGrnxN8s+Ah4A3J/nFVtuNwZvuR2apr8maN88Nptfr1QyeF79AH4cUt1JVzzKYufrNJD/cQT+jXoefYzCzPnYF/XsY/Tq7EPjtdqhMO2DE8+DPmeP3kar6ewb/369lB/6BU1XPA+8HTmuBfisZXJ29C4PwP2sMaVMzdpz6PuB/A58B/svQ+vHnpI2auejBrwHfrKqxqeqLgdcl+edz2BMMDj18YlztBgbnMByU9tUFDN6U/7iq/mz8DmbZhcDLV5dV1U0M3iD+bzuX4WPArw7NUH4IODjJV4AvMzhv7n+M32k7P/Aivn+u47S1f0i8Ezgug6/guA/4IINzcU4Efi/JAwzOPbkTeMXXEWyjr0UMrrTb2ab63Jit/oZN+XlcVfcD3wFurap/mK2Gd0RVfRn4Ctu4IGIOjH8dforBye53tUNzb2LEzE1V3VdVV8xWk5OVwddG/ORc97E9w8+DFnSm8z4yU65mcGXpNUO18eekjbrgbFMbO3aR19hn/d0MDoevqqqXdkK/2+QvDkialjabfHdVzfWVftuUZA3wUFWNOtQoSV1yJk3SlCX5JQazFefMdS/bkuQW4A0MDsVI0rzhTJokSVKHnEmTJEnqkCFNkiSpQ4Y0SZKkDhnSJEmSOmRIkyRJ6pAhTZIkqUP/HymhqLMgoKgyAAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "tag_distribution = Counter(tag for sample in train_data for _, tag in sample)\n",
        "tag_distribution = [tag_distribution[tag] for tag in tags]\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "bar_width = 0.35\n",
        "plt.bar(np.arange(len(tags)), tag_distribution, bar_width, align='center', alpha=0.5)\n",
        "plt.xticks(np.arange(len(tags)), tags)\n",
        "    \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gArQwbzWWkgi"
      },
      "source": [
        "## Бейзлайн\n",
        "\n",
        "Какой самый простой теггер можно придумать? Давайте просто запоминать, какие теги самые вероятные для слова (или для последовательности):\n",
        "\n",
        "![tag-context](https://www.nltk.org/images/tag-context.png)  \n",
        "*From [Categorizing and Tagging Words, nltk](https://www.nltk.org/book/ch05.html)*\n",
        "\n",
        "На картинке показано, что для предсказания $t_n$ используются два предыдущих предсказанных тега + текущее слово. По корпусу считаются вероятность для $P(t_n| w_n, t_{n-1}, t_{n-2})$, выбирается тег с максимальной вероятностью.\n",
        "\n",
        "Более аккуратно такая идея реализована в Hidden Markov Models: по тренировочному корпусу вычисляются вероятности $P(w_n| t_n), P(t_n|t_{n-1}, t_{n-2})$ и максимизируется их произведение.\n",
        "\n",
        "Простейший вариант - униграммная модель, учитывающая только слово:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MergdmlUcDDz"
      },
      "source": [
        "На вход UnigramTagger принимает данные в таком формате:\\\n",
        "*train (list(list(tuple(str, str)))) – The corpus of training data, a list of tagged sentences*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jihL_L1EcwzV",
        "outputId": "600dc6e5-9a99-4340-889f-3c3cbfb31fff"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[('Open', 'ADJ'), ('market', 'NOUN'), ('policy', 'NOUN')],\n",
              " [('And', 'CONJ'),\n",
              "  ('you', 'PRON'),\n",
              "  ('think', 'VERB'),\n",
              "  ('you', 'PRON'),\n",
              "  ('have', 'VERB'),\n",
              "  ('language', 'NOUN'),\n",
              "  ('problems', 'NOUN'),\n",
              "  ('.', '.')]]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_data[:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rWmSToIaeAo",
        "outputId": "4fbc35e9-4661-4bfa-8d00-590167cc8a9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of unigram tagger = 92.62%\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "default_tagger = nltk.DefaultTagger('NN')  # tags every word witn a noun\n",
        "\n",
        "# The UnigramTagger finds the most likely tag for each word in a training corpus, \n",
        "# and then uses that information to assign tags to new tokens.\n",
        "unigram_tagger = nltk.UnigramTagger(train_data, backoff=default_tagger)\n",
        "print('Accuracy of unigram tagger = {:.2%}'.format(unigram_tagger.evaluate(test_data)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GocIMguUeNKq"
      },
      "source": [
        "Хорошая [заметка](https://stackoverflow.com/questions/46713629/evaluating-pos-tagger-in-nltk) на SO про синтаксис Taggers из nltk\n",
        "\n",
        "Выжимка:\n",
        "```python\n",
        "tagged_sentences = brown.tagged_sents(categories=\"news\", tagset=\"universal\")\n",
        "\n",
        "# let's keep 20% of the data for testing, and 80 for training\n",
        "i = int(len(tagged_sentences)*0.2)\n",
        "train_sentences = tagged_sentences[i:]\n",
        "test_sentences = tagged_sentences[:i]\n",
        "\n",
        "# train\n",
        "unigram_tagger = UnigramTagger(train_sentences)\n",
        "# get ACCURACY; default evaluation metric for nltk taggers is accuracy\n",
        "accuracy = unigram_tagger.evaluate(test_sentences)\n",
        "\n",
        "tagged_test_sentences = unigram_tagger.tag_sents([[token for token,tag in sent] for sent in test_sentences])  # words(tokens) only\n",
        "pred = [str(tag) for sentence in tagged_test_sentences for token,tag in sentence]  # predicted tags\n",
        "gold = [str(tag) for sentence in test_sentences for token,tag in sentence]  # true tags\n",
        "\n",
        "from sklearn import metrics\n",
        "print(metrics.classification_report(gold, pred))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07Ymb_MkbWsF"
      },
      "source": [
        "Добавим вероятности переходов:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjz_Rk0bbMyH",
        "outputId": "ec158140-775d-4796-8ee1-73a782dd82da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of bigram tagger = 93.42%\n"
          ]
        }
      ],
      "source": [
        "# A tagger that chooses a token’s tag based its word string and on the preceding \n",
        "# words’ tag. In particular, a tuple consisting of the previous tag and \n",
        "# the word is looked up in a table, and the corresponding tag is returned.\n",
        "bigram_tagger = nltk.BigramTagger(train_data, backoff=unigram_tagger)\n",
        "print('Accuracy of bigram tagger = {:.2%}'.format(bigram_tagger.evaluate(test_data)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWMw6QHvbaDd"
      },
      "source": [
        "Обратите внимание, что `backoff` важен:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XCuxEBVbOY_",
        "outputId": "9bae0058-f819-492d-d2f7-2eff9712a47b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of trigram tagger = 23.33%\n"
          ]
        }
      ],
      "source": [
        "trigram_tagger = nltk.TrigramTagger(train_data)\n",
        "print('Accuracy of trigram tagger = {:.2%}'.format(trigram_tagger.evaluate(test_data)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRbIzTcOuSQj",
        "outputId": "85d864d9-5952-40ad-f9ff-fd80b1dd163c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of trigram tagger = 93.28%\n"
          ]
        }
      ],
      "source": [
        "trigram_tagger = nltk.TrigramTagger(train_data, backoff=unigram_tagger)\n",
        "print('Accuracy of trigram tagger = {:.2%}'.format(trigram_tagger.evaluate(test_data)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4t3xyYd__8d-"
      },
      "source": [
        "## Увеличиваем контекст с рекуррентными сетями\n",
        "\n",
        "Униграмная модель работает на удивление хорошо, но мы же собрались учить сеточки.\n",
        "\n",
        "Омонимия - основная причина, почему униграмная модель плоха:  \n",
        "*“he cashed a check at the **bank**”*  \n",
        "vs  \n",
        "*“he sat on the **bank** of the river”*\n",
        "\n",
        "Поэтому нам очень полезно учитывать контекст при предсказании тега.\n",
        "\n",
        "Воспользуемся LSTM - он умеет работать с контекстом очень даже хорошо:\n",
        "\n",
        "![](https://image.ibb.co/kgmoff/Baseline-Tagger.png)\n",
        "\n",
        "Синим показано выделение фичей из слова, LSTM оранжевенький - он строит эмбеддинги слов с учетом контекста, а дальше зелененькая логистическая регрессия делает предсказания тегов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "RtRbz1SwgEqc"
      },
      "outputs": [],
      "source": [
        "def convert_data(data, word2ind, tag2ind):\n",
        "    X = [[word2ind.get(word, 0) for word, _ in sample] for sample in data]\n",
        "    y = [[tag2ind[tag] for _, tag in sample] for sample in data]\n",
        "    \n",
        "    return X, y\n",
        "\n",
        "X_train, y_train = convert_data(train_data, word2ind, tag2ind)\n",
        "X_val, y_val = convert_data(val_data, word2ind, tag2ind)\n",
        "X_test, y_test = convert_data(test_data, word2ind, tag2ind)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkGLzl2OuhBk",
        "outputId": "034cb255-3bce-4c7b-da33-a03330ca1bec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(36554, 36554)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(X_train), len(train_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "это заменяет батчи.\n",
        "это генератор."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "DhsTKZalfih6"
      },
      "outputs": [],
      "source": [
        "def iterate_batches(data, batch_size):\n",
        "    X, y = data\n",
        "    n_samples = len(X)\n",
        "    print('n_samples', n_samples)\n",
        "    indices = np.arange(n_samples)\n",
        "    np.random.shuffle(indices)\n",
        "    \n",
        "    for start in range(0, n_samples, batch_size):\n",
        "        end = min(start + batch_size, n_samples)\n",
        "        \n",
        "        batch_indices = indices[start:end]\n",
        "        print('batch_indices', batch_indices)\n",
        "        max_sent_len = max(len(X[ind]) for ind in batch_indices)\n",
        "        print('max_sent_len', max_sent_len)\n",
        "        X_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
        "        y_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
        "        \n",
        "        for batch_ind, sample_ind in enumerate(batch_indices):\n",
        "            X_batch[:len(X[sample_ind]), batch_ind] = X[sample_ind]\n",
        "            y_batch[:len(y[sample_ind]), batch_ind] = y[sample_ind]\n",
        "            \n",
        "        yield X_batch, y_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a2BLQIzwfAJ",
        "outputId": "994d6970-4d03-4e72-9d29-8b5c0baa1a10"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(36554, 36554)"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(X_train), len(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4XsRII5kW5x",
        "outputId": "29ff1f7e-6fb6-4ec7-ccd2-467e5618fffc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "n_samples 36554\n",
            "batch_indices [  608  2649  5742 24448]\n",
            "max_sent_len 32\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "((32, 4), (32, 4))"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_batch, y_batch = next(iterate_batches((X_train, y_train), 4))\n",
        "\n",
        "X_batch.shape, y_batch.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rY18rkng_c3",
        "outputId": "5f8168b1-b867-44e3-cff7-a15fc6e613ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[32771.  7068. 30782.   746.]\n",
            " [10856.  3843.  3322. 11139.]]\n",
            "[[ 2.  4. 12.  1.]\n",
            " [ 4.  4.  3.  4.]]\n"
          ]
        }
      ],
      "source": [
        "print(X_batch[:2])\n",
        "\n",
        "print(y_batch[:2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5I9E9P6eFYv"
      },
      "source": [
        "**Задание** Реализуйте `LSTMTagger`:\n",
        "\n",
        "* [nn.Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)\n",
        "    - Input: (∗)\n",
        "    - Output: (∗, H)\n",
        "* [nn.LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html)\n",
        "    - Input: (N, L, H_in)\n",
        "    - Output: (N, L, D * H_out), (h_n, c_n)\\\n",
        "    D = 2 if bidirectional=True otherwise 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAr7DIjN616j"
      },
      "source": [
        "$L\\times bs \\xrightarrow{\\text{nn.Embedding}} L \\times bs \\times H_{in} \\xrightarrow{\\text{nn.LSTM}} L \\times bs \\times H^*_{out}\\xrightarrow{\\text{nn.Linear}} L \\times bs \\times \\text{tagset_size}$ \n",
        "\n",
        "\\*$H_{out}$ = lstm_hidden_dim\n",
        "\n",
        "- L - sequence length\n",
        "- bs - batch size\n",
        "- $H_{in}$ - number of input features representing an object\n",
        "- $H_{out}$ - number of output features representing an object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "WVEHju54d68T"
      },
      "outputs": [],
      "source": [
        "class LSTMTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self._emb = nn.Embedding(vocab_size, word_emb_dim)\n",
        "        self._lstm = nn.LSTM(word_emb_dim, lstm_hidden_dim, num_layers=lstm_layers_count)\n",
        "        self._out_layer = nn.Linear(lstm_hidden_dim, tagset_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        emb = self._emb(inputs)\n",
        "        # print('emb shape:', emb.shape)\n",
        "        output, _ = self._lstm(emb)\n",
        "        out = self._out_layer(output)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_HA8zyheYGH"
      },
      "source": [
        "**Задание** Научитесь считать accuracy и loss (а заодно проверьте, что модель работает)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbrxsZ2mehWB",
        "outputId": "d695380d-32fb-4cf7-8228-a9859645e5b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.0870\n"
          ]
        }
      ],
      "source": [
        "model = LSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ")\n",
        "\n",
        "X_batch, y_batch = torch.LongTensor(X_batch), torch.LongTensor(y_batch)\n",
        "\n",
        "logits = model(X_batch)\n",
        "preds = torch.argmax(logits, dim=-1)\n",
        "\n",
        "# accuracy\n",
        "mask = (y_batch != 0).float() # помним, что тэг 0 соотвествует слову <pad>, не учитываем <pad> в рассчете точности\n",
        "correct_count = ((preds == y_batch).float() * mask).sum().item()\n",
        "total_count = mask.sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct_count / total_count:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3_DgaXv3EZS",
        "outputId": "523b2667-5e1c-4ac0-f6e1-95ceebf35e31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 4]) torch.Size([32, 4, 13]) torch.Size([32, 4])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[8, 6, 7, 8],\n",
              "        [8, 6, 3, 8]])"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(X_batch.shape, logits.shape, preds.shape)\n",
        "# 32x4 ->(embed) 32x4x100 ->(lstm) 32x4x128 ->(linear) 32x4x13\n",
        "preds[:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9B4nuFy9d-C",
        "outputId": "a68a2e94-0297-45ce-a249-d1894a972cbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 4])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[ 2,  4, 12,  1],\n",
              "        [ 4,  4,  3,  4],\n",
              "        [ 3, 10,  4, 12],\n",
              "        [ 1, 11,  7,  6],\n",
              "        [ 4, 12,  3, 10],\n",
              "        [10,  4,  4,  1],\n",
              "        [ 1,  2, 10,  4],\n",
              "        [ 4,  2,  4,  6],\n",
              "        [ 3,  3, 12,  5],\n",
              "        [ 4,  1,  1, 12],\n",
              "        [10, 12,  4, 10],\n",
              "        [11,  4,  3,  5],\n",
              "        [12, 10,  1,  8],\n",
              "        [10, 12,  6, 12],\n",
              "        [ 0,  9,  4,  7],\n",
              "        [ 0,  7, 10, 12],\n",
              "        [ 0,  6,  1,  8],\n",
              "        [ 0,  3,  4, 12],\n",
              "        [ 0,  1,  3,  6],\n",
              "        [ 0,  4,  1,  4],\n",
              "        [ 0,  3,  1,  3],\n",
              "        [ 0,  4,  4, 11],\n",
              "        [ 0, 10, 12, 10],\n",
              "        [ 0,  0, 12,  0],\n",
              "        [ 0,  0, 12,  0],\n",
              "        [ 0,  0, 12,  0],\n",
              "        [ 0,  0, 11,  0],\n",
              "        [ 0,  0,  7,  0],\n",
              "        [ 0,  0,  1,  0],\n",
              "        [ 0,  0,  6,  0],\n",
              "        [ 0,  0,  4,  0],\n",
              "        [ 0,  0, 10,  0]])"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(y_batch.shape)\n",
        "y_batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnFo9lilTw52"
      },
      "source": [
        "[nn.CrossEnropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html?highlight=ignore_index)\n",
        "\n",
        "$LogLoss = -\\sum_{i=1}^{C} t_i log(p_i)$\n",
        "\n",
        "$\\ell(x, y)=L=\\left\\{l_{1}, \\ldots, l_{N}\\right\\}^{\\top}, \\quad l_{n}=-w_{y_{n}} \\log \\dfrac{\\exp \\left(x_{n, y_{n}}\\right)}{\\sum_{c=1}^{C} \\exp \\left(x_{n, c}\\right)} \\cdot 1\\left\\{y_{n} \\neq\\right. \\text { ignore_index }$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMUyUm1hgpe3",
        "outputId": "fa42f621-720b-43e8-f71d-9b54a26c3dc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(81.8836, grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "logits = model(X_batch)\n",
        "loss = 0\n",
        "for ind, row in enumerate(logits):\n",
        "    loss += criterion(row, y_batch[ind])  # Input: (N, C) & (N) where C - classes #\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSgV3NPUpcjH"
      },
      "source": [
        "**Задание** Вставьте эти вычисление в функцию:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "FprPQ0gllo7b"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def do_epoch(model, criterion, data, batch_size, optimizer=None, name=None):\n",
        "    epoch_loss = 0\n",
        "    correct_count = 0\n",
        "    sum_count = 0\n",
        "    \n",
        "    is_train = not optimizer is None\n",
        "    name = name or ''\n",
        "    model.train(is_train)\n",
        "    \n",
        "    batches_count = math.ceil(len(data[0]) / batch_size)\n",
        "    \n",
        "    with torch.autograd.set_grad_enabled(is_train):\n",
        "        with tqdm(total=batches_count) as progress_bar:\n",
        "            for i, (X_batch, y_batch) in enumerate(iterate_batches(data, batch_size)):\n",
        "                X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
        "                logits = model(X_batch)\n",
        "\n",
        "                loss = 0\n",
        "                for ind, row in enumerate(logits):\n",
        "                    loss += criterion(row, y_batch[ind])\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "                if optimizer:\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                preds = torch.argmax(logits, dim=-1)\n",
        "                mask = (y_batch != 0).float()\n",
        "                \n",
        "                cur_correct_count, cur_sum_count = ((preds == y_batch).float() * mask).sum().item(), mask.sum().item()\n",
        "\n",
        "                correct_count += cur_correct_count\n",
        "                sum_count += cur_sum_count\n",
        "\n",
        "                progress_bar.update()\n",
        "                progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
        "                    name, loss.item(), cur_correct_count / cur_sum_count)\n",
        "                )\n",
        "                \n",
        "            progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
        "                name, epoch_loss / batches_count, correct_count / sum_count)\n",
        "            )\n",
        "\n",
        "    return epoch_loss / batches_count, correct_count / sum_count\n",
        "\n",
        "\n",
        "def fit(model, criterion, optimizer, train_data, epochs_count=1, batch_size=32,\n",
        "        val_data=None, val_batch_size=None):\n",
        "        \n",
        "    if not val_data is None and val_batch_size is None:\n",
        "        val_batch_size = batch_size\n",
        "        \n",
        "    for epoch in range(epochs_count):\n",
        "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
        "        train_loss, train_acc = do_epoch(model, criterion, train_data, batch_size, optimizer, name_prefix + 'Train:')\n",
        "        \n",
        "        if not val_data is None:\n",
        "            val_loss, val_acc = do_epoch(model, criterion, val_data, val_batch_size, None, name_prefix + '  Val:')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "!conda install pytorch torchvision torchaudio cudatoolkit=10.2 -c pytorch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pqfbeh1ltEYa",
        "outputId": "7a51b5a7-672c-46ef-d4b0-7f8158975996"
      },
      "outputs": [
        {
          "ename": "AssertionError",
          "evalue": "Torch not compiled with CUDA enabled",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3548/2272550618.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m model = LSTMTagger(\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mvocab_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword2ind\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mtagset_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtag2ind\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m ).cuda()\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mcuda\u001b[1;34m(self, device)\u001b[0m\n\u001b[0;32m    489\u001b[0m             \u001b[0mModule\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \"\"\"\n\u001b[1;32m--> 491\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mxpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    385\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 387\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    388\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    407\u001b[0m                 \u001b[1;31m# `with torch.no_grad():`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    408\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 409\u001b[1;33m                     \u001b[0mparam_applied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    410\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    411\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    489\u001b[0m             \u001b[0mModule\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \"\"\"\n\u001b[1;32m--> 491\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mxpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    162\u001b[0m                 \"multiprocessing, you must use the 'spawn' start method\")\n\u001b[0;32m    163\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_cuda_getDeviceCount'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 164\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    165\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_cudart\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m             raise AssertionError(\n",
            "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
          ]
        }
      ],
      "source": [
        "model = LSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0).cuda()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=5,\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0qGetIhfUE5"
      },
      "source": [
        "### Masking\n",
        "\n",
        "**Задание** Проверьте себя - не считаете ли вы потери и accuracy на паддингах - очень легко получить высокое качество за счет этого.\n",
        "\n",
        "У функции потерь есть параметр `ignore_index`, для таких целей. Для accuracy нужно использовать маскинг - умножение на маску из нулей и единиц, где нули на позициях паддингов (а потом усреднение по ненулевым позициям в маске)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAfV2dEOfHo5"
      },
      "source": [
        "**Задание** Посчитайте качество модели на тесте. Ожидается результат лучше бейзлайна!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98wr38_rw55D",
        "outputId": "76f0d1d3-5c88-49d1-e593-bfb0d65e79b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy is 94.02%\n"
          ]
        }
      ],
      "source": [
        "def compute_accuracy(model, data, batch_size=64):\n",
        "    model.eval()\n",
        "    val_accuracy = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for i, (X_batch, y_batch) in enumerate(iterate_batches(data, batch_size)):\n",
        "        X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
        "        logits = model(X_batch)\n",
        "        \n",
        "        pred = torch.argmax(logits, dim=-1)\n",
        "        mask = (y_batch != 0).float()\n",
        "        \n",
        "        correct += ((pred == y_batch).float() * mask).sum().item()\n",
        "        \n",
        "        total += mask.sum().item()        \n",
        "        \n",
        "    val_accuracy = float(correct)/total\n",
        "        \n",
        "    return val_accuracy\n",
        "\n",
        "test_ac =  compute_accuracy(model, (X_test, y_test))\n",
        "print(f'Test accuracy is {test_ac:.2%}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXUTSFaEHbDG"
      },
      "source": [
        "### Bidirectional LSTM\n",
        "\n",
        "Благодаря BiLSTM можно использовать сразу оба контеста при предсказании тега слова. Т.е. для каждого токена $w_i$ forward LSTM будет выдавать представление $\\mathbf{f_i} \\sim (w_1, \\ldots, w_i)$ - построенное по всему левому контексту - и $\\mathbf{b_i} \\sim (w_n, \\ldots, w_i)$ - представление правого контекста. Их конкатенация автоматически захватит весь доступный контекст слова: $\\mathbf{h_i} = [\\mathbf{f_i}, \\mathbf{b_i}] \\sim (w_1, \\ldots, w_n)$.\n",
        "\n",
        "![BiLSTM](https://www.researchgate.net/profile/Wang_Ling/publication/280912217/figure/fig2/AS:391505383575555@1470353565299/Illustration-of-our-neural-network-for-POS-tagging.png)  \n",
        "*From [Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation](https://arxiv.org/abs/1508.02096)*\n",
        "\n",
        "**Задание** Добавьте Bidirectional LSTM.\n",
        "\n",
        "Вспомним, что Unidirectional LSTM выглядела так:\n",
        "```python\n",
        "class LSTMTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self._emb = nn.Embedding(vocab_size, word_emb_dim)\n",
        "        self._lstm = nn.LSTM(word_emb_dim, lstm_hidden_dim, num_layers=lstm_layers_count)\n",
        "        self._out_layer = nn.Linear(lstm_hidden_dim, tagset_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        emb = self._emb(inputs)\n",
        "        # print('emb shape:', emb.shape)\n",
        "        output, _ = self._lstm(emb)\n",
        "        out = self._out_layer(output)\n",
        "        return out\n",
        "```\n",
        "\n",
        "$L\\times bs \\xrightarrow{\\text{nn.Embedding}} L \\times bs \\times H_{in} \\xrightarrow{\\text{nn.LSTM}} L \\times bs \\times D\\cdot H_{out}\\xrightarrow{\\text{nn.Linear}} L \\times bs \\times \\text{tagset_size}$ "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lWBE0sYXUMz"
      },
      "outputs": [],
      "source": [
        "class BidirectionalLSTMTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self._emb = nn.Embedding(vocab_size, word_emb_dim)\n",
        "        self._lstm = nn.LSTM(word_emb_dim, \n",
        "                             lstm_hidden_dim, \n",
        "                             num_layers=lstm_layers_count,\n",
        "                             bidirectional = True)\n",
        "        self._out_layer = nn.Linear(2*lstm_hidden_dim, tagset_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        emb = self._emb(inputs)\n",
        "        output, _ = self._lstm(emb)\n",
        "        out = self._out_layer(output)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qB8o9Dx3hJ_u",
        "outputId": "036c7457-55bc-4ca9-eff5-8a81b95a60b0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[1 / 6] Train: Loss = 21.08800, Accuracy = 88.51%: 100%|██████████| 572/572 [00:25<00:00, 22.03it/s]\n",
            "[1 / 6]   Val: Loss = 17.80767, Accuracy = 94.38%: 100%|██████████| 13/13 [00:00<00:00, 16.89it/s]\n",
            "[2 / 6] Train: Loss = 6.36271, Accuracy = 96.45%: 100%|██████████| 572/572 [00:25<00:00, 22.22it/s]\n",
            "[2 / 6]   Val: Loss = 17.89357, Accuracy = 94.84%: 100%|██████████| 13/13 [00:00<00:00, 17.97it/s]\n",
            "[3 / 6] Train: Loss = 3.90675, Accuracy = 97.51%: 100%|██████████| 572/572 [00:25<00:00, 22.19it/s]\n",
            "[3 / 6]   Val: Loss = 18.95265, Accuracy = 95.23%: 100%|██████████| 13/13 [00:00<00:00, 17.73it/s]\n",
            "[4 / 6] Train: Loss = 3.29084, Accuracy = 97.83%: 100%|██████████| 572/572 [00:25<00:00, 22.08it/s]\n",
            "[4 / 6]   Val: Loss = 18.86710, Accuracy = 95.24%: 100%|██████████| 13/13 [00:00<00:00, 17.25it/s]\n",
            "[5 / 6] Train: Loss = 3.09439, Accuracy = 97.96%: 100%|██████████| 572/572 [00:25<00:00, 22.06it/s]\n",
            "[5 / 6]   Val: Loss = 17.52249, Accuracy = 95.42%: 100%|██████████| 13/13 [00:00<00:00, 16.34it/s]\n",
            "[6 / 6] Train: Loss = 2.74650, Accuracy = 98.16%: 100%|██████████| 572/572 [00:25<00:00, 22.04it/s]\n",
            "[6 / 6]   Val: Loss = 17.15901, Accuracy = 95.29%: 100%|██████████| 13/13 [00:00<00:00, 17.63it/s]\n"
          ]
        }
      ],
      "source": [
        "model = BidirectionalLSTMTagger(\n",
        "    lstm_layers_count=2,\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = 0).cuda()\n",
        "optimizer = optim.Adam(model.parameters(), lr = 5e-3, weight_decay = 5e-4)\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), \n",
        "    epochs_count=6, batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTXmYGD_ANhm"
      },
      "source": [
        "### Предобученные эмбеддинги\n",
        "\n",
        "Мы знаем, какая клёвая вещь - предобученные эмбеддинги. При текущем размере обучающей выборки еще можно было учить их и с нуля - с меньшей было бы совсем плохо.\n",
        "\n",
        "Поэтому стандартный пайплайн - скачать эмбеддинги, засунуть их в сеточку. Запустим его:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZpY_Q1xZ18h",
        "outputId": "a5db2fc0-fe1f-4506-ca23-b673d6cdde41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
          ]
        }
      ],
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "w2v_model = api.load('glove-wiki-gigaword-100')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DyHohi1GgXMr",
        "outputId": "824f825d-3946-4b8b-f220-cec56f5c4246"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(100,)"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "w2v_model.vectors.shape\n",
        "w2v_model.get_vector('is').shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYogOoKlgtcf"
      },
      "source": [
        "Построим подматрицу для слов из нашей тренировочной выборки:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6Pko77VjFQx",
        "outputId": "25031ae0-1d84-4178-96db-e3c8d313230a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<gensim.models.keyedvectors.KeyedVectors at 0x7f828585f050>"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "w2v_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VsCstxiO03oT",
        "outputId": "4ee14280-ae5a-48e2-fbd6-62a9b83a89e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Know 38736 out of 45441 word embeddings\n"
          ]
        }
      ],
      "source": [
        "known_count = 0\n",
        "embeddings = np.zeros((len(word2ind), w2v_model.vectors.shape[1]))\n",
        "for word, ind in word2ind.items():\n",
        "    word = word.lower()\n",
        "    if word in w2v_model.key_to_index:\n",
        "        embeddings[ind] = w2v_model.get_vector(word)\n",
        "        known_count += 1\n",
        "        \n",
        "print('Know {} out of {} word embeddings'.format(known_count, len(word2ind)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcG7i-R8hbY3"
      },
      "source": [
        "**Задание** Сделайте модель с предобученной матрицей. Используйте `nn.Embedding.from_pretrained`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-A9O54U8jaxf",
        "outputId": "afb7d89c-7a25-4695-a20a-794d487db271"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(45441, 100)"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embeddings.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0OMsebzNjSuX"
      },
      "outputs": [],
      "source": [
        "embeddings_t = torch.from_numpy(embeddings)\n",
        "embeddings_t.requires_grad = True\n",
        "embeddings_t = embeddings_t.float().cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxaRBpQd0pat"
      },
      "outputs": [],
      "source": [
        "class LSTMTaggerWithPretrainedEmbs(nn.Module):\n",
        "    def __init__(self, embeddings, tagset_size, lstm_hidden_dim=64, lstm_layers_count=1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self._emb = nn.Embedding.from_pretrained(embeddings_t) # важно: по дефолту freeze=True, embeddings не обучаются!\n",
        "        self._lstm = nn.LSTM(embeddings_t.shape[1], \n",
        "                             lstm_hidden_dim, \n",
        "                             num_layers=lstm_layers_count, \n",
        "                             bidirectional = True)\n",
        "        self._out_layer = nn.Linear(2*lstm_hidden_dim, tagset_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        emb = self._emb(inputs)\n",
        "        output, _ = self._lstm(emb)\n",
        "        out = self._out_layer(output)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBtI6BDE-Fc7",
        "outputId": "8655f590-0840-4342-fbd5-7ecdecc2010f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[1 / 35] Train: Loss = 42.76890, Accuracy = 79.92%: 100%|██████████| 572/572 [00:15<00:00, 36.99it/s]\n",
            "[1 / 35]   Val: Loss = 35.28130, Accuracy = 89.52%: 100%|██████████| 13/13 [00:00<00:00, 33.35it/s]\n",
            "[2 / 35] Train: Loss = 16.91721, Accuracy = 91.89%: 100%|██████████| 572/572 [00:15<00:00, 37.27it/s]\n",
            "[2 / 35]   Val: Loss = 25.15937, Accuracy = 92.29%: 100%|██████████| 13/13 [00:00<00:00, 33.53it/s]\n",
            "[3 / 35] Train: Loss = 12.54278, Accuracy = 93.78%: 100%|██████████| 572/572 [00:15<00:00, 38.13it/s]\n",
            "[3 / 35]   Val: Loss = 18.91427, Accuracy = 93.73%: 100%|██████████| 13/13 [00:00<00:00, 34.91it/s]\n",
            "[4 / 35] Train: Loss = 9.85814, Accuracy = 94.85%: 100%|██████████| 572/572 [00:15<00:00, 38.12it/s]\n",
            "[4 / 35]   Val: Loss = 16.98180, Accuracy = 94.53%: 100%|██████████| 13/13 [00:00<00:00, 35.39it/s]\n",
            "[5 / 35] Train: Loss = 8.32855, Accuracy = 95.52%: 100%|██████████| 572/572 [00:15<00:00, 37.25it/s]\n",
            "[5 / 35]   Val: Loss = 14.47994, Accuracy = 95.00%: 100%|██████████| 13/13 [00:00<00:00, 33.87it/s]\n",
            "[6 / 35] Train: Loss = 7.13231, Accuracy = 95.97%: 100%|██████████| 572/572 [00:15<00:00, 38.04it/s]\n",
            "[6 / 35]   Val: Loss = 13.18629, Accuracy = 95.41%: 100%|██████████| 13/13 [00:00<00:00, 35.60it/s]\n",
            "[7 / 35] Train: Loss = 6.34487, Accuracy = 96.31%: 100%|██████████| 572/572 [00:14<00:00, 38.38it/s]\n",
            "[7 / 35]   Val: Loss = 12.83894, Accuracy = 95.71%: 100%|██████████| 13/13 [00:00<00:00, 36.04it/s]\n",
            "[8 / 35] Train: Loss = 5.52015, Accuracy = 96.61%: 100%|██████████| 572/572 [00:15<00:00, 37.69it/s]\n",
            "[8 / 35]   Val: Loss = 11.92172, Accuracy = 95.81%: 100%|██████████| 13/13 [00:00<00:00, 33.55it/s]\n",
            "[9 / 35] Train: Loss = 5.03650, Accuracy = 96.83%: 100%|██████████| 572/572 [00:15<00:00, 37.39it/s]\n",
            "[9 / 35]   Val: Loss = 10.80454, Accuracy = 96.03%: 100%|██████████| 13/13 [00:00<00:00, 35.39it/s]\n",
            "[10 / 35] Train: Loss = 4.54112, Accuracy = 97.04%: 100%|██████████| 572/572 [00:15<00:00, 37.61it/s]\n",
            "[10 / 35]   Val: Loss = 11.31178, Accuracy = 96.11%: 100%|██████████| 13/13 [00:00<00:00, 34.39it/s]\n",
            "[11 / 35] Train: Loss = 4.16700, Accuracy = 97.18%: 100%|██████████| 572/572 [00:15<00:00, 37.05it/s]\n",
            "[11 / 35]   Val: Loss = 10.26857, Accuracy = 96.31%: 100%|██████████| 13/13 [00:00<00:00, 35.06it/s]\n",
            "[12 / 35] Train: Loss = 3.86666, Accuracy = 97.31%: 100%|██████████| 572/572 [00:15<00:00, 37.95it/s]\n",
            "[12 / 35]   Val: Loss = 11.33402, Accuracy = 96.32%: 100%|██████████| 13/13 [00:00<00:00, 34.21it/s]\n",
            "[13 / 35] Train: Loss = 3.59250, Accuracy = 97.44%: 100%|██████████| 572/572 [00:14<00:00, 38.75it/s]\n",
            "[13 / 35]   Val: Loss = 11.24074, Accuracy = 96.45%: 100%|██████████| 13/13 [00:00<00:00, 37.28it/s]\n",
            "[14 / 35] Train: Loss = 3.30422, Accuracy = 97.56%: 100%|██████████| 572/572 [00:15<00:00, 37.47it/s]\n",
            "[14 / 35]   Val: Loss = 10.88618, Accuracy = 96.45%: 100%|██████████| 13/13 [00:00<00:00, 34.17it/s]\n",
            "[15 / 35] Train: Loss = 3.12870, Accuracy = 97.66%: 100%|██████████| 572/572 [00:15<00:00, 37.61it/s]\n",
            "[15 / 35]   Val: Loss = 9.63042, Accuracy = 96.55%: 100%|██████████| 13/13 [00:00<00:00, 36.42it/s]\n",
            "[16 / 35] Train: Loss = 2.93031, Accuracy = 97.74%: 100%|██████████| 572/572 [00:14<00:00, 38.38it/s]\n",
            "[16 / 35]   Val: Loss = 10.18558, Accuracy = 96.51%: 100%|██████████| 13/13 [00:00<00:00, 35.36it/s]\n",
            "[17 / 35] Train: Loss = 2.72006, Accuracy = 97.85%: 100%|██████████| 572/572 [00:15<00:00, 38.08it/s]\n",
            "[17 / 35]   Val: Loss = 10.24596, Accuracy = 96.52%: 100%|██████████| 13/13 [00:00<00:00, 34.10it/s]\n",
            "[18 / 35] Train: Loss = 2.58963, Accuracy = 97.93%: 100%|██████████| 572/572 [00:15<00:00, 36.54it/s]\n",
            "[18 / 35]   Val: Loss = 8.90968, Accuracy = 96.58%: 100%|██████████| 13/13 [00:00<00:00, 33.62it/s]\n",
            "[19 / 35] Train: Loss = 2.46029, Accuracy = 97.99%: 100%|██████████| 572/572 [00:15<00:00, 36.96it/s]\n",
            "[19 / 35]   Val: Loss = 10.25926, Accuracy = 96.67%: 100%|██████████| 13/13 [00:00<00:00, 35.59it/s]\n",
            "[20 / 35] Train: Loss = 2.29794, Accuracy = 98.08%: 100%|██████████| 572/572 [00:15<00:00, 37.56it/s]\n",
            "[20 / 35]   Val: Loss = 10.47576, Accuracy = 96.70%: 100%|██████████| 13/13 [00:00<00:00, 33.04it/s]\n",
            "[21 / 35] Train: Loss = 2.20685, Accuracy = 98.14%: 100%|██████████| 572/572 [00:15<00:00, 38.05it/s]\n",
            "[21 / 35]   Val: Loss = 9.45592, Accuracy = 96.75%: 100%|██████████| 13/13 [00:00<00:00, 34.99it/s]\n",
            "[22 / 35] Train: Loss = 2.07402, Accuracy = 98.22%: 100%|██████████| 572/572 [00:15<00:00, 37.65it/s]\n",
            "[22 / 35]   Val: Loss = 10.32854, Accuracy = 96.78%: 100%|██████████| 13/13 [00:00<00:00, 35.28it/s]\n",
            "[23 / 35] Train: Loss = 1.99154, Accuracy = 98.27%: 100%|██████████| 572/572 [00:14<00:00, 38.54it/s]\n",
            "[23 / 35]   Val: Loss = 8.60891, Accuracy = 96.74%: 100%|██████████| 13/13 [00:00<00:00, 34.74it/s]\n",
            "[24 / 35] Train: Loss = 1.87851, Accuracy = 98.32%: 100%|██████████| 572/572 [00:15<00:00, 36.49it/s]\n",
            "[24 / 35]   Val: Loss = 9.10361, Accuracy = 96.79%: 100%|██████████| 13/13 [00:00<00:00, 33.28it/s]\n",
            "[25 / 35] Train: Loss = 1.78113, Accuracy = 98.41%: 100%|██████████| 572/572 [00:15<00:00, 36.88it/s]\n",
            "[25 / 35]   Val: Loss = 9.53620, Accuracy = 96.78%: 100%|██████████| 13/13 [00:00<00:00, 33.59it/s]\n",
            "[26 / 35] Train: Loss = 1.70533, Accuracy = 98.46%: 100%|██████████| 572/572 [00:15<00:00, 36.55it/s]\n",
            "[26 / 35]   Val: Loss = 10.96571, Accuracy = 96.75%: 100%|██████████| 13/13 [00:00<00:00, 31.83it/s]\n",
            "[27 / 35] Train: Loss = 1.72525, Accuracy = 98.23%:  53%|█████▎    | 301/572 [00:08<00:07, 35.85it/s]"
          ]
        }
      ],
      "source": [
        "model = LSTMTaggerWithPretrainedEmbs(\n",
        "    embeddings=embeddings,\n",
        "    tagset_size=len(tag2ind)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=35,\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ne_8f24h8kg"
      },
      "source": [
        "**Задание** Оцените качество модели на тестовой выборке. Обратите внимание, вовсе не обязательно ограничиваться векторами из урезанной матрицы - вполне могут найтись слова в тесте, которых не было в трейне и для которых есть эмбеддинги.\n",
        "\n",
        "Добейтесь качества лучше прошлых моделей."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPUuAPGhEGVR",
        "outputId": "8133867c-bba1-443c-ddb7-7a0d5462b839"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy is 96.82%\n"
          ]
        }
      ],
      "source": [
        "test_ac =  compute_accuracy(model, (X_test, y_test))\n",
        "print(f'Test accuracy is {test_ac:.2%}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTjnRRF42lJH"
      },
      "source": [
        "### Сравнение результатов работы разных подходов\n",
        "| №      | Название        | Точность\n",
        "| :---:  |:-------------:  | :------------------ | \n",
        "| 1      | Unigram Tagger  | 92.62%              | \n",
        "| 2      | Bigram Tagger   | 93.42%              | \n",
        "| 3      | Trigram Tagger  | 93.28%              | \n",
        "| 4      | LSTM            | 94.02%              | \n",
        "| 5      | BiLSTM          | 95.42%              | \n",
        "| 6      | BiLSTM Pretrained embeddings| 96.82%  |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6A7Km9BYmAgC"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "RNNs.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
